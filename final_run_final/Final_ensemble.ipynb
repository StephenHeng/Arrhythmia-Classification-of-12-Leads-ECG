{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pywt\n",
    "import os\n",
    "from sklearn.metrics import f1_score,hamming_loss\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "path = \"/media/jdcloud/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_y(labels, y, num_class=10):\n",
    "    bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1, label_nona.shape[0]):\n",
    "            bin_label[i, int(label_nona[j])] = 1\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_attention_maxpooling_10net_fold0.csv\").values)\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_attention_maxpooling_10net_fold1.csv\").values)\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_attention_maxpooling_10net_fold2.csv\").values)\n",
    "path = './'\n",
    "\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_4block_10net_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_4block_10net_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_4block_10net_fold2.csv\").values) #1fold f0.822\n",
    "\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_one_net_fold0.csv\").values)\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_one_net_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_one_net_fold2.csv\").values) # 1fold f0.813\n",
    "\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_1net_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_1net_fold1.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_1net_fold2.csv\").values)\n",
    "\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_fold0.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_fold2.csv\").values) # 3folds f0.817\n",
    "\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_f0819_10net_fold.csv\").values) # 1fold f0.819\n",
    "\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_sym_addori_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_sym_addori_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_sym_addori_fold2.csv\").values) # 3folds f0.817\n",
    "\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_db6_addori_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_db6_addori_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_db6_addori_fold2.csv\").values)\n",
    "\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_ori_addori_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_ori_addori_fold1.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_ori_addori_fold2.csv\").values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "#test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_attention_maxpooling_10net_fold0.csv\").values)\n",
    "#test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_attention_maxpooling_10net_fold1.csv\").values)\n",
    "#test.append( pd.read_csv(path+\"test_csv/\"+\"densenet_attention_maxpooling_10net_fold2.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_4block_10net_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_4block_10net_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_4block_10net_fold2.csv\").values)\n",
    "\n",
    "#test.append(pd.read_csv(path+\"test_csv/\"+\"attention_one_net_fold0.csv\").values)\n",
    "#test.append(pd.read_csv(path+\"test_csv/\"+\"attention_one_net_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_one_net_fold2.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_1net_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_1net_fold1.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_1net_fold2.csv\").values)\n",
    "\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_fold0.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_fold2.csv\").values)\n",
    "\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_f0819_10net_fold.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_sym_addori_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_sym_addori_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_sym_addori_fold2.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_db6_addori_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_db6_addori_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_db6_addori_fold2.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_ori_addori_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_ori_addori_fold1.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_ori_addori_fold2.csv\").values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# # specify your configurations as a dict\n",
    "# params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'binary',#'multiclass',\n",
    "#     'metric': 'binary_logloss',\n",
    "#     'num_class': 1,\n",
    "#     'num_leaves': 100,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'feature_fraction': 0.9,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'bagging_freq': 5,\n",
    "#     'verbose': 0,\n",
    "#     'seed':2019\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.005, loss='deviance', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=20, min_sa...      subsample=0.7, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, Lasso, RidgeCV,LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from mlknn import MLkNN\n",
    "from mltsvm import MLTSVM\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate=0.005,n_estimators=100,max_depth=5,min_samples_leaf=20,\n",
    "                                min_samples_split=600,subsample=0.7,random_state=2019)\n",
    "LR = LogisticRegression(penalty=\"l2\",C=1.0)\n",
    "#Eln = ElasticNet()\n",
    "#Las = Lasso(alpha=0.2)\n",
    "#LRR = RidgeCV()\n",
    "\n",
    "pre_type = \"db6\"#\"db6\"#\"sym\"\n",
    "labels = pd.read_csv(\"/media/uuser/data/final_codes/final_run_test/REFERENCE.csv\")\n",
    "\n",
    "index = np.arange(6689)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "x_train = np.hstack(train)\n",
    "\n",
    "#train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.333, random_state=0)\n",
    "#train = lgb.Dataset(x_train,label=y_train)\n",
    "#valid = lgb.Dataset(valid_x, label=valid_y\n",
    "#gbm = lgb.train(params,train,num_boost_round=1000,#valid_sets=valid,early_stopping_rounds=5)\n",
    "LR1 = LinearRegression()\n",
    "clf = MLkNN(k=10)\n",
    "#clf = MLTSVM(c_k=0.5)\n",
    "clf = OneVsRestClassifier(gbm)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train data f1_score  : 0.9438611559218751\n",
      "f1 score of ab 0 is 0.970402878458721\n",
      "f1 score of ab 1 is 0.9927127363990341\n",
      "f1 score of ab 2 is 0.9688245635681747\n",
      "f1 score of ab 3 is 0.987408533509226\n",
      "f1 score of ab 4 is 0.9593696850761612\n",
      "f1 score of ab 5 is 0.9713568647666131\n",
      "f1 score of ab 6 is 0.9613769768078739\n",
      "f1 score of ab 7 is 0.9616455422371305\n",
      "f1 score of ab 8 is 0.9552754993527292\n",
      "f1 score of ab 9 is 0.9529752533087663\n",
      " train data hamming_loss  : 0.011989834055912692\n",
      " train data precision recall f1  : (0.9469527084267703, 0.9477450540688693, 0.9434733469716277, None)\n"
     ]
    }
   ],
   "source": [
    "# out = np.hstack(test)#\n",
    "# out0 = np.hstack(train[:-1])#\n",
    "# out1 = np.hstack(train[:-2])\n",
    "# out2 = np.hstack(train[:-2]+[train[-1]])\n",
    "\n",
    "# LR_clf = joblib.load(\"LR_ensemble0810.pkl\")\n",
    "# MLkNN8_clf = joblib.load(\"MLkNN8_ensemble.pkl\")\n",
    "# MLkNN10_clf = joblib.load(\"MLkNN10_ensemble.pkl\")\n",
    "# MLkNN10_db6_clf = joblib.load(\"MLkNN10_ensemble_db6_0810.pkl\")\n",
    "\n",
    "# y_pred_LR = LR_clf.predict(out0)\n",
    "# y_pred_proba_LR = LR_clf.predict_proba(out0)\n",
    "\n",
    "# y_pred_MLkNN8 = MLkNN8_clf.predict(out1).toarray()\n",
    "# y_pred_proba_MLkNN8 = MLkNN8_clf.predict_proba(out1).toarray()\n",
    "\n",
    "# y_pred_MLkNN10 = MLkNN10_clf.predict(out1).toarray()\n",
    "# y_pred_proba_MLkNN10 = MLkNN10_clf.predict_proba(out1).toarray()\n",
    "\n",
    "# y_pred_MLkNN10_db6 = MLkNN10_db6_clf.predict(out2).toarray()\n",
    "# y_pred_proba_MLkNN10_db6 = MLkNN10_db6_clf.predict_proba(out2).toarray()\n",
    "\n",
    "# y_pred_LR[:,2] = y_pred_MLkNN8[:,2]\n",
    "# y_pred_proba_LR[:,2] = y_pred_proba_MLkNN8[:,2]\n",
    "\n",
    "# y_pred_LR[:,5] = y_pred_MLkNN8[:,5]\n",
    "# y_pred_proba_LR[:,5] = y_pred_proba_MLkNN8[:,5]\n",
    "\n",
    "# y_pred_LR[:,6] = y_pred_MLkNN8[:,6]\n",
    "# y_pred_proba_LR[:,6] = y_pred_proba_MLkNN8[:,6]\n",
    "\n",
    "# y_pred_LR[:,9] = y_pred_MLkNN8[:,9]\n",
    "# y_pred_proba_LR[:,9] = y_pred_proba_MLkNN8[:,9]\n",
    "\n",
    "# y_pred_LR[:,4] = y_pred_MLkNN10[:,4]\n",
    "# y_pred_proba_LR[:,4] = y_pred_proba_MLkNN10[:,4]\n",
    "\n",
    "# y_pred_LR[:,8] = y_pred_MLkNN10[:,8]\n",
    "# y_pred_proba_LR[:,8] = y_pred_proba_MLkNN10[:,8]\n",
    "\n",
    "# y_pred_LR[:,7] = y_pred_MLkNN10_db6[:,7]\n",
    "# y_pred_proba_LR[:,7] = y_pred_proba_MLkNN10_db6[:,7]\n",
    "\n",
    "\n",
    "# y_pred = y_pred_LR\n",
    "# y_pred_proba_train = y_pred_proba_LR\n",
    "\n",
    "y_pred = clf.predict(x_train)#.toarray()\n",
    "y_pred_proba_train = clf.predict(x_train)#.toarray()\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "    \n",
    "print(\" train data hamming_loss  :\", hamming_loss(y_train, y_pred)) \n",
    "print(\" train data precision recall f1  :\", prf(y_train, y_pred,average=\"samples\"))# 'micro', 'weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " train data f1_score  : 0.9503375188836033\n",
    "f1 score of ab 0 is 0.9720396461822378\n",
    "f1 score of ab 1 is 0.9904887607485836\n",
    "f1 score of ab 2 is 0.9677540961794703\n",
    "f1 score of ab 3 is 0.986531683978493\n",
    "f1 score of ab 4 is 0.9623842989623262\n",
    "f1 score of ab 5 is 0.9790668350732303\n",
    "f1 score of ab 6 is 0.9730658300240653\n",
    "f1 score of ab 7 is 0.964883553430314\n",
    "f1 score of ab 8 is 0.9585124067148397\n",
    "f1 score of ab 9 is 0.9623326213834353\n",
    " train data hamming_loss  : 0.010913439976080132\n",
    " train data precision recall f1  : (0.9574550256640254, 0.9600039866447402, 0.9554396882996076, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-a91a982189d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.toarray()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0my_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.toarray()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "out = np.hstack(test)#\n",
    "# out0 = np.hstack(test[:-1])#\n",
    "# out1 = np.hstack(test[:-2])\n",
    "# out2 = np.hstack(test[:-2]+[test[-1]])\n",
    "\n",
    "# LR_clf = joblib.load(\"LR_ensemble0810.pkl\")\n",
    "# MLkNN8_clf = joblib.load(\"MLkNN8_ensemble.pkl\")\n",
    "# MLkNN10_clf = joblib.load(\"MLkNN10_ensemble.pkl\")\n",
    "# MLkNN10_db6_clf = joblib.load(\"MLkNN10_ensemble_db6_0810.pkl\")\n",
    "\n",
    "# y_pred_LR = LR_clf.predict(out0)\n",
    "# y_pred_proba_LR = LR_clf.predict_proba(out0)\n",
    "\n",
    "# y_pred_MLkNN8 = MLkNN8_clf.predict(out1).toarray()\n",
    "# y_pred_proba_MLkNN8 = MLkNN8_clf.predict_proba(out1).toarray()\n",
    "\n",
    "# y_pred_MLkNN10 = MLkNN10_clf.predict(out1).toarray()\n",
    "# y_pred_proba_MLkNN10 = MLkNN10_clf.predict_proba(out1).toarray()\n",
    "\n",
    "# y_pred_MLkNN10_db6 = MLkNN10_db6_clf.predict(out2).toarray()\n",
    "# y_pred_proba_MLkNN10_db6 = MLkNN10_db6_clf.predict_proba(out2).toarray()\n",
    "\n",
    "# y_pred_LR[:,2] = y_pred_MLkNN8[:,2]\n",
    "# y_pred_proba_LR[:,2] = y_pred_proba_MLkNN8[:,2]\n",
    "\n",
    "# y_pred_LR[:,5] = y_pred_MLkNN8[:,5]\n",
    "# y_pred_proba_LR[:,5] = y_pred_proba_MLkNN8[:,5]\n",
    "\n",
    "# y_pred_LR[:,6] = y_pred_MLkNN8[:,6]\n",
    "# y_pred_proba_LR[:,6] = y_pred_proba_MLkNN8[:,6]\n",
    "\n",
    "# y_pred_LR[:,9] = y_pred_MLkNN8[:,9]\n",
    "# y_pred_proba_LR[:,9] = y_pred_proba_MLkNN8[:,9]\n",
    "\n",
    "# y_pred_LR[:,4] = y_pred_MLkNN10[:,4]\n",
    "# y_pred_proba_LR[:,4] = y_pred_proba_MLkNN10[:,4]\n",
    "\n",
    "# y_pred_LR[:,8] = y_pred_MLkNN10[:,8]\n",
    "# y_pred_proba_LR[:,8] = y_pred_proba_MLkNN10[:,8]\n",
    "\n",
    "# y_pred_LR[:,7] = y_pred_MLkNN10_db6[:,7]\n",
    "# y_pred_proba_LR[:,7] = y_pred_proba_MLkNN10_db6[:,7]\n",
    "\n",
    "\n",
    "# y_pred = y_pred_LR\n",
    "# y_pred_proba = y_pred_proba_LR\n",
    "\n",
    "y_pred = clf.predict(out)#.toarray()\n",
    "y_pred_proba = clf.predict_proba(out)#.toarray()\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "pred = []\n",
    "for j in range(y_pred.shape[0]):\n",
    "    pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "\n",
    "''' ''' \n",
    "for i, val in enumerate(pred):\n",
    "    if val == []:\n",
    "        pass\n",
    "        #for i_p, val_p in enumerate(y_pred_proba[i]):\n",
    "        #    if val_p >= 0.4:\n",
    "        #        pred[i].append(i_p)    # f1 == 0.832\n",
    "                \n",
    "        if y_pred_proba[i][np.argmax(y_pred_proba[i])] >= 0.4:\n",
    "            pred[i] = [np.argmax(y_pred_proba[i])]     # f1 == 0.833  0.4\n",
    "path = \"/media/jdcloud\"\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('jupyter_answers_densenet_{}_0809.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(10 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLkNN10_ensemble_db6_0810.pkl']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Save\n",
    "joblib.dump(clf,\"MLkNN10_ensemble_db6_0810.pkl\")#LR_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold : [0.7 0.5 0.5 0.4 0.6 0.2 0.3 0.4 0.5 0.3]\n",
      " train data f1_score  : 0.9226479088215832\n",
      "f1 score of ab 0 is 0.9576243119144202\n",
      "f1 score of ab 1 is 0.9794953051205606\n",
      "f1 score of ab 2 is 0.951682745472497\n",
      "f1 score of ab 3 is 0.982355844958436\n",
      "f1 score of ab 4 is 0.9477985152051004\n",
      "f1 score of ab 5 is 0.9721827424838709\n",
      "f1 score of ab 6 is 0.958814994471973\n",
      "f1 score of ab 7 is 0.9415063228088438\n",
      "f1 score of ab 8 is 0.9398197061954244\n",
      "f1 score of ab 9 is 0.9297211830048123\n",
      " train data hamming_loss  : 0.016474809388548364\n",
      " train data precision recall f1  : (0.9262096975133303, 0.9349429411471569, 0.9257665392364153, None)\n"
     ]
    }
   ],
   "source": [
    "thr = 0.6\n",
    "#out1 = thr * (0.1 * train[0] + 0.3 * train[1] + 0.6 * train[2])\n",
    "#out2 = (1 - thr) * (1.0 * train[3])\n",
    "thr0 = 0; thr1 = 0.4; thr2 = 0.; thr3 = 0.0; thr4 = 0.; thr5 = 0.6;\n",
    "train_y = thr0*train[0] + thr1*train[1] + thr2*train[2] + thr3*train[3] + thr4*train[4] + thr5*train[5]\n",
    "\n",
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(train_y.shape[1])\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "    y_prob = np.array(train_y[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if train_y[i, j] >= best_threshold[j] else 0 for j in range(train_y.shape[1])]\n",
    "          for i in range(len(train_y))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "print(\" train data hamming_loss  :\", hamming_loss(y_train, y_pred)) \n",
    "print(\" train data precision recall f1  :\", prf(y_train, y_pred,average=\"samples\"))# 'micro', 'weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0.6, 0.5, 0.5, 0.6, 0.2, 0.2, 0.7, 0.5, 0.4])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = thr0 * test[0]# + thr1 * test[1] + thr2 * test[2]# + thr3 * test[3] + thr4 * test[4] + thr5 * test[5]\n",
    "\n",
    "y_pred_test = np.array(\n",
    "    [[1 if out[i, j] >= best_threshold[j] else 0 for j in range(out.shape[1])] for i in range(len(out))])\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "test_y = y_pred_test\n",
    "\n",
    "y_pred = [[1 if test_y[i, j] >= best_threshold[j] else 0 for j in range(test_y.shape[1])]\n",
    "          for i in range(len(test_y))]\n",
    "pred = []\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "    \n",
    "for i, val in enumerate(pred):\n",
    "    ''' \n",
    "    if 0 in val and len(val) > 1:\n",
    "        flag = 0\n",
    "        for j in val:\n",
    "            if (test_y[i][0] - best_threshold[0]) > (test_y[i][j] - best_threshold[j]):\n",
    "                pass\n",
    "            else:\n",
    "                flag = 1\n",
    "        if flag == 1:\n",
    "            pred[i] = val[1:]\n",
    "        else:\n",
    "            pred[i] = val[0]\n",
    "    '''\n",
    "    if len(val) == 0:\n",
    "        pred[i] = [np.argmin(np.abs(best_threshold - out_test[i]))]\n",
    "        \n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('jupyter_answers_densenet_{}_0803.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(10 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.833 and 0.842 ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train data f1_score  : 0.9211840609081451\n",
      "f1 score of ab 0 is 0.9603084096823336\n",
      "f1 score of ab 1 is 0.9793386425479889\n",
      "f1 score of ab 2 is 0.9513702179183157\n",
      "f1 score of ab 3 is 0.9822910749256628\n",
      "f1 score of ab 4 is 0.9444799508628059\n",
      "f1 score of ab 5 is 0.9696737288321967\n",
      "f1 score of ab 6 is 0.9584407222873685\n",
      "f1 score of ab 7 is 0.9350094053104944\n",
      "f1 score of ab 8 is 0.9451176555412449\n",
      "f1 score of ab 9 is 0.930177490596017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(penalty=\"l2\",C=1.0)\n",
    "\n",
    "\n",
    "pre_type = \"db6\"#\"db6\"#\"sym\"\n",
    "labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "\n",
    "index = np.arange(6689)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "x_train = np.hstack([train_y,y_pred_proba_train])\n",
    "\n",
    "clf = OneVsRestClassifier(LR)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = 0.4 * test[0] + 0.6 * test[1]\n",
    "x_test = np.hstack([out,y_pred_proba])\n",
    "y_pred = clf.predict(x_test)\n",
    "y_pred_proba = clf.predict_proba(x_test)\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "pred = []\n",
    "for j in range(y_pred.shape[0]):\n",
    "    pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "\n",
    "''' ''' \n",
    "for i, val in enumerate(pred):\n",
    "    if val == []:\n",
    "        pass\n",
    "        #for i_p, val_p in enumerate(y_pred_proba[i]):\n",
    "        #    if val_p >= 0.4:\n",
    "        #        pred[i].append(i_p)    # f1 == 0.832\n",
    "                \n",
    "        if y_pred_proba[i][np.argmax(y_pred_proba[i])] >= 0.4:\n",
    "            pred[i] = [np.argmax(y_pred_proba[i])]     # f1 == 0.833\n",
    "\n",
    "\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('jupyter_answers_densenet_{}_0806.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(10 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quarter-final data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quarter_labels= pd.read_csv(\"/media/uuser/data/final_codes/final_run_semi/reference.csv\")\n",
    "quarter_index = np.arange(6500)\n",
    "quarter_y_train = preprocess_y(quarter_labels, quarter_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train data f1_score  : 0.8439097952017833\n",
      "f1 score of ab 0 is 0.971215859308423\n",
      "f1 score of ab 1 is 0.9877104179896463\n",
      "f1 score of ab 2 is 0.9606090069979423\n",
      "f1 score of ab 3 is 0.9621594687564023\n",
      "f1 score of ab 4 is 0.9481268620677086\n",
      "f1 score of ab 5 is 0.9751797264318665\n",
      "f1 score of ab 6 is 0.9664972199135371\n",
      "f1 score of ab 7 is 0.9524740975127066\n",
      "f1 score of ab 8 is 0.9526784074522501\n",
      "f1 score of ab 9 is 0.49884348496530456\n",
      " train data hamming_loss  : 0.01376923076923077\n",
      " train data precision recall f1  : (0.9443076923076923, 0.9599820512820513, 0.9466612942612942, None)\n"
     ]
    }
   ],
   "source": [
    "quarter_train = []\n",
    "path = \"./\"\n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_one_net_fold2.csv\").values) \n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_f0819_10net_fold.csv\").values) \n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_attention_10net_fold2.csv\").values) \n",
    "\n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_4block_10net_fold0.csv\").values)\n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_4block_10net_fold1.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_4block_10net_fold2.csv\").values)\n",
    "\n",
    "#quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_one_net_fold0.csv\").values)\n",
    "#quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_one_net_fold1.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_one_net_fold2.csv\").values)\n",
    "\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_10net_fold0.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_10net_fold1.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_10net_fold2.csv\").values)\n",
    "\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_f0819_10net_fold.csv\").values)\n",
    "\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_10net_sym_addori_fold2.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_10net_db6_addori_fold2.csv\").values)\n",
    "\n",
    "''' '''\n",
    "# out = np.hstack(test)#\n",
    "out0 = np.hstack(quarter_train[:-1])#\n",
    "out1 = np.hstack(quarter_train[:-2])\n",
    "out2 = np.hstack(quarter_train[:-2]+[quarter_train[-1]])\n",
    "\n",
    "# LR_clf = joblib.load(\"LR_ensemble0810.pkl\")\n",
    "# MLkNN8_clf = joblib.load(\"MLkNN8_ensemble.pkl\")\n",
    "# MLkNN10_clf = joblib.load(\"MLkNN10_ensemble.pkl\")\n",
    "# MLkNN10_db6_clf = joblib.load(\"MLkNN10_ensemble_db6_0810.pkl\")\n",
    "\n",
    "y_pred_LR = LR_clf.predict(out0)\n",
    "y_pred_proba_LR = LR_clf.predict_proba(out0)\n",
    "\n",
    "y_pred_MLkNN8 = MLkNN8_clf.predict(out1).toarray()\n",
    "y_pred_proba_MLkNN8 = MLkNN8_clf.predict_proba(out1).toarray()\n",
    "\n",
    "y_pred_MLkNN10 = MLkNN10_clf.predict(out1).toarray()\n",
    "y_pred_proba_MLkNN10 = MLkNN10_clf.predict_proba(out1).toarray()\n",
    "\n",
    "y_pred_MLkNN10_db6 = MLkNN10_db6_clf.predict(out2).toarray()\n",
    "y_pred_proba_MLkNN10_db6 = MLkNN10_db6_clf.predict_proba(out2).toarray()\n",
    "\n",
    "y_pred_LR[:,2] = y_pred_MLkNN8[:,2]\n",
    "# y_pred_proba_LR[:,2] = y_pred_proba_MLkNN8[:,2]\n",
    "\n",
    "y_pred_LR[:,5] = y_pred_MLkNN8[:,5]\n",
    "# y_pred_proba_LR[:,5] = y_pred_proba_MLkNN8[:,5]\n",
    "\n",
    "y_pred_LR[:,6] = y_pred_MLkNN8[:,6]\n",
    "# y_pred_proba_LR[:,6] = y_pred_proba_MLkNN8[:,6]\n",
    "\n",
    "y_pred_LR[:,9] = y_pred_MLkNN8[:,9]\n",
    "# y_pred_proba_LR[:,9] = y_pred_proba_MLkNN8[:,9]\n",
    "\n",
    "y_pred_LR[:,4] = y_pred_MLkNN10[:,4]\n",
    "# y_pred_proba_LR[:,4] = y_pred_proba_MLkNN10[:,4]\n",
    "\n",
    "y_pred_LR[:,8] = y_pred_MLkNN10[:,8]\n",
    "# y_pred_proba_LR[:,8] = y_pred_proba_MLkNN10[:,8]\n",
    "\n",
    "y_pred_LR[:,7] = y_pred_MLkNN10_db6[:,7]\n",
    "# y_pred_proba_LR[:,7] = y_pred_proba_MLkNN10_db6[:,7]\n",
    "\n",
    "\n",
    "quarter_y_pred = y_pred_LR\n",
    "# quarter_y_pred_proba = y_pred_proba_LR\n",
    "\n",
    "# quarter_x_train = np.hstack(quarter_train)\n",
    "# quarter_y_pred = clf.predict(quarter_x_train).toarray()\n",
    "\n",
    "print(\" train data f1_score  :\", f1_score(quarter_y_train, quarter_y_pred, average='macro'))\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(quarter_y_train[:, i], quarter_y_pred[:, i], average='macro')))\n",
    "    \n",
    "print(\" train data hamming_loss  :\", hamming_loss(quarter_y_train, quarter_y_pred)) \n",
    "print(\" train data precision recall f1  :\", prf(quarter_y_train, quarter_y_pred,average=\"samples\"))# 'micro', 'weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6276, 1: 224})"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(quarter_y_train[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6276, 1: 224})"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(quarter_y_pred[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8732"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.843 * 10 - 0.498 + 0.8) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " train data f1_score  : 0.9226479088215832\n",
    "f1 score of ab 0 is 0.9576243119144202\n",
    "f1 score of ab 1 is 0.9794953051205606\n",
    "f1 score of ab 2 is 0.951682745472497\n",
    "f1 score of ab 3 is 0.982355844958436\n",
    "f1 score of ab 4 is 0.9477985152051004\n",
    "f1 score of ab 5 is 0.9721827424838709\n",
    "f1 score of ab 6 is 0.958814994471973\n",
    "f1 score of ab 7 is 0.9415063228088438\n",
    "f1 score of ab 8 is 0.9398197061954244\n",
    "f1 score of ab 9 is 0.9297211830048123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
