{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "''' '''\n",
    "# from resnet_ecg.utils import one_hot,get_batches\n",
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "from resnet_ecg import attentionmodel\n",
    "from resnet_ecg.densemodel import Net\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, load_model\n",
    "import keras\n",
    "import pywt\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "'''\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "'''\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "\n",
    "# path of training data\n",
    "path = '/media/jdcloud/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.conv_subsample_lengths = [1, 2, 1, 2, 1, 2, 1, 2]\n",
    "        self.conv_filter_length = 32\n",
    "        self.conv_num_filters_start = 12\n",
    "        self.conv_init = \"he_normal\"\n",
    "        self.conv_activation = \"relu\"\n",
    "        self.conv_dropout = 0.5\n",
    "        self.conv_num_skip = 2\n",
    "        self.conv_increase_channels_at = 2\n",
    "        self.batch_size = 32  # 128\n",
    "        self.input_shape = [2560, 12]  # [1280, 1]\n",
    "        self.num_categories = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 0.1\n",
    "        if epoch >= 10 and epoch < 20:\n",
    "            lr = 0.01\n",
    "        if epoch >= 20:\n",
    "            lr = 0.001\n",
    "        # print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "def wavelet(ecg, wavefunc, lv, m, n):  #\n",
    "\n",
    "    coeff = pywt.wavedec(ecg, wavefunc, mode='sym', level=lv)  #\n",
    "    # sgn = lambda x: 1 if x > 0 else -1 if x < 0 else 0\n",
    "\n",
    "    for i in range(m, n + 1):\n",
    "        cD = coeff[i]\n",
    "        for j in range(len(cD)):\n",
    "            Tr = np.sqrt(2 * np.log(len(cD)))\n",
    "            if cD[j] >= Tr:\n",
    "                coeff[i][j] = np.sign(cD[j]) - Tr\n",
    "            else:\n",
    "                coeff[i][j] = 0\n",
    "\n",
    "    denoised_ecg = pywt.waverec(coeff, wavefunc)\n",
    "    return denoised_ecg\n",
    "\n",
    "\n",
    "def wavelet_db6(sig):\n",
    "    \"\"\"\n",
    "    R J, Acharya U R, Min L C. ECG beat classification using PCA, LDA, ICA and discrete\n",
    "     wavelet transform[J].Biomedical Signal Processing and Control, 2013, 8(5): 437-448.\n",
    "    param sig: 1-D numpy Array\n",
    "    return: 1-D numpy Array\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(sig, 'db6', level=9)\n",
    "    coeffs[-1] = np.zeros(len(coeffs[-1]))\n",
    "    coeffs[-2] = np.zeros(len(coeffs[-2]))\n",
    "    coeffs[0] = np.zeros(len(coeffs[0]))\n",
    "    sig_filt = pywt.waverec(coeffs, 'db6')\n",
    "    return sig_filt\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "def read_data_seg(data_path, split=\"Train\", preprocess=False, fs=500, newFs=256, winSecond=10, winNum=10, n_index=0,pre_type=\"sym\"):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    # n_index = 0\n",
    "    n_class = 10\n",
    "    winSize = winSecond * fs\n",
    "    new_winSize = winSecond * newFs\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    # label_path = os.path.join(data_path, \"reference.txt\")\n",
    "    # labels = pd.read_csv(label_path, sep='\\t',header = None)\n",
    "    # labels = pd.read_csv(\"reference.csv\")\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    # print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12  # len(channel_files)\n",
    "    # posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "\n",
    "    X = np.zeros((len(channel_files), new_winSize, n_channels)).astype('float32')\n",
    "    i_ch = 0\n",
    "\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1', 'II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II', 'aVR', 'V2', 'V5']\n",
    "    channel_post_name = ['III', 'aVF', 'V3', 'V6']\n",
    "\n",
    "    for i_ch, fil_ch in enumerate(channel_files[:]):  # tqdm\n",
    "\n",
    "        if i_ch % 1000 == 0:\n",
    "            print(i_ch)\n",
    "\n",
    "        ecg = sio.loadmat(os.path.join(path_signals, fil_ch))\n",
    "        ecg_length = ecg[\"I\"].shape[1]\n",
    "\n",
    "        if ecg_length > fs * winNum * winSecond:\n",
    "            print(\" too long !!!\", ecg_length)\n",
    "            ecg_length = fs * winNum * winSecond\n",
    "        if ecg_length < 4500:\n",
    "            print(\" too short !!!\", ecg_length)\n",
    "            break\n",
    "\n",
    "        slide_steps = int((ecg_length - winSize) / winSecond)\n",
    "\n",
    "        if ecg_length <= 4500:\n",
    "            slide_steps = 0\n",
    "\n",
    "        ecg_channels = np.zeros((new_winSize, n_channels)).astype('float32')\n",
    "\n",
    "        for i_n, ch_name in enumerate(channel_name):\n",
    "\n",
    "            ecg_channels[:, i_n] = signal.resample(ecg[ch_name]\n",
    "                                                   [:, n_index * slide_steps:n_index * slide_steps + winSize].T\n",
    "                                                   , new_winSize).T\n",
    "            if preprocess:\n",
    "                if pre_type == \"sym\":\n",
    "                    ecg_channels[:, i_n] = ecg_preprocessing(ecg_channels[:, i_n].reshape(1, new_winSize), 'sym8', 8, 3,\n",
    "                                                             newFs, removebaseline=False, normalize=False)[0]\n",
    "                elif pre_type == \"db4\":\n",
    "                    ecg_channels[:, i_n] = wavelet(ecg_channels[:, i_n], 'db4', 4, 2, 4)\n",
    "                elif pre_type == \"db6\":\n",
    "                    ecg_channels[:, i_n] = wavelet_db6(ecg_channels[:, i_n])\n",
    "\n",
    "                # ecg_channels[:, i_n] = (ecg_channels[:, i_n]-np.mean(ecg_channels[:, i_n]))/np.std(ecg_channels[:, i_n])\n",
    "            else:\n",
    "                pass\n",
    "                print(\" no preprocess !!! \")\n",
    "\n",
    "        X[i_ch, :, :] = ecg_channels\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_y(labels, y, num_class=10):\n",
    "    bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1, label_nona.shape[0]):\n",
    "            bin_label[i, int(label_nona[j])] = 1\n",
    "    return bin_label\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    # ' Generates data for Keras '\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        # 'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # 'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # 'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # 'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size,  *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.n_classes), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load(\"training_data/\" + ID+\".npy\")\n",
    "            # Store class\n",
    "            y[i,:] = preprocess_y(self.labels,self.labels[self.labels[\"File_name\"] == ID.split(\"_\")[0]].index)\n",
    "\n",
    "        # X_list = [(X[:, i]-np.mean(X[:, i]))/np.std(X[:, i]) for i in range(10)]\n",
    "        X_list = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n",
    "        del X\n",
    "\n",
    "        return X_list, y  # keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "def add_compile(model, config):\n",
    "    optimizer = SGD(lr=config.lr_schedule(0), momentum=0.9)  # Adam()#\n",
    "    model.compile(loss='binary_crossentropy',  # weighted_loss,#'binary_crossentropy',\n",
    "                  optimizer='adam',  # optimizer,#'adam',\n",
    "                  metrics=['accuracy', fmeasure, precision])  # recall\n",
    "    # ['accuracy',fbetaMacro,recallMacro,precisionMacro])\n",
    "    # ['accuracy',fmeasure,recall,precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "fold:  1  training\n",
      "tr_IDs :  21048\n",
      "val_IDs :  8932\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = path + \"/Train/\"\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "\n",
    "train_files = os.listdir(train_dataset_path)\n",
    "train_files.sort()\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "labels_en = pd.read_csv(path + \"kfold_labels_en.csv\")\n",
    "#data_info = pd.read_csv(path + \"data_info.csv\")\n",
    "\n",
    "input_size = (2560, 12)\n",
    "net_num = 10\n",
    "inputs_list = [Input(shape=input_size) for _ in range(net_num)]\n",
    "net = Net()\n",
    "outputs = net.nnet(inputs_list, 0.5, num_classes=10 , attention=False)\n",
    "model = Model(inputs=inputs_list, outputs=outputs)\n",
    "# print(model.summary())\n",
    "\n",
    "raw_IDs = labels_en[\"File_name\"].values.tolist()\n",
    "extend_db4_IDs = [i + \"_db4\" for i in raw_IDs]\n",
    "extend_db6_IDs = [i + \"_db6\" for i in raw_IDs]\n",
    "extend_ori_IDs = [i + \"_ori\" for i in raw_IDs]\n",
    "all_IDs = raw_IDs + extend_db4_IDs + extend_db6_IDs+extend_ori_IDs\n",
    "\n",
    "train_labels = labels_en[\"label1\"].values\n",
    "all_train_labels = np.hstack((train_labels, train_labels, train_labels))\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': (10, 2560),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 10,\n",
    "          'n_channels': 12,\n",
    "          'shuffle': True}\n",
    "\n",
    "en_amount = 1\n",
    "model_path = './official_densenet_model/'\n",
    "\n",
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 3\n",
    "    n_classes = 10\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1234)\n",
    "    # kf = kfold.split(all_IDs, all_train_labels)\n",
    "    kf = kfold.split(labels[\"File_name\"].values.tolist(), labels[\"label1\"].values)\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i + 1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        #tr_IDs = np.array(all_IDs)[index_train]\n",
    "        #val_IDs = np.array(all_IDs)[index_valid]\n",
    "        #print(tr_IDs.shape)\n",
    "        tr_IDs = labels[\"File_name\"].values[index_train].tolist() \n",
    "        val_IDs = labels[\"File_name\"].values[index_valid].tolist()\n",
    "\n",
    "        for j in range(4):\n",
    "            for ids in labels[labels.label1==4][\"File_name\"]:\n",
    "                if ids in tr_IDs:\n",
    "                    tr_IDs.append(ids)\n",
    "\n",
    "        for j in range(2):\n",
    "            for ids in labels[labels.label1==7][\"File_name\"]:\n",
    "                if ids in tr_IDs:\n",
    "                    tr_IDs.append(ids)\n",
    "\n",
    "        for j in range(1):\n",
    "            for ids in labels[labels.label1==9][\"File_name\"]:\n",
    "                if ids in tr_IDs:\n",
    "                    tr_IDs.append(ids)\n",
    "\n",
    "        tr_IDs_db4 = [ids+\"_db4\" for ids in tr_IDs]\n",
    "        tr_IDs_db6 = [ids+\"_db6\" for ids in tr_IDs]\n",
    "        tr_IDs_ori = [ids+\"_ori\" for ids in tr_IDs]\n",
    "\n",
    "        val_IDs_db4 = [ids+\"_db4\" for ids in val_IDs]\n",
    "        val_IDs_db6 = [ids+\"_db6\" for ids in val_IDs]\n",
    "        val_IDs_ori = [ids+\"_ori\" for ids in val_IDs]\n",
    "\n",
    "        tr_IDs = tr_IDs+ tr_IDs_db4 + tr_IDs_db6 + tr_IDs_ori\n",
    "        val_IDs = val_IDs + val_IDs_db4 + val_IDs_db6 + val_IDs_ori\n",
    "        print(\"tr_IDs : \",len(tr_IDs))\n",
    "        print(\"val_IDs : \",len(val_IDs))\n",
    "\n",
    "\n",
    "        # Generators\n",
    "        training_generator = DataGenerator(tr_IDs, labels, **params)\n",
    "        validation_generator = DataGenerator(val_IDs, labels, **params)\n",
    "\n",
    "        checkpointer = ModelCheckpoint(filepath=model_path + 'densenet_extend_weights-best_k{}_r{}_0809_30.hdf5'.format(seed, i),\n",
    "                                       monitor='val_fmeasure', verbose=1, save_best_only=True,\n",
    "                                       save_weights_only=True,\n",
    "                                       mode='max')  # val_fmeasure\n",
    "        reduce = ReduceLROnPlateau(monitor='val_fmeasure', factor=0.5, patience=2, verbose=1, min_delta=1e-5,\n",
    "                                   mode='max')\n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_fmeasure', patience=10)\n",
    "\n",
    "        config = Config()\n",
    "        add_compile(model, config)\n",
    "\n",
    "        callback_lists = [checkpointer, reduce]\n",
    "\n",
    "        history = model.fit_generator(generator=training_generator,\n",
    "                                      validation_data=validation_generator,\n",
    "                                      use_multiprocessing=False,\n",
    "                                      epochs=30, # 50\n",
    "                                      verbose=1,\n",
    "                                      callbacks=callback_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
