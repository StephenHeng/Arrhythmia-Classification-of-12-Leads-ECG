{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "''' '''\n",
    "# from resnet_ecg.utils import one_hot,get_batches\n",
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "from resnet_ecg import attentionmodel\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, load_model\n",
    "import keras\n",
    "import pywt\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "'''\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "'''\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "\n",
    "# path of training data\n",
    "path = '/media/jdcloud/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.conv_subsample_lengths = [1, 2, 1, 2, 1, 2, 1, 2]\n",
    "        self.conv_filter_length = 32\n",
    "        self.conv_num_filters_start = 12\n",
    "        self.conv_init = \"he_normal\"\n",
    "        self.conv_activation = \"relu\"\n",
    "        self.conv_dropout = 0.5\n",
    "        self.conv_num_skip = 2\n",
    "        self.conv_increase_channels_at = 2\n",
    "        self.batch_size = 32  # 128\n",
    "        self.input_shape = [2560, 12]  # [1280, 1]\n",
    "        self.num_categories = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 0.1\n",
    "        if epoch >= 10 and epoch < 20:\n",
    "            lr = 0.01\n",
    "        if epoch >= 20:\n",
    "            lr = 0.001\n",
    "        # print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "def wavelet(ecg, wavefunc, lv, m, n):  #\n",
    "\n",
    "    coeff = pywt.wavedec(ecg, wavefunc, mode='sym', level=lv)  #\n",
    "    # sgn = lambda x: 1 if x > 0 else -1 if x < 0 else 0\n",
    "\n",
    "    for i in range(m, n + 1):\n",
    "        cD = coeff[i]\n",
    "        for j in range(len(cD)):\n",
    "            Tr = np.sqrt(2 * np.log(len(cD)))\n",
    "            if cD[j] >= Tr:\n",
    "                coeff[i][j] = np.sign(cD[j]) - Tr\n",
    "            else:\n",
    "                coeff[i][j] = 0\n",
    "\n",
    "    denoised_ecg = pywt.waverec(coeff, wavefunc)\n",
    "    return denoised_ecg\n",
    "\n",
    "\n",
    "def wavelet_db6(sig):\n",
    "    \"\"\"\n",
    "    R J, Acharya U R, Min L C. ECG beat classification using PCA, LDA, ICA and discrete\n",
    "     wavelet transform[J].Biomedical Signal Processing and Control, 2013, 8(5): 437-448.\n",
    "    param sig: 1-D numpy Array\n",
    "    return: 1-D numpy Array\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(sig, 'db6', level=9)\n",
    "    coeffs[-1] = np.zeros(len(coeffs[-1]))\n",
    "    coeffs[-2] = np.zeros(len(coeffs[-2]))\n",
    "    coeffs[0] = np.zeros(len(coeffs[0]))\n",
    "    sig_filt = pywt.waverec(coeffs, 'db6')\n",
    "    return sig_filt\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "def read_data_seg(data_path, split=\"Train\", preprocess=False, fs=500, newFs=256, winSecond=10, winNum=10, n_index=0,pre_type=\"sym\"):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    # n_index = 0\n",
    "    n_class = 10\n",
    "    winSize = winSecond * fs\n",
    "    new_winSize = winSecond * newFs\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    # label_path = os.path.join(data_path, \"reference.txt\")\n",
    "    # labels = pd.read_csv(label_path, sep='\\t',header = None)\n",
    "    # labels = pd.read_csv(\"reference.csv\")\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    # print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12  # len(channel_files)\n",
    "    # posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "\n",
    "    X = np.zeros((len(channel_files), new_winSize, n_channels)).astype('float32')\n",
    "    i_ch = 0\n",
    "\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1', 'II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II', 'aVR', 'V2', 'V5']\n",
    "    channel_post_name = ['III', 'aVF', 'V3', 'V6']\n",
    "\n",
    "    for i_ch, fil_ch in enumerate(channel_files[:]):  # tqdm\n",
    "\n",
    "        if i_ch % 1000 == 0:\n",
    "            print(i_ch)\n",
    "\n",
    "        ecg = sio.loadmat(os.path.join(path_signals, fil_ch))\n",
    "        ecg_length = ecg[\"I\"].shape[1]\n",
    "\n",
    "        if ecg_length > fs * winNum * winSecond:\n",
    "            print(\" too long !!!\", ecg_length)\n",
    "            ecg_length = fs * winNum * winSecond\n",
    "        if ecg_length < 4500:\n",
    "            print(\" too short !!!\", ecg_length)\n",
    "            break\n",
    "\n",
    "        slide_steps = int((ecg_length - winSize) / winSecond)\n",
    "\n",
    "        if ecg_length <= 4500:\n",
    "            slide_steps = 0\n",
    "\n",
    "        ecg_channels = np.zeros((new_winSize, n_channels)).astype('float32')\n",
    "\n",
    "        for i_n, ch_name in enumerate(channel_name):\n",
    "\n",
    "            ecg_channels[:, i_n] = signal.resample(ecg[ch_name]\n",
    "                                                   [:, n_index * slide_steps:n_index * slide_steps + winSize].T\n",
    "                                                   , new_winSize).T\n",
    "            if preprocess:\n",
    "                if pre_type == \"sym\":\n",
    "                    ecg_channels[:, i_n] = ecg_preprocessing(ecg_channels[:, i_n].reshape(1, new_winSize), 'sym8', 8, 3,\n",
    "                                                             newFs, removebaseline=False, normalize=False)[0]\n",
    "                elif pre_type == \"db4\":\n",
    "                    ecg_channels[:, i_n] = wavelet(ecg_channels[:, i_n], 'db4', 4, 2, 4)\n",
    "                elif pre_type == \"db6\":\n",
    "                    ecg_channels[:, i_n] = wavelet_db6(ecg_channels[:, i_n])\n",
    "\n",
    "                # ecg_channels[:, i_n] = (ecg_channels[:, i_n]-np.mean(ecg_channels[:, i_n]))/np.std(ecg_channels[:, i_n])\n",
    "            else:\n",
    "                pass\n",
    "                print(\" no preprocess !!! \")\n",
    "\n",
    "        X[i_ch, :, :] = ecg_channels\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_y(labels, y, num_class=10):\n",
    "    bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1, label_nona.shape[0]):\n",
    "            bin_label[i, int(label_nona[j])] = 1\n",
    "    return bin_label\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    # ' Generates data for Keras '\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        # 'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # 'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # 'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # 'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size,  *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.n_classes), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load(\"training_data/\" + ID+\".npy\")\n",
    "            # Store class\n",
    "            y[i,:] = preprocess_y(self.labels,self.labels[self.labels[\"File_name\"] == ID.split(\"_\")[0]].index)\n",
    "\n",
    "        # X_list = [(X[:, i]-np.mean(X[:, i]))/np.std(X[:, i]) for i in range(10)]\n",
    "        X_list = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n",
    "        del X\n",
    "\n",
    "        return X_list, y  # keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "def add_compile(model, config):\n",
    "    optimizer = SGD(lr=config.lr_schedule(0), momentum=0.9)  # Adam()#\n",
    "    model.compile(loss='binary_crossentropy',  # weighted_loss,#'binary_crossentropy',\n",
    "                  optimizer='adam',  # optimizer,#'adam',\n",
    "                  metrics=['accuracy', fmeasure, precision])  # recall\n",
    "    # ['accuracy',fbetaMacro,recallMacro,precisionMacro])\n",
    "    # ['accuracy',fmeasure,recall,precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "fold:  1  training\n",
      "tr_IDs :  21048\n",
      "val_IDs :  8932\n",
      "Epoch 1/30\n",
      "328/328 [==============================] - 272s 829ms/step - loss: 0.3685 - acc: 0.8625 - fmeasure: 0.3060 - precision: 0.4859 - val_loss: 0.2568 - val_acc: 0.9026 - val_fmeasure: 0.3197 - val_precision: 0.9417\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.31972, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 2/30\n",
      "328/328 [==============================] - 146s 446ms/step - loss: 0.2661 - acc: 0.9078 - fmeasure: 0.5041 - precision: 0.6982 - val_loss: 0.1819 - val_acc: 0.9350 - val_fmeasure: 0.6935 - val_precision: 0.7922\n",
      "\n",
      "Epoch 00002: val_fmeasure improved from 0.31972 to 0.69353, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 3/30\n",
      "328/328 [==============================] - 146s 444ms/step - loss: 0.2273 - acc: 0.9202 - fmeasure: 0.5997 - precision: 0.7500 - val_loss: 0.1677 - val_acc: 0.9358 - val_fmeasure: 0.6922 - val_precision: 0.8057\n",
      "\n",
      "Epoch 00003: val_fmeasure did not improve from 0.69353\n",
      "Epoch 4/30\n",
      "328/328 [==============================] - 150s 458ms/step - loss: 0.2049 - acc: 0.9255 - fmeasure: 0.6357 - precision: 0.7633 - val_loss: 0.1586 - val_acc: 0.9433 - val_fmeasure: 0.7192 - val_precision: 0.8752\n",
      "\n",
      "Epoch 00004: val_fmeasure improved from 0.69353 to 0.71919, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 5/30\n",
      "328/328 [==============================] - 149s 456ms/step - loss: 0.1933 - acc: 0.9288 - fmeasure: 0.6574 - precision: 0.7693 - val_loss: 0.1520 - val_acc: 0.9448 - val_fmeasure: 0.7347 - val_precision: 0.8580\n",
      "\n",
      "Epoch 00005: val_fmeasure improved from 0.71919 to 0.73474, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 6/30\n",
      "328/328 [==============================] - 150s 458ms/step - loss: 0.1756 - acc: 0.9349 - fmeasure: 0.6929 - precision: 0.7911 - val_loss: 0.1360 - val_acc: 0.9500 - val_fmeasure: 0.7705 - val_precision: 0.8513\n",
      "\n",
      "Epoch 00006: val_fmeasure improved from 0.73474 to 0.77051, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 7/30\n",
      "328/328 [==============================] - 151s 460ms/step - loss: 0.1669 - acc: 0.9384 - fmeasure: 0.7105 - precision: 0.8015 - val_loss: 0.1235 - val_acc: 0.9544 - val_fmeasure: 0.7902 - val_precision: 0.8733\n",
      "\n",
      "Epoch 00007: val_fmeasure improved from 0.77051 to 0.79016, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 8/30\n",
      "328/328 [==============================] - 155s 473ms/step - loss: 0.1607 - acc: 0.9411 - fmeasure: 0.7270 - precision: 0.8100 - val_loss: 0.1159 - val_acc: 0.9572 - val_fmeasure: 0.8064 - val_precision: 0.8731\n",
      "\n",
      "Epoch 00008: val_fmeasure improved from 0.79016 to 0.80640, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 9/30\n",
      "328/328 [==============================] - 171s 520ms/step - loss: 0.1528 - acc: 0.9435 - fmeasure: 0.7373 - precision: 0.8128 - val_loss: 0.1222 - val_acc: 0.9557 - val_fmeasure: 0.7939 - val_precision: 0.8885\n",
      "\n",
      "Epoch 00009: val_fmeasure did not improve from 0.80640\n",
      "Epoch 10/30\n",
      "328/328 [==============================] - 170s 518ms/step - loss: 0.1510 - acc: 0.9440 - fmeasure: 0.7415 - precision: 0.8202 - val_loss: 0.1177 - val_acc: 0.9573 - val_fmeasure: 0.8122 - val_precision: 0.8529\n",
      "\n",
      "Epoch 00010: val_fmeasure improved from 0.80640 to 0.81221, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 11/30\n",
      "328/328 [==============================] - 185s 563ms/step - loss: 0.1417 - acc: 0.9473 - fmeasure: 0.7606 - precision: 0.8292 - val_loss: 0.1382 - val_acc: 0.9505 - val_fmeasure: 0.7736 - val_precision: 0.8492\n",
      "\n",
      "Epoch 00011: val_fmeasure did not improve from 0.81221\n",
      "Epoch 12/30\n",
      "328/328 [==============================] - 186s 566ms/step - loss: 0.1386 - acc: 0.9488 - fmeasure: 0.7661 - precision: 0.8317 - val_loss: 0.1171 - val_acc: 0.9600 - val_fmeasure: 0.8200 - val_precision: 0.8830\n",
      "\n",
      "Epoch 00012: val_fmeasure improved from 0.81221 to 0.82005, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 13/30\n",
      "328/328 [==============================] - 189s 576ms/step - loss: 0.1343 - acc: 0.9504 - fmeasure: 0.7713 - precision: 0.8405 - val_loss: 0.1154 - val_acc: 0.9583 - val_fmeasure: 0.8146 - val_precision: 0.8639\n",
      "\n",
      "Epoch 00013: val_fmeasure did not improve from 0.82005\n",
      "Epoch 14/30\n",
      "328/328 [==============================] - 185s 565ms/step - loss: 0.1317 - acc: 0.9513 - fmeasure: 0.7800 - precision: 0.8419 - val_loss: 0.1194 - val_acc: 0.9589 - val_fmeasure: 0.8123 - val_precision: 0.8906\n",
      "\n",
      "Epoch 00014: val_fmeasure did not improve from 0.82005\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 15/30\n",
      "328/328 [==============================] - 191s 583ms/step - loss: 0.1151 - acc: 0.9578 - fmeasure: 0.8114 - precision: 0.8617 - val_loss: 0.1111 - val_acc: 0.9603 - val_fmeasure: 0.8253 - val_precision: 0.8676\n",
      "\n",
      "Epoch 00015: val_fmeasure improved from 0.82005 to 0.82534, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 16/30\n",
      "328/328 [==============================] - 197s 599ms/step - loss: 0.1125 - acc: 0.9585 - fmeasure: 0.8147 - precision: 0.8675 - val_loss: 0.1081 - val_acc: 0.9619 - val_fmeasure: 0.8318 - val_precision: 0.8763\n",
      "\n",
      "Epoch 00016: val_fmeasure improved from 0.82534 to 0.83176, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 17/30\n",
      "328/328 [==============================] - 202s 616ms/step - loss: 0.1098 - acc: 0.9595 - fmeasure: 0.8177 - precision: 0.8682 - val_loss: 0.1123 - val_acc: 0.9615 - val_fmeasure: 0.8294 - val_precision: 0.8774\n",
      "\n",
      "Epoch 00017: val_fmeasure did not improve from 0.83176\n",
      "Epoch 18/30\n",
      "328/328 [==============================] - 203s 618ms/step - loss: 0.1080 - acc: 0.9600 - fmeasure: 0.8198 - precision: 0.8725 - val_loss: 0.1123 - val_acc: 0.9616 - val_fmeasure: 0.8297 - val_precision: 0.8801\n",
      "\n",
      "Epoch 00018: val_fmeasure did not improve from 0.83176\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 19/30\n",
      "328/328 [==============================] - 234s 715ms/step - loss: 0.1021 - acc: 0.9622 - fmeasure: 0.8319 - precision: 0.8792 - val_loss: 0.1055 - val_acc: 0.9644 - val_fmeasure: 0.8430 - val_precision: 0.8894\n",
      "\n",
      "Epoch 00019: val_fmeasure improved from 0.83176 to 0.84295, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 20/30\n",
      "328/328 [==============================] - 196s 599ms/step - loss: 0.1011 - acc: 0.9627 - fmeasure: 0.8347 - precision: 0.8803 - val_loss: 0.1073 - val_acc: 0.9642 - val_fmeasure: 0.8421 - val_precision: 0.8876\n",
      "\n",
      "Epoch 00020: val_fmeasure did not improve from 0.84295\n",
      "Epoch 21/30\n",
      "328/328 [==============================] - 199s 606ms/step - loss: 0.0990 - acc: 0.9636 - fmeasure: 0.8374 - precision: 0.8856 - val_loss: 0.1065 - val_acc: 0.9647 - val_fmeasure: 0.8445 - val_precision: 0.8891\n",
      "\n",
      "Epoch 00021: val_fmeasure improved from 0.84295 to 0.84450, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 22/30\n",
      "328/328 [==============================] - 196s 598ms/step - loss: 0.0943 - acc: 0.9654 - fmeasure: 0.8468 - precision: 0.8894 - val_loss: 0.1094 - val_acc: 0.9646 - val_fmeasure: 0.8440 - val_precision: 0.8873\n",
      "\n",
      "Epoch 00022: val_fmeasure did not improve from 0.84450\n",
      "Epoch 23/30\n",
      "328/328 [==============================] - 223s 679ms/step - loss: 0.0943 - acc: 0.9649 - fmeasure: 0.8454 - precision: 0.8873 - val_loss: 0.1050 - val_acc: 0.9650 - val_fmeasure: 0.8458 - val_precision: 0.8893\n",
      "\n",
      "Epoch 00023: val_fmeasure improved from 0.84450 to 0.84583, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 24/30\n",
      "328/328 [==============================] - 207s 631ms/step - loss: 0.0935 - acc: 0.9652 - fmeasure: 0.8460 - precision: 0.8874 - val_loss: 0.1059 - val_acc: 0.9660 - val_fmeasure: 0.8513 - val_precision: 0.8883\n",
      "\n",
      "Epoch 00024: val_fmeasure improved from 0.84583 to 0.85126, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 25/30\n",
      "328/328 [==============================] - 217s 661ms/step - loss: 0.0971 - acc: 0.9641 - fmeasure: 0.8391 - precision: 0.8856 - val_loss: 0.1126 - val_acc: 0.9645 - val_fmeasure: 0.8450 - val_precision: 0.8780\n",
      "\n",
      "Epoch 00025: val_fmeasure did not improve from 0.85126\n",
      "Epoch 26/30\n",
      "328/328 [==============================] - 235s 717ms/step - loss: 0.0892 - acc: 0.9672 - fmeasure: 0.8558 - precision: 0.8941 - val_loss: 0.1058 - val_acc: 0.9659 - val_fmeasure: 0.8503 - val_precision: 0.8918\n",
      "\n",
      "Epoch 00026: val_fmeasure did not improve from 0.85126\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 27/30\n",
      "328/328 [==============================] - 219s 669ms/step - loss: 0.0904 - acc: 0.9673 - fmeasure: 0.8541 - precision: 0.8905 - val_loss: 0.1061 - val_acc: 0.9663 - val_fmeasure: 0.8522 - val_precision: 0.8915\n",
      "\n",
      "Epoch 00027: val_fmeasure improved from 0.85126 to 0.85222, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 28/30\n",
      "328/328 [==============================] - 220s 670ms/step - loss: 0.0897 - acc: 0.9676 - fmeasure: 0.8550 - precision: 0.8940 - val_loss: 0.1065 - val_acc: 0.9658 - val_fmeasure: 0.8494 - val_precision: 0.8933\n",
      "\n",
      "Epoch 00028: val_fmeasure did not improve from 0.85222\n",
      "Epoch 29/30\n",
      "328/328 [==============================] - 151s 460ms/step - loss: 0.0867 - acc: 0.9680 - fmeasure: 0.8583 - precision: 0.8985 - val_loss: 0.1073 - val_acc: 0.9665 - val_fmeasure: 0.8542 - val_precision: 0.8879\n",
      "\n",
      "Epoch 00029: val_fmeasure improved from 0.85222 to 0.85418, saving model to ./official_attention_model/attention_extend_weights-best_k0_r0_0809_30.hdf5\n",
      "Epoch 30/30\n",
      "328/328 [==============================] - 148s 452ms/step - loss: 0.0810 - acc: 0.9702 - fmeasure: 0.8702 - precision: 0.9046 - val_loss: 0.1155 - val_acc: 0.9640 - val_fmeasure: 0.8412 - val_precision: 0.8861\n",
      "\n",
      "Epoch 00030: val_fmeasure did not improve from 0.85418\n",
      "fold:  2  training\n",
      "tr_IDs :  21068\n",
      "val_IDs :  8916\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 185s 563ms/step - loss: 0.1351 - acc: 0.9519 - fmeasure: 0.7855 - precision: 0.8406 - val_loss: 0.0801 - val_acc: 0.9692 - val_fmeasure: 0.8624 - val_precision: 0.9176\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.86241, saving model to ./official_attention_model/attention_extend_weights-best_k0_r1_0809_30.hdf5\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 145s 441ms/step - loss: 0.1238 - acc: 0.9550 - fmeasure: 0.7974 - precision: 0.8523 - val_loss: 0.0833 - val_acc: 0.9673 - val_fmeasure: 0.8532 - val_precision: 0.9138\n",
      "\n",
      "Epoch 00002: val_fmeasure did not improve from 0.86241\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 147s 446ms/step - loss: 0.1258 - acc: 0.9541 - fmeasure: 0.7909 - precision: 0.8478 - val_loss: 0.0843 - val_acc: 0.9687 - val_fmeasure: 0.8603 - val_precision: 0.9128\n",
      "\n",
      "Epoch 00003: val_fmeasure did not improve from 0.86241\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 148s 449ms/step - loss: 0.1104 - acc: 0.9599 - fmeasure: 0.8200 - precision: 0.8724 - val_loss: 0.0698 - val_acc: 0.9735 - val_fmeasure: 0.8836 - val_precision: 0.9251\n",
      "\n",
      "Epoch 00004: val_fmeasure improved from 0.86241 to 0.88360, saving model to ./official_attention_model/attention_extend_weights-best_k0_r1_0809_30.hdf5\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 147s 446ms/step - loss: 0.1035 - acc: 0.9619 - fmeasure: 0.8287 - precision: 0.8744 - val_loss: 0.0715 - val_acc: 0.9733 - val_fmeasure: 0.8831 - val_precision: 0.9209\n",
      "\n",
      "Epoch 00005: val_fmeasure did not improve from 0.88360\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 150s 456ms/step - loss: 0.1014 - acc: 0.9626 - fmeasure: 0.8346 - precision: 0.8806 - val_loss: 0.0777 - val_acc: 0.9714 - val_fmeasure: 0.8756 - val_precision: 0.9072\n",
      "\n",
      "Epoch 00006: val_fmeasure did not improve from 0.88360\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 152s 461ms/step - loss: 0.0946 - acc: 0.9649 - fmeasure: 0.8457 - precision: 0.8875 - val_loss: 0.0730 - val_acc: 0.9725 - val_fmeasure: 0.8802 - val_precision: 0.9103\n",
      "\n",
      "Epoch 00007: val_fmeasure did not improve from 0.88360\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 151s 459ms/step - loss: 0.0959 - acc: 0.9649 - fmeasure: 0.8425 - precision: 0.8919 - val_loss: 0.0702 - val_acc: 0.9739 - val_fmeasure: 0.8864 - val_precision: 0.9161\n",
      "\n",
      "Epoch 00008: val_fmeasure improved from 0.88360 to 0.88637, saving model to ./official_attention_model/attention_extend_weights-best_k0_r1_0809_30.hdf5\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 150s 455ms/step - loss: 0.0960 - acc: 0.9649 - fmeasure: 0.8428 - precision: 0.8920 - val_loss: 0.0721 - val_acc: 0.9730 - val_fmeasure: 0.8816 - val_precision: 0.9190\n",
      "\n",
      "Epoch 00009: val_fmeasure did not improve from 0.88637\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 149s 452ms/step - loss: 0.0861 - acc: 0.9685 - fmeasure: 0.8613 - precision: 0.9006 - val_loss: 0.0721 - val_acc: 0.9736 - val_fmeasure: 0.8859 - val_precision: 0.9082\n",
      "\n",
      "Epoch 00010: val_fmeasure did not improve from 0.88637\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 148s 451ms/step - loss: 0.0872 - acc: 0.9681 - fmeasure: 0.8567 - precision: 0.8964 - val_loss: 0.0715 - val_acc: 0.9740 - val_fmeasure: 0.8873 - val_precision: 0.9145\n",
      "\n",
      "Epoch 00011: val_fmeasure improved from 0.88637 to 0.88733, saving model to ./official_attention_model/attention_extend_weights-best_k0_r1_0809_30.hdf5\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 149s 453ms/step - loss: 0.0835 - acc: 0.9695 - fmeasure: 0.8659 - precision: 0.9042 - val_loss: 0.0711 - val_acc: 0.9740 - val_fmeasure: 0.8873 - val_precision: 0.9152\n",
      "\n",
      "Epoch 00012: val_fmeasure did not improve from 0.88733\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 149s 454ms/step - loss: 0.0886 - acc: 0.9675 - fmeasure: 0.8528 - precision: 0.8919 - val_loss: 0.0734 - val_acc: 0.9734 - val_fmeasure: 0.8846 - val_precision: 0.9101\n",
      "\n",
      "Epoch 00013: val_fmeasure did not improve from 0.88733\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 149s 454ms/step - loss: 0.0826 - acc: 0.9697 - fmeasure: 0.8666 - precision: 0.9052 - val_loss: 0.0717 - val_acc: 0.9744 - val_fmeasure: 0.8894 - val_precision: 0.9136\n",
      "\n",
      "Epoch 00014: val_fmeasure improved from 0.88733 to 0.88944, saving model to ./official_attention_model/attention_extend_weights-best_k0_r1_0809_30.hdf5\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 149s 453ms/step - loss: 0.0837 - acc: 0.9694 - fmeasure: 0.8628 - precision: 0.9056 - val_loss: 0.0724 - val_acc: 0.9740 - val_fmeasure: 0.8875 - val_precision: 0.9123\n",
      "\n",
      "Epoch 00015: val_fmeasure did not improve from 0.88944\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 144s 438ms/step - loss: 0.0843 - acc: 0.9696 - fmeasure: 0.8653 - precision: 0.9062 - val_loss: 0.0728 - val_acc: 0.9735 - val_fmeasure: 0.8847 - val_precision: 0.9161\n",
      "\n",
      "Epoch 00016: val_fmeasure did not improve from 0.88944\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 142s 432ms/step - loss: 0.0820 - acc: 0.9701 - fmeasure: 0.8658 - precision: 0.9012 - val_loss: 0.0731 - val_acc: 0.9735 - val_fmeasure: 0.8852 - val_precision: 0.9124\n",
      "\n",
      "Epoch 00017: val_fmeasure did not improve from 0.88944\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 148s 448ms/step - loss: 0.0790 - acc: 0.9709 - fmeasure: 0.8714 - precision: 0.9096 - val_loss: 0.0719 - val_acc: 0.9739 - val_fmeasure: 0.8866 - val_precision: 0.9143\n",
      "\n",
      "Epoch 00018: val_fmeasure did not improve from 0.88944\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 143s 433ms/step - loss: 0.0767 - acc: 0.9717 - fmeasure: 0.8765 - precision: 0.9100 - val_loss: 0.0719 - val_acc: 0.9742 - val_fmeasure: 0.8881 - val_precision: 0.9144\n",
      "\n",
      "Epoch 00019: val_fmeasure did not improve from 0.88944\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 156s 475ms/step - loss: 0.0805 - acc: 0.9707 - fmeasure: 0.8711 - precision: 0.9084 - val_loss: 0.0722 - val_acc: 0.9744 - val_fmeasure: 0.8889 - val_precision: 0.9150\n",
      "\n",
      "Epoch 00020: val_fmeasure did not improve from 0.88944\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 163s 497ms/step - loss: 0.0794 - acc: 0.9711 - fmeasure: 0.8734 - precision: 0.9087 - val_loss: 0.0720 - val_acc: 0.9744 - val_fmeasure: 0.8893 - val_precision: 0.9152\n",
      "\n",
      "Epoch 00021: val_fmeasure did not improve from 0.88944\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 154s 468ms/step - loss: 0.0791 - acc: 0.9709 - fmeasure: 0.8720 - precision: 0.9102 - val_loss: 0.0718 - val_acc: 0.9743 - val_fmeasure: 0.8886 - val_precision: 0.9141\n",
      "\n",
      "Epoch 00022: val_fmeasure did not improve from 0.88944\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 158s 480ms/step - loss: 0.0778 - acc: 0.9715 - fmeasure: 0.8747 - precision: 0.9064 - val_loss: 0.0721 - val_acc: 0.9742 - val_fmeasure: 0.8884 - val_precision: 0.9147\n",
      "\n",
      "Epoch 00023: val_fmeasure did not improve from 0.88944\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 162s 493ms/step - loss: 0.0773 - acc: 0.9725 - fmeasure: 0.8783 - precision: 0.9120 - val_loss: 0.0717 - val_acc: 0.9744 - val_fmeasure: 0.8889 - val_precision: 0.9145\n",
      "\n",
      "Epoch 00024: val_fmeasure did not improve from 0.88944\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 153s 466ms/step - loss: 0.0760 - acc: 0.9721 - fmeasure: 0.8784 - precision: 0.9129 - val_loss: 0.0720 - val_acc: 0.9744 - val_fmeasure: 0.8886 - val_precision: 0.9144\n",
      "\n",
      "Epoch 00025: val_fmeasure did not improve from 0.88944\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 165s 503ms/step - loss: 0.0763 - acc: 0.9719 - fmeasure: 0.8777 - precision: 0.9119 - val_loss: 0.0719 - val_acc: 0.9743 - val_fmeasure: 0.8888 - val_precision: 0.9150\n",
      "\n",
      "Epoch 00026: val_fmeasure did not improve from 0.88944\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 139s 422ms/step - loss: 0.0793 - acc: 0.9712 - fmeasure: 0.8723 - precision: 0.9071 - val_loss: 0.0719 - val_acc: 0.9744 - val_fmeasure: 0.8888 - val_precision: 0.9147\n",
      "\n",
      "Epoch 00027: val_fmeasure did not improve from 0.88944\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 140s 424ms/step - loss: 0.0781 - acc: 0.9716 - fmeasure: 0.8754 - precision: 0.9107 - val_loss: 0.0722 - val_acc: 0.9743 - val_fmeasure: 0.8886 - val_precision: 0.9143\n",
      "\n",
      "Epoch 00028: val_fmeasure did not improve from 0.88944\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 136s 413ms/step - loss: 0.0775 - acc: 0.9716 - fmeasure: 0.8760 - precision: 0.9110 - val_loss: 0.0719 - val_acc: 0.9744 - val_fmeasure: 0.8894 - val_precision: 0.9154\n",
      "\n",
      "Epoch 00029: val_fmeasure did not improve from 0.88944\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 135s 410ms/step - loss: 0.0780 - acc: 0.9715 - fmeasure: 0.8741 - precision: 0.9090 - val_loss: 0.0720 - val_acc: 0.9744 - val_fmeasure: 0.8888 - val_precision: 0.9152\n",
      "\n",
      "Epoch 00030: val_fmeasure did not improve from 0.88944\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "fold:  3  training\n",
      "tr_IDs :  21092\n",
      "val_IDs :  8908\n",
      "Epoch 1/30\n",
      "329/329 [==============================] - 202s 615ms/step - loss: 0.1144 - acc: 0.9580 - fmeasure: 0.8148 - precision: 0.8587 - val_loss: 0.0581 - val_acc: 0.9783 - val_fmeasure: 0.9044 - val_precision: 0.9415\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.90440, saving model to ./official_attention_model/attention_extend_weights-best_k0_r2_0809_30.hdf5\n",
      "Epoch 2/30\n",
      "329/329 [==============================] - 139s 423ms/step - loss: 0.1093 - acc: 0.9600 - fmeasure: 0.8207 - precision: 0.8651 - val_loss: 0.0674 - val_acc: 0.9731 - val_fmeasure: 0.8818 - val_precision: 0.9157\n",
      "\n",
      "Epoch 00002: val_fmeasure did not improve from 0.90440\n",
      "Epoch 3/30\n",
      "329/329 [==============================] - 138s 419ms/step - loss: 0.1054 - acc: 0.9611 - fmeasure: 0.8271 - precision: 0.8733 - val_loss: 0.0698 - val_acc: 0.9732 - val_fmeasure: 0.8844 - val_precision: 0.9006\n",
      "\n",
      "Epoch 00003: val_fmeasure did not improve from 0.90440\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/30\n",
      "329/329 [==============================] - 137s 415ms/step - loss: 0.0934 - acc: 0.9660 - fmeasure: 0.8505 - precision: 0.8917 - val_loss: 0.0652 - val_acc: 0.9751 - val_fmeasure: 0.8921 - val_precision: 0.9163\n",
      "\n",
      "Epoch 00004: val_fmeasure did not improve from 0.90440\n",
      "Epoch 5/30\n",
      "329/329 [==============================] - 139s 421ms/step - loss: 0.0888 - acc: 0.9672 - fmeasure: 0.8563 - precision: 0.8940 - val_loss: 0.0637 - val_acc: 0.9760 - val_fmeasure: 0.8961 - val_precision: 0.9184\n",
      "\n",
      "Epoch 00005: val_fmeasure did not improve from 0.90440\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 6/30\n",
      "329/329 [==============================] - 138s 421ms/step - loss: 0.0833 - acc: 0.9698 - fmeasure: 0.8663 - precision: 0.9049 - val_loss: 0.0578 - val_acc: 0.9780 - val_fmeasure: 0.9050 - val_precision: 0.9246\n",
      "\n",
      "Epoch 00006: val_fmeasure improved from 0.90440 to 0.90497, saving model to ./official_attention_model/attention_extend_weights-best_k0_r2_0809_30.hdf5\n",
      "Epoch 7/30\n",
      "329/329 [==============================] - 136s 414ms/step - loss: 0.0836 - acc: 0.9696 - fmeasure: 0.8668 - precision: 0.9042 - val_loss: 0.0618 - val_acc: 0.9762 - val_fmeasure: 0.8966 - val_precision: 0.9231\n",
      "\n",
      "Epoch 00007: val_fmeasure did not improve from 0.90497\n",
      "Epoch 8/30\n",
      "329/329 [==============================] - 137s 415ms/step - loss: 0.0786 - acc: 0.9713 - fmeasure: 0.8747 - precision: 0.9083 - val_loss: 0.0619 - val_acc: 0.9768 - val_fmeasure: 0.9001 - val_precision: 0.9159\n",
      "\n",
      "Epoch 00008: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/30\n",
      "329/329 [==============================] - 133s 403ms/step - loss: 0.0815 - acc: 0.9702 - fmeasure: 0.8662 - precision: 0.9038 - val_loss: 0.0623 - val_acc: 0.9773 - val_fmeasure: 0.9024 - val_precision: 0.9171\n",
      "\n",
      "Epoch 00009: val_fmeasure did not improve from 0.90497\n",
      "Epoch 10/30\n",
      "329/329 [==============================] - 134s 407ms/step - loss: 0.0755 - acc: 0.9721 - fmeasure: 0.8786 - precision: 0.9119 - val_loss: 0.0628 - val_acc: 0.9767 - val_fmeasure: 0.8992 - val_precision: 0.9174\n",
      "\n",
      "Epoch 00010: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/30\n",
      "329/329 [==============================] - 137s 418ms/step - loss: 0.0740 - acc: 0.9732 - fmeasure: 0.8835 - precision: 0.9144 - val_loss: 0.0606 - val_acc: 0.9779 - val_fmeasure: 0.9045 - val_precision: 0.9223\n",
      "\n",
      "Epoch 00011: val_fmeasure did not improve from 0.90497\n",
      "Epoch 12/30\n",
      "329/329 [==============================] - 138s 420ms/step - loss: 0.0756 - acc: 0.9724 - fmeasure: 0.8776 - precision: 0.9107 - val_loss: 0.0604 - val_acc: 0.9777 - val_fmeasure: 0.9041 - val_precision: 0.9213\n",
      "\n",
      "Epoch 00012: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/30\n",
      "329/329 [==============================] - 139s 423ms/step - loss: 0.0712 - acc: 0.9736 - fmeasure: 0.8840 - precision: 0.9150 - val_loss: 0.0608 - val_acc: 0.9774 - val_fmeasure: 0.9025 - val_precision: 0.9194\n",
      "\n",
      "Epoch 00013: val_fmeasure did not improve from 0.90497\n",
      "Epoch 14/30\n",
      "329/329 [==============================] - 140s 427ms/step - loss: 0.0745 - acc: 0.9727 - fmeasure: 0.8783 - precision: 0.9141 - val_loss: 0.0608 - val_acc: 0.9774 - val_fmeasure: 0.9026 - val_precision: 0.9196\n",
      "\n",
      "Epoch 00014: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 15/30\n",
      "329/329 [==============================] - 140s 426ms/step - loss: 0.0725 - acc: 0.9734 - fmeasure: 0.8836 - precision: 0.9159 - val_loss: 0.0610 - val_acc: 0.9776 - val_fmeasure: 0.9032 - val_precision: 0.9195\n",
      "\n",
      "Epoch 00015: val_fmeasure did not improve from 0.90497\n",
      "Epoch 16/30\n",
      "329/329 [==============================] - 140s 425ms/step - loss: 0.0733 - acc: 0.9731 - fmeasure: 0.8808 - precision: 0.9159 - val_loss: 0.0609 - val_acc: 0.9776 - val_fmeasure: 0.9038 - val_precision: 0.9214\n",
      "\n",
      "Epoch 00016: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 17/30\n",
      "329/329 [==============================] - 136s 413ms/step - loss: 0.0712 - acc: 0.9740 - fmeasure: 0.8868 - precision: 0.9181 - val_loss: 0.0607 - val_acc: 0.9778 - val_fmeasure: 0.9044 - val_precision: 0.9215\n",
      "\n",
      "Epoch 00017: val_fmeasure did not improve from 0.90497\n",
      "Epoch 18/30\n",
      "329/329 [==============================] - 139s 421ms/step - loss: 0.0715 - acc: 0.9735 - fmeasure: 0.8844 - precision: 0.9151 - val_loss: 0.0604 - val_acc: 0.9778 - val_fmeasure: 0.9041 - val_precision: 0.9208\n",
      "\n",
      "Epoch 00018: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 19/30\n",
      "329/329 [==============================] - 138s 420ms/step - loss: 0.0739 - acc: 0.9730 - fmeasure: 0.8817 - precision: 0.9157 - val_loss: 0.0607 - val_acc: 0.9777 - val_fmeasure: 0.9040 - val_precision: 0.9215\n",
      "\n",
      "Epoch 00019: val_fmeasure did not improve from 0.90497\n",
      "Epoch 20/30\n",
      "329/329 [==============================] - 137s 417ms/step - loss: 0.0716 - acc: 0.9739 - fmeasure: 0.8864 - precision: 0.9175 - val_loss: 0.0607 - val_acc: 0.9778 - val_fmeasure: 0.9044 - val_precision: 0.9214\n",
      "\n",
      "Epoch 00020: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 21/30\n",
      "329/329 [==============================] - 135s 410ms/step - loss: 0.0724 - acc: 0.9737 - fmeasure: 0.8850 - precision: 0.9174 - val_loss: 0.0606 - val_acc: 0.9778 - val_fmeasure: 0.9045 - val_precision: 0.9218\n",
      "\n",
      "Epoch 00021: val_fmeasure did not improve from 0.90497\n",
      "Epoch 22/30\n",
      "329/329 [==============================] - 133s 405ms/step - loss: 0.0710 - acc: 0.9737 - fmeasure: 0.8849 - precision: 0.9173 - val_loss: 0.0605 - val_acc: 0.9778 - val_fmeasure: 0.9042 - val_precision: 0.9215\n",
      "\n",
      "Epoch 00022: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 23/30\n",
      "329/329 [==============================] - 136s 415ms/step - loss: 0.0751 - acc: 0.9727 - fmeasure: 0.8797 - precision: 0.9161 - val_loss: 0.0605 - val_acc: 0.9777 - val_fmeasure: 0.9042 - val_precision: 0.9210\n",
      "\n",
      "Epoch 00023: val_fmeasure did not improve from 0.90497\n",
      "Epoch 24/30\n",
      "329/329 [==============================] - 135s 409ms/step - loss: 0.0716 - acc: 0.9740 - fmeasure: 0.8855 - precision: 0.9190 - val_loss: 0.0606 - val_acc: 0.9778 - val_fmeasure: 0.9040 - val_precision: 0.9215\n",
      "\n",
      "Epoch 00024: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 25/30\n",
      "329/329 [==============================] - 133s 406ms/step - loss: 0.0722 - acc: 0.9739 - fmeasure: 0.8849 - precision: 0.9179 - val_loss: 0.0606 - val_acc: 0.9778 - val_fmeasure: 0.9044 - val_precision: 0.9219\n",
      "\n",
      "Epoch 00025: val_fmeasure did not improve from 0.90497\n",
      "Epoch 26/30\n",
      "329/329 [==============================] - 140s 424ms/step - loss: 0.0735 - acc: 0.9733 - fmeasure: 0.8815 - precision: 0.9177 - val_loss: 0.0605 - val_acc: 0.9779 - val_fmeasure: 0.9045 - val_precision: 0.9220\n",
      "\n",
      "Epoch 00026: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 27/30\n",
      "329/329 [==============================] - 138s 421ms/step - loss: 0.0702 - acc: 0.9744 - fmeasure: 0.8884 - precision: 0.9191 - val_loss: 0.0606 - val_acc: 0.9777 - val_fmeasure: 0.9042 - val_precision: 0.9212\n",
      "\n",
      "Epoch 00027: val_fmeasure did not improve from 0.90497\n",
      "Epoch 28/30\n",
      "329/329 [==============================] - 139s 423ms/step - loss: 0.0739 - acc: 0.9729 - fmeasure: 0.8809 - precision: 0.9147 - val_loss: 0.0607 - val_acc: 0.9777 - val_fmeasure: 0.9040 - val_precision: 0.9208\n",
      "\n",
      "Epoch 00028: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 29/30\n",
      "329/329 [==============================] - 136s 414ms/step - loss: 0.0721 - acc: 0.9735 - fmeasure: 0.8821 - precision: 0.9167 - val_loss: 0.0606 - val_acc: 0.9778 - val_fmeasure: 0.9040 - val_precision: 0.9214\n",
      "\n",
      "Epoch 00029: val_fmeasure did not improve from 0.90497\n",
      "Epoch 30/30\n",
      "329/329 [==============================] - 133s 405ms/step - loss: 0.0745 - acc: 0.9729 - fmeasure: 0.8805 - precision: 0.9153 - val_loss: 0.0606 - val_acc: 0.9777 - val_fmeasure: 0.9043 - val_precision: 0.9213\n",
      "\n",
      "Epoch 00030: val_fmeasure did not improve from 0.90497\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = path + \"/Train/\"\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "\n",
    "train_files = os.listdir(train_dataset_path)\n",
    "train_files.sort()\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "labels_en = pd.read_csv(path + \"kfold_labels_en.csv\")\n",
    "#data_info = pd.read_csv(path + \"data_info.csv\")\n",
    "\n",
    "input_size = (2560, 12)\n",
    "net_num = 10\n",
    "inputs_list = [Input(shape=input_size) for _ in range(net_num)]\n",
    "outputs = attentionmodel.build_network(inputs_list, 0.5, num_classes=10, block_size=4, relu=False)\n",
    "model = Model(inputs=inputs_list, outputs=outputs)\n",
    "# print(model.summary())\n",
    "\n",
    "raw_IDs = labels_en[\"File_name\"].values.tolist()\n",
    "extend_db4_IDs = [i + \"_db4\" for i in raw_IDs]\n",
    "extend_db6_IDs = [i + \"_db6\" for i in raw_IDs]\n",
    "extend_ori_IDs = [i + \"_ori\" for i in raw_IDs]\n",
    "all_IDs = raw_IDs + extend_db4_IDs + extend_db6_IDs+extend_ori_IDs\n",
    "\n",
    "train_labels = labels_en[\"label1\"].values\n",
    "all_train_labels = np.hstack((train_labels, train_labels, train_labels))\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': (10, 2560),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 10,\n",
    "          'n_channels': 12,\n",
    "          'shuffle': True}\n",
    "\n",
    "en_amount = 1\n",
    "model_path = './official_attention_model/'\n",
    "\n",
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 3\n",
    "    n_classes = 10\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1234)\n",
    "    # kf = kfold.split(all_IDs, all_train_labels)\n",
    "    kf = kfold.split(labels[\"File_name\"].values.tolist(), labels[\"label1\"].values)\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i + 1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        #tr_IDs = np.array(all_IDs)[index_train]\n",
    "        #val_IDs = np.array(all_IDs)[index_valid]\n",
    "        #print(tr_IDs.shape)\n",
    "        tr_IDs = labels[\"File_name\"].values[index_train].tolist() \n",
    "        val_IDs = labels[\"File_name\"].values[index_valid].tolist()\n",
    "\n",
    "        for j in range(4):\n",
    "            for ids in labels[labels.label1==4][\"File_name\"]:\n",
    "                if ids in tr_IDs:\n",
    "                    tr_IDs.append(ids)\n",
    "\n",
    "        for j in range(2):\n",
    "            for ids in labels[labels.label1==7][\"File_name\"]:\n",
    "                if ids in tr_IDs:\n",
    "                    tr_IDs.append(ids)\n",
    "\n",
    "        for j in range(1):\n",
    "            for ids in labels[labels.label1==9][\"File_name\"]:\n",
    "                if ids in tr_IDs:\n",
    "                    tr_IDs.append(ids)\n",
    "\n",
    "        tr_IDs_db4 = [ids+\"_db4\" for ids in tr_IDs]\n",
    "        tr_IDs_db6 = [ids+\"_db6\" for ids in tr_IDs]\n",
    "        tr_IDs_ori = [ids+\"_ori\" for ids in tr_IDs]\n",
    "\n",
    "        val_IDs_db4 = [ids+\"_db4\" for ids in val_IDs]\n",
    "        val_IDs_db6 = [ids+\"_db6\" for ids in val_IDs]\n",
    "        val_IDs_ori = [ids+\"_ori\" for ids in val_IDs]\n",
    "\n",
    "        tr_IDs = tr_IDs+ tr_IDs_db4 + tr_IDs_db6 + tr_IDs_ori\n",
    "        val_IDs = val_IDs + val_IDs_db4 + val_IDs_db6 + val_IDs_ori\n",
    "        print(\"tr_IDs : \",len(tr_IDs))\n",
    "        print(\"val_IDs : \",len(val_IDs))\n",
    "\n",
    "\n",
    "        # Generators\n",
    "        training_generator = DataGenerator(tr_IDs, labels, **params)\n",
    "        validation_generator = DataGenerator(val_IDs, labels, **params)\n",
    "\n",
    "        checkpointer = ModelCheckpoint(filepath=model_path + 'attention_extend_weights-best_k{}_r{}_0809_30.hdf5'.format(seed, i),\n",
    "                                       monitor='val_fmeasure', verbose=1, save_best_only=True,\n",
    "                                       save_weights_only=True,\n",
    "                                       mode='max')  # val_fmeasure\n",
    "        reduce = ReduceLROnPlateau(monitor='val_fmeasure', factor=0.5, patience=2, verbose=1, min_delta=1e-5,\n",
    "                                   mode='max')\n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_fmeasure', patience=10)\n",
    "\n",
    "        config = Config()\n",
    "        add_compile(model, config)\n",
    "\n",
    "        callback_lists = [checkpointer, reduce]\n",
    "\n",
    "        history = model.fit_generator(generator=training_generator,\n",
    "                                      validation_data=validation_generator,\n",
    "                                      use_multiprocessing=False,\n",
    "                                      epochs=30, # 50\n",
    "                                      verbose=1,\n",
    "                                      callbacks=callback_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predcit_net_kfolds(pre_type = \"sym\"):\n",
    "\n",
    "    #pre_type = \"sym\" # \"sym\"\n",
    "    path = \"/media/jdcloud/\"\n",
    "    labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "    raw_IDs = labels[\"File_name\"].values.tolist()\n",
    "\n",
    "    IDs = {}\n",
    "    IDs[\"sym\"] = raw_IDs\n",
    "    IDs[\"db4\"] = [i + \"_db4\" for i in raw_IDs]\n",
    "    IDs[\"db6\"] = [i + \"_db6\" for i in raw_IDs]\n",
    "    IDs[\"ori\"] = [i + \"_ori\" for i in raw_IDs]\n",
    "    \n",
    "    input_size = (2560, 12)\n",
    "    net_num = 10\n",
    "    inputs_list = [Input(shape=input_size) for _ in range(net_num)]\n",
    "    outputs = attentionmodel.build_network(inputs_list, 0.5, num_classes=10, block_size=4, relu=False)\n",
    "    model = Model(inputs=inputs_list, outputs=outputs)\n",
    "\n",
    "    net_num = 10\n",
    "    test_x = [read_data_seg(path, split='Val', preprocess=True, n_index=i, pre_type=pre_type) for i in range(net_num)]\n",
    "\n",
    "    model_path = './official_attention_model/'\n",
    "    model_name = 'attention_extend_weights-best_one_fold.hdf5'\n",
    "\n",
    "    en_amount = 1\n",
    "    for seed in range(en_amount):\n",
    "        print(\"************************\")\n",
    "        n_fold = 3  # 3\n",
    "        n_classes = 10\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "        kf = kfold.split(IDs[pre_type], labels['label1'])\n",
    "\n",
    "        blend_train = np.zeros((6689, n_fold, n_classes)).astype('float32')  # len(train_x)\n",
    "        blend_test = np.zeros((558, n_fold, n_classes)).astype('float32')  # len(test_x)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for i, (index_train, index_valid) in enumerate(kf):\n",
    "            print('fold: ', i + 1, ' training')\n",
    "            t = time.time()\n",
    "\n",
    "            tr_IDs = np.array(IDs[pre_type]) # [index_train]\n",
    "            # val_IDs = np.array(IDs[pre_type])[index_valid]\n",
    "            print(tr_IDs.shape)\n",
    "\n",
    "            X = np.empty((tr_IDs.shape[0], 10, 2560, 12))\n",
    "            for j, ID in enumerate(tr_IDs):\n",
    "                X[j, ] = np.load(\"training_data/\" + ID + \".npy\")\n",
    "            # X_tr = [(X[:, i] - np.mean(X[:, i])) / np.std(X[:, i]) for i in range(10)]\n",
    "            X_tr = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n",
    "            # print(X.shape)\n",
    "            del X\n",
    "\n",
    "            # Evaluate best trained model\n",
    "            model.load_weights(model_path + 'attention_extend_weights-best_k{}_r{}_0809_30.hdf5'.format(seed, i))\n",
    "\n",
    "            blend_train[:, i, :] = model.predict(X_tr)\n",
    "            blend_test[:, i, :] = model.predict(test_x)\n",
    "\n",
    "            del X_tr\n",
    "            gc.collect()\n",
    "            gc.collect()\n",
    "            count += 1\n",
    "\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    LR = LogisticRegression(penalty=\"l2\",C=1.0)\n",
    "\n",
    "    #pre_type = \"sym\"#\"db6\"#\"sym\"\n",
    "    labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "\n",
    "    index = np.arange(6689)\n",
    "    y_train = preprocess_y(labels, index)\n",
    "\n",
    "    x_train = np.hstack([blend_train[:,0,:],blend_train[:,1,:],blend_train[:,2,:]])\n",
    "\n",
    "    clf = OneVsRestClassifier(LR)\n",
    "    clf.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_train)\n",
    "\n",
    "    print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "    for i in range(10):\n",
    "        print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "\n",
    "\n",
    "    '''\n",
    "    index = np.arange(6689)\n",
    "    y_train = preprocess_y(labels, index)\n",
    "\n",
    "    train_y = 0.1 * blend_train[:, 0, :] + 0.1 * blend_train[:, 1, :] + 0.8 * blend_train[:, 2, :]\n",
    "\n",
    "    threshold = np.arange(0.1, 0.9, 0.1)\n",
    "    acc = []\n",
    "    accuracies = []\n",
    "    best_threshold = np.zeros(train_y.shape[1])\n",
    "\n",
    "    for i in range(train_y.shape[1]):\n",
    "        y_prob = np.array(train_y[:, i])\n",
    "        for j in threshold:\n",
    "            y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "            acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "        acc = np.array(acc)\n",
    "        index = np.where(acc == acc.max())\n",
    "        accuracies.append(acc.max())\n",
    "        best_threshold[i] = threshold[index[0][0]]\n",
    "        acc = []\n",
    "\n",
    "    print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "    y_pred = np.array([[1 if train_y[i, j] >= best_threshold[j] else 0 for j in range(train_y.shape[1])]\n",
    "              for i in range(len(train_y))])\n",
    "    print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "    for i in range(10):\n",
    "        print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "\n",
    "\n",
    "    out = 0.1 * blend_test[:, 0, :] + 0.1 * blend_test[:, 1, :] + 0.8 * blend_test[:, 2, :]\n",
    "\n",
    "    y_pred_test = np.array(\n",
    "        [[1 if out[i, j] >= best_threshold[j] else 0 for j in range(out.shape[1])] for i in range(len(out))])\n",
    "\n",
    "    classes = [0, 1, 2, 3, 4, 5, 6, 7, 8,9]\n",
    "\n",
    "    test_y = y_pred_test\n",
    "\n",
    "    y_pred = [[1 if test_y[i, j] >= best_threshold[j] else 0 for j in range(test_y.shape[1])]\n",
    "              for i in range(len(test_y))]\n",
    "    '''\n",
    "    out = np.hstack([blend_test[:,0,:],blend_test[:,1,:],blend_test[:,2,:]])#\n",
    "\n",
    "    y_pred = clf.predict(out)\n",
    "\n",
    "    classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    pred = []\n",
    "    for j in range(y_pred.shape[0]):#test_y\n",
    "        pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "\n",
    "    val_dataset_path = path + \"/Val/\"\n",
    "    val_files = os.listdir(val_dataset_path)\n",
    "    val_files.sort()\n",
    "\n",
    "    with open('jupyter_answers_attention_{}_0809.csv'.format(pre_type), 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['File_name', 'label1', 'label2',\n",
    "                         'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "        count = 0\n",
    "        for file_name in val_files:\n",
    "            if file_name.endswith('.mat'):\n",
    "\n",
    "                record_name = file_name.strip('.mat')\n",
    "                answer = []\n",
    "                answer.append(record_name)\n",
    "\n",
    "                result = pred[count]\n",
    "\n",
    "                answer.extend(result)\n",
    "                for i in range(10 - len(result)):\n",
    "                    answer.append('')\n",
    "                count += 1\n",
    "                writer.writerow(answer)\n",
    "        csvfile.close()\n",
    "\n",
    "    train_pd0 = pd.DataFrame(blend_train[:,0,:])\n",
    "    train_pd1 = pd.DataFrame(blend_train[:,1,:])\n",
    "    train_pd2 = pd.DataFrame(blend_train[:,2,:])\n",
    "    csv_path = \"./ensemble_csv/\"\n",
    "    train_pd0.to_csv(csv_path+\"attention_10net_{}_addori_fold0.csv\".format(pre_type),index=None)\n",
    "    train_pd1.to_csv(csv_path+\"attention_10net_{}_addori_fold1.csv\".format(pre_type),index=None)\n",
    "    train_pd2.to_csv(csv_path+\"attention_10net_{}_addori_fold2.csv\".format(pre_type),index=None)\n",
    "\n",
    "    test_pd0 = pd.DataFrame(blend_test[:,0,:])\n",
    "    test_pd1 = pd.DataFrame(blend_test[:,1,:])\n",
    "    test_pd2 = pd.DataFrame(blend_test[:,2,:])\n",
    "    csv_path = \"./test_csv/\"\n",
    "    test_pd0.to_csv(csv_path+\"attention_10net_{}_addori_fold0.csv\".format(pre_type),index=None)\n",
    "    test_pd1.to_csv(csv_path+\"attention_10net_{}_addori_fold1.csv\".format(pre_type),index=None)\n",
    "    test_pd2.to_csv(csv_path+\"attention_10net_{}_addori_fold2.csv\".format(pre_type),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "************************\n",
      "fold:  1  training\n",
      "(6689,)\n",
      "fold:  2  training\n",
      "(6689,)\n",
      "fold:  3  training\n",
      "(6689,)\n",
      " train data f1_score  : 0.9364256418580391\n",
      "f1 score of ab 0 is 0.9658348169213595\n",
      "f1 score of ab 1 is 0.9898416333572271\n",
      "f1 score of ab 2 is 0.9655428489767162\n",
      "f1 score of ab 3 is 0.9825511727369634\n",
      "f1 score of ab 4 is 0.9620437044981247\n",
      "f1 score of ab 5 is 0.9677196412449129\n",
      "f1 score of ab 6 is 0.9396350192316612\n",
      "f1 score of ab 7 is 0.953039546753347\n",
      "f1 score of ab 8 is 0.9482459357393056\n",
      "f1 score of ab 9 is 0.9634379884906863\n"
     ]
    }
   ],
   "source": [
    "predcit_net_kfolds(pre_type = \"ori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "path = './'\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_sym_addori_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_sym_addori_fold1.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_sym_addori_fold2.csv\").values) # 3folds f0.817\n",
    "\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_db6_addori_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_db6_addori_fold1.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_db6_addori_fold2.csv\").values) # 3folds f0.818\n",
    "\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_ori_addori_fold0.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_ori_addori_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_ori_addori_fold2.csv\").values) # 3folds f0.817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_sym_addori_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_sym_addori_fold1.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_sym_addori_fold2.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_db6_addori_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_db6_addori_fold1.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_db6_addori_fold2.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_ori_addori_fold0.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_ori_addori_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_ori_addori_fold2.csv\").values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6689, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLkNN(ignore_first_neighbours=0, k=8, s=1.0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, Lasso, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from mlknn import MLkNN\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate=0.005,n_estimators=100,max_depth=5,min_samples_leaf=20,\n",
    "                                min_samples_split=600,subsample=0.7,random_state=2019)\n",
    "LR = LogisticRegression(penalty=\"l2\",C=1.0)\n",
    "Eln = ElasticNet()\n",
    "Las = Lasso(alpha=0.2)\n",
    "LRR = RidgeCV()\n",
    "\n",
    "pre_type = \"db6\"#\"db6\"#\"sym\"\n",
    "labels = pd.read_csv(\"/media/jdcloud/\" + \"REFERENCE.csv\")\n",
    "\n",
    "index = np.arange(6689)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "x_train = np.hstack(train[:])\n",
    "# x_train = np.column_stack([x_train,train[-1]])\n",
    "# x_train = np.column_stack([x_train,train[-2]])\n",
    "#train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.333, random_state=0)\n",
    "#train = lgb.Dataset(x_train,label=y_train)\n",
    "#valid = lgb.Dataset(valid_x, label=valid_y\n",
    "#gbm = lgb.train(params,train,num_boost_round=1000,#valid_sets=valid,early_stopping_rounds=5)\n",
    "\n",
    "clf = MLkNN(k=8)\n",
    "\n",
    "#clf = OneVsRestClassifier(LR)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,hamming_loss\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train data f1_score  : 0.9459682850306548\n",
      "f1 score of ab 0 is 0.9698879144657897\n",
      "f1 score of ab 1 is 0.9921295035618354\n",
      "f1 score of ab 2 is 0.968707806818373\n",
      "f1 score of ab 3 is 0.9864826677400564\n",
      "f1 score of ab 4 is 0.9631518930859532\n",
      "f1 score of ab 5 is 0.9762565047862113\n",
      "f1 score of ab 6 is 0.9515934021319987\n",
      "f1 score of ab 7 is 0.9552460119452224\n",
      "f1 score of ab 8 is 0.9550018867888549\n",
      "f1 score of ab 9 is 0.9737361997784897\n",
      " train data hamming_loss  : 0.01190013454925998\n",
      " train data precision recall f1  : (0.9521104300593013, 0.9487741067424128, 0.9467209847012509, None)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_train).toarray()\n",
    "y_pred_proba_train = clf.predict(x_train).toarray()\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "for i in range(10):\n",
    "    \n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "    \n",
    "print(\" train data hamming_loss  :\", hamming_loss(y_train, y_pred)) \n",
    "print(\" train data precision recall f1  :\", prf(y_train, y_pred,average=\"samples\"))# 'micro', 'weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = np.hstack(test[:])\n",
    "#out = np.column_stack([out,test[-1]])\n",
    "#out = np.column_stack([out,test[-2]])\n",
    "# LR_clf = joblib.load(\"LR_ensemble.pkl\")\n",
    "# MLkNN_clf = joblib.load(\"MLkNN_ensemble.pkl\")\n",
    "\n",
    "# y_pred_LR = LR_clf.predict(out)\n",
    "# y_pred_proba_LR = LR_clf.predict_proba(out)\n",
    "\n",
    "# y_pred_MLkNN = MLkNN_clf.predict(out).toarray()\n",
    "# y_pred_proba_MLkNN = MLkNN_clf.predict_proba(out).toarray()\n",
    "\n",
    "# y_pred_MLkNN[:,7] = y_pred_LR[:,7]\n",
    "# y_pred_proba_MLkNN[:,7] = y_pred_proba_LR[:,7]\n",
    "# y_pred = y_pred_MLkNN\n",
    "# y_pred_proba = y_pred_proba_MLkNN\n",
    "\n",
    "y_pred = clf.predict(out).toarray()\n",
    "y_pred_proba = clf.predict_proba(out).toarray()\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "pred = []\n",
    "for j in range(y_pred.shape[0]):\n",
    "    pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "\n",
    "''' ''' \n",
    "for i, val in enumerate(pred):\n",
    "    if val == []:\n",
    "        pass\n",
    "        #for i_p, val_p in enumerate(y_pred_proba[i]):\n",
    "        #    if val_p >= 0.4:\n",
    "        #        pred[i].append(i_p)    # f1 == 0.832\n",
    "                \n",
    "        if y_pred_proba[i][np.argmax(y_pred_proba[i])] >= 0.3:\n",
    "            pred[i] = [np.argmax(y_pred_proba[i])]     # f1 == 0.833  0.4\n",
    "\n",
    "val_dataset_path = '/media/jdcloud' + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('jupyter_answers_densenet_{}_0809_ensemble.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(10 - len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict quarter final data\n",
    "## attention 10nets \n",
    "### model : attention_extend_weights-best_k{}_r{}_0809_30.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_type = \"db6\"#\"db6\"\n",
    "\n",
    "#labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "labels = pd.read_csv(\"/media/uuser/data/final_codes/final_run_semi/reference.csv\")\n",
    "raw_IDs = labels[\"File_name\"].values.tolist()\n",
    "\n",
    "IDs = {}\n",
    "IDs[\"sym\"] = raw_IDs\n",
    "IDs[\"db4\"] = [i + \"_db4\" for i in raw_IDs]\n",
    "IDs[\"db6\"] = [i + \"_db6\" for i in raw_IDs]\n",
    "\n",
    "X = np.empty((6500, 10, 2560, 12))\n",
    "for i, ID in enumerate(IDs[pre_type]):\n",
    "    #print(ID)\n",
    "    X[i,] = np.load(\"/media/uuser/data/ysecgtest/training_data/\" + ID + \".npy\")\n",
    "#train_x = [(X[:, i]-np.mean(X[:, i]))/np.std(X[:, i]) for i in range(10)]\n",
    "train_x = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n",
    "\n",
    "def preprocess_y(labels, y, num_class=10):\n",
    "    bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1, label_nona.shape[0]):\n",
    "            bin_label[i, int(label_nona[j])] = 1\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/uuser/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from resnet_ecg import attentionmodel\n",
    "\n",
    "#pre_type = \"sym\"\n",
    "\n",
    "#labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "#labels = pd.read_csv(\"/media/uuser/data/final_run/reference.csv\")\n",
    "raw_IDs = labels[\"File_name\"].values.tolist()\n",
    "\n",
    "IDs = {}\n",
    "IDs[\"sym\"] = raw_IDs\n",
    "IDs[\"db4\"] = [i + \"_db4\" for i in raw_IDs]\n",
    "IDs[\"db6\"] = [i + \"_db6\" for i in raw_IDs]\n",
    "\n",
    "# X = np.empty((6500, 10, 2560, 12))\n",
    "# for i, ID in enumerate(IDs[pre_type]):\n",
    "#     X[i,] = np.load(\"/media/uuser/data/final_run/training_data/\" + ID + \".npy\")\n",
    "# #train_x = [(X[:, i]-np.mean(X[:, i]))/np.std(X[:, i]) for i in range(10)]\n",
    "# train_x = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n",
    "\n",
    "# def preprocess_y(labels, y, num_class=10):\n",
    "#     bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "#     for i in range(len(y)):\n",
    "#         label_nona = labels.loc[y[i]].dropna()\n",
    "#         for j in range(1, label_nona.shape[0]):\n",
    "#             bin_label[i, int(label_nona[j])] = 1\n",
    "#     return bin_label\n",
    "\n",
    "# net_num = 10\n",
    "# test_x = [read_data_seg(path, split='Val', preprocess=True, n_index=i, pre_type=pre_type) for i in range(net_num)]\n",
    "\n",
    "index = np.arange(6500)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "input_size = (2560, 12)\n",
    "net_num = 10\n",
    "inputs_list = [Input(shape=input_size) for _ in range(net_num)]\n",
    "outputs = attentionmodel.build_network(inputs_list, 0.5, num_classes=10, block_size=4, relu=False)\n",
    "model = Model(inputs=inputs_list, outputs=outputs)\n",
    "\n",
    "# print(model.summary())\n",
    "n_classes = 10\n",
    "n_fold = 3\n",
    "model_path = './official_attention_model/'\n",
    "#'densenet_extend_weights-best_one_fold_0607.hdf5'\n",
    "blend_train = np.zeros((6500, n_fold, n_classes)).astype('float32')\n",
    "en_amount = 1\n",
    "for seed in range(en_amount):\n",
    "    for i in range(n_fold):\n",
    "        model_name = \"attention_extend_weights-best_k{}_r{}_0809_30.hdf5\".format(seed, i)\n",
    "        model.load_weights(model_path + model_name)\n",
    "        blend_train[:,i,:] = model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pd0 = pd.DataFrame(blend_train[:,0,:])\n",
    "train_pd1 = pd.DataFrame(blend_train[:,1,:])\n",
    "train_pd2 = pd.DataFrame(blend_train[:,2,:])\n",
    "csv_path = \"/media/uuser/data/final_codes/final_run_final/quarter_final/\"\n",
    "train_pd0.to_csv(csv_path+\"attention_10net_db6_addori_fold0.csv\",index=None)\n",
    "train_pd1.to_csv(csv_path+\"attention_10net_db6_addori_fold1.csv\",index=None)\n",
    "train_pd2.to_csv(csv_path+\"attention_10net_db6_addori_fold2.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
