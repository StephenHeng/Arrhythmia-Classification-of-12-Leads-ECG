{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pywt\n",
    "import os\n",
    "from sklearn.metrics import f1_score,hamming_loss\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "path = \"/media/jdcloud/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_y(labels, y, num_class=10):\n",
    "    bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1, label_nona.shape[0]):\n",
    "            bin_label[i, int(label_nona[j])] = 1\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    if x==\"MALE\":\n",
    "        return 0\n",
    "    elif x==\"FEMALE\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "labels_en = pd.read_csv(path + \"kfold_labels_en.csv\")\n",
    "labels_en_drop = labels_en.drop_duplicates([\"File_name\"])\n",
    "labels_en_drop.age.values.shape\n",
    "labels_en_drop.sex.apply(lambda x:f(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_attention_maxpooling_10net_fold0.csv\").values)\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_attention_maxpooling_10net_fold1.csv\").values)\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_attention_maxpooling_10net_fold2.csv\").values)\n",
    "\n",
    "path = \"./\"\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_4block_10net_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_4block_10net_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_4block_10net_fold2.csv\").values) #1fold f0.822\n",
    "\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_one_net_fold0.csv\").values)\n",
    "#train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_one_net_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_one_net_fold2.csv\").values) # 1fold f0.813\n",
    "\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_1net_fold0.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_1net_fold1.csv\").values)\n",
    "# train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_1net_fold2.csv\").values)\n",
    "\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_fold0.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_fold1.csv\").values)\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"attention_10net_fold2.csv\").values) # 3folds f0.817\n",
    "\n",
    "train.append(pd.read_csv(path+\"ensemble_csv/\"+\"densenet_f0819_10net_fold.csv\").values) # 1fold f0.819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.append(labels_en_drop.age.values)\n",
    "train.append(labels_en_drop.sex.apply(lambda x:f(x)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "#test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_attention_maxpooling_10net_fold0.csv\").values)\n",
    "#test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_attention_maxpooling_10net_fold1.csv\").values)\n",
    "#test.append( pd.read_csv(path+\"test_csv/\"+\"densenet_attention_maxpooling_10net_fold2.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_4block_10net_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_4block_10net_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_4block_10net_fold2.csv\").values)\n",
    "\n",
    "#test.append(pd.read_csv(path+\"test_csv/\"+\"attention_one_net_fold0.csv\").values)\n",
    "#test.append(pd.read_csv(path+\"test_csv/\"+\"attention_one_net_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_one_net_fold2.csv\").values)\n",
    "\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_1net_fold0.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_1net_fold1.csv\").values)\n",
    "# test.append(pd.read_csv(path+\"test_csv/\"+\"attention_1net_fold2.csv\").values)\n",
    "\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_fold0.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_fold1.csv\").values)\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"attention_10net_fold2.csv\").values)\n",
    "\n",
    "test.append(pd.read_csv(path+\"test_csv/\"+\"densenet_f0819_10net_fold.csv\").values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "def read_data_labels_info(data_path, split = \"Train\",preprocess=True):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    labels = pd.read_csv(data_path+\"REFERENCE.csv\")\n",
    "    #2156+224+672+654+180+826+534+504+1953\n",
    "    \n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12#len(channel_files)\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "    i_ch = 0\n",
    "    \n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    \n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1','II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II','aVR','V2','V5']\n",
    "    channel_post_name = ['III','aVF','V3','V6']\n",
    "      \n",
    "    info = []\n",
    "    \n",
    "    for i_ch,fil_ch in enumerate(channel_files[:]):#tqdm\n",
    "\n",
    "        labels_list = list(labels.iloc[i_ch].values[:])#.dropna()\n",
    "        #print(labels_list)\n",
    "\n",
    "        ecg = sio.loadmat(os.path.join(path_signals,fil_ch))\n",
    "        a = 0\n",
    "        for m in labels_list:\n",
    "            if m in [0,1,2,3,4,5,6,7,8,9]:\n",
    "                a += 1\n",
    "        labels_list.append(a)\n",
    "        labels_list.append(ecg['sex'][0])\n",
    "        labels_list.append(ecg['age'][0][0])\n",
    "        labels_list.append(ecg['I'].shape[1])\n",
    "        \n",
    "        \n",
    "        info.append(labels_list)\n",
    "    column_name = ['File_name','label1','label2','label3','label4','label5','label6',\n",
    "                   'label7','label8','label9','label10','labels_num','sex','age','dataSize']\n",
    "    info_pd =pd.DataFrame(columns=column_name,index=range(len(channel_files)),data=info)\n",
    "    # Return \n",
    "    return info_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_info = read_data_labels_info(data_path=\"/media/jdcloud/\",split=\"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.append(test_info.age.values)\n",
    "test.append(test_info.sex.apply(lambda x:f(x)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',#'multiclass',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_class': 1,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'seed':2019\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, Lasso, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from mlknn import MLkNN\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate=0.005,n_estimators=100,max_depth=5,min_samples_leaf=20,\n",
    "                                min_samples_split=600,subsample=0.7,random_state=2019)\n",
    "LR = LogisticRegression(penalty=\"l2\",C=1.0)\n",
    "Eln = ElasticNet()\n",
    "Las = Lasso(alpha=0.2)\n",
    "LRR = RidgeCV()\n",
    "\n",
    "pre_type = \"db6\"#\"db6\"#\"sym\"\n",
    "labels = pd.read_csv(\"/media/jdcloud/\" + \"REFERENCE.csv\")\n",
    "\n",
    "index = np.arange(6689)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "x_train = np.hstack(train[:-2])\n",
    "x_train = np.column_stack([x_train,train[-1]])\n",
    "x_train = np.column_stack([x_train,train[-2]])\n",
    "#train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.333, random_state=0)\n",
    "#train = lgb.Dataset(x_train,label=y_train)\n",
    "#valid = lgb.Dataset(valid_x, label=valid_y\n",
    "#gbm = lgb.train(params,train,num_boost_round=1000,#valid_sets=valid,early_stopping_rounds=5)\n",
    "\n",
    "#clf = MLkNN(k=8)\n",
    "\n",
    "clf = OneVsRestClassifier(LR)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train data f1_score  : 0.9425787389674495\n",
      "f1 score of ab 0 is 0.9676716335176512\n",
      "f1 score of ab 1 is 0.9870324128569159\n",
      "f1 score of ab 2 is 0.9664490036021178\n",
      "f1 score of ab 3 is 0.985171184676515\n",
      "f1 score of ab 4 is 0.9591266834502481\n",
      "f1 score of ab 5 is 0.9724710614168124\n",
      "f1 score of ab 6 is 0.9651989734821169\n",
      "f1 score of ab 7 is 0.9573247757461305\n",
      "f1 score of ab 8 is 0.9528357405368497\n",
      "f1 score of ab 9 is 0.9599398212397567\n",
      " train data hamming_loss  : 0.012498131260278069\n",
      " train data precision recall f1  : (0.9458314645936113, 0.944941944485972, 0.9417092566741244, None)\n"
     ]
    }
   ],
   "source": [
    "# LR_clf = joblib.load(\"LR_ensemble.pkl\")\n",
    "# MLkNN_clf = joblib.load(\"MLkNN_ensemble.pkl\")\n",
    "\n",
    "# y_pred_LR = LR_clf.predict(x_train)\n",
    "# y_pred_proba_LR = LR_clf.predict_proba(x_train)\n",
    "\n",
    "# y_pred_MLkNN = MLkNN_clf.predict(x_train).toarray()\n",
    "# y_pred_proba_MLkNN = MLkNN_clf.predict_proba(x_train).toarray()\n",
    "\n",
    "# #y_pred_MLkNN[:,7] = y_pred_LR[:,7]\n",
    "# #y_pred_proba_MLkNN[:,7] = y_pred_proba_LR[:,7]\n",
    "# y_pred = y_pred_MLkNN\n",
    "# y_pred_proba_train = y_pred_proba_MLkNN\n",
    "\n",
    "y_pred = clf.predict(x_train)#.toarray()\n",
    "y_pred_proba_train = clf.predict(x_train)#.toarray()\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "    \n",
    "print(\" train data hamming_loss  :\", hamming_loss(y_train, y_pred)) \n",
    "print(\" train data precision recall f1  :\", prf(y_train, y_pred,average=\"samples\"))# 'micro', 'weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = np.hstack(test[:-2])\n",
    "out = np.column_stack([out,test[-1]])\n",
    "out = np.column_stack([out,test[-2]])\n",
    "# LR_clf = joblib.load(\"LR_ensemble.pkl\")\n",
    "# MLkNN_clf = joblib.load(\"MLkNN_ensemble.pkl\")\n",
    "\n",
    "# y_pred_LR = LR_clf.predict(out)\n",
    "# y_pred_proba_LR = LR_clf.predict_proba(out)\n",
    "\n",
    "# y_pred_MLkNN = MLkNN_clf.predict(out).toarray()\n",
    "# y_pred_proba_MLkNN = MLkNN_clf.predict_proba(out).toarray()\n",
    "\n",
    "# y_pred_MLkNN[:,7] = y_pred_LR[:,7]\n",
    "# y_pred_proba_MLkNN[:,7] = y_pred_proba_LR[:,7]\n",
    "# y_pred = y_pred_MLkNN\n",
    "# y_pred_proba = y_pred_proba_MLkNN\n",
    "\n",
    "y_pred = clf.predict(out)#.toarray()\n",
    "y_pred_proba = clf.predict_proba(out)#.toarray()\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "pred = []\n",
    "for j in range(y_pred.shape[0]):\n",
    "    pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "\n",
    "''' ''' \n",
    "for i, val in enumerate(pred):\n",
    "    if val == []:\n",
    "        pass\n",
    "        #for i_p, val_p in enumerate(y_pred_proba[i]):\n",
    "        #    if val_p >= 0.4:\n",
    "        #        pred[i].append(i_p)    # f1 == 0.832\n",
    "                \n",
    "        if y_pred_proba[i][np.argmax(y_pred_proba[i])] >= 0.3:\n",
    "            pred[i] = [np.argmax(y_pred_proba[i])]     # f1 == 0.833  0.4\n",
    "\n",
    "val_dataset_path = '/media/jdcloud' + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('jupyter_answers_densenet_{}_0809.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(10 - len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Save\n",
    "# joblib.dump(clf,\"MLkNN8_ensemble.pkl\")#LR_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold : [0.7 0.5 0.5 0.4 0.6 0.2 0.3 0.4 0.5 0.3]\n",
      " train data f1_score  : 0.9226479088215832\n",
      "f1 score of ab 0 is 0.9576243119144202\n",
      "f1 score of ab 1 is 0.9794953051205606\n",
      "f1 score of ab 2 is 0.951682745472497\n",
      "f1 score of ab 3 is 0.982355844958436\n",
      "f1 score of ab 4 is 0.9477985152051004\n",
      "f1 score of ab 5 is 0.9721827424838709\n",
      "f1 score of ab 6 is 0.958814994471973\n",
      "f1 score of ab 7 is 0.9415063228088438\n",
      "f1 score of ab 8 is 0.9398197061954244\n",
      "f1 score of ab 9 is 0.9297211830048123\n",
      " train data hamming_loss  : 0.016474809388548364\n",
      " train data precision recall f1  : (0.9262096975133303, 0.9349429411471569, 0.9257665392364153, None)\n"
     ]
    }
   ],
   "source": [
    "thr = 0.6\n",
    "#out1 = thr * (0.1 * train[0] + 0.3 * train[1] + 0.6 * train[2])\n",
    "#out2 = (1 - thr) * (1.0 * train[3])\n",
    "thr0 = 0; thr1 = 0.4; thr2 = 0.; thr3 = 0.0; thr4 = 0.; thr5 = 0.6;\n",
    "train_y = thr0*train[0] + thr1*train[1] + thr2*train[2] + thr3*train[3] + thr4*train[4] + thr5*train[5]\n",
    "\n",
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(train_y.shape[1])\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "    y_prob = np.array(train_y[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if train_y[i, j] >= best_threshold[j] else 0 for j in range(train_y.shape[1])]\n",
    "          for i in range(len(train_y))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "print(\" train data hamming_loss  :\", hamming_loss(y_train, y_pred)) \n",
    "print(\" train data precision recall f1  :\", prf(y_train, y_pred,average=\"samples\"))# 'micro', 'weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0.6, 0.5, 0.5, 0.6, 0.2, 0.2, 0.7, 0.5, 0.4])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = thr0 * test[0]# + thr1 * test[1] + thr2 * test[2]# + thr3 * test[3] + thr4 * test[4] + thr5 * test[5]\n",
    "\n",
    "y_pred_test = np.array(\n",
    "    [[1 if out[i, j] >= best_threshold[j] else 0 for j in range(out.shape[1])] for i in range(len(out))])\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "test_y = y_pred_test\n",
    "\n",
    "y_pred = [[1 if test_y[i, j] >= best_threshold[j] else 0 for j in range(test_y.shape[1])]\n",
    "          for i in range(len(test_y))]\n",
    "pred = []\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "    \n",
    "for i, val in enumerate(pred):\n",
    "    ''' \n",
    "    if 0 in val and len(val) > 1:\n",
    "        flag = 0\n",
    "        for j in val:\n",
    "            if (test_y[i][0] - best_threshold[0]) > (test_y[i][j] - best_threshold[j]):\n",
    "                pass\n",
    "            else:\n",
    "                flag = 1\n",
    "        if flag == 1:\n",
    "            pred[i] = val[1:]\n",
    "        else:\n",
    "            pred[i] = val[0]\n",
    "    '''\n",
    "    if len(val) == 0:\n",
    "        pred[i] = [np.argmin(np.abs(best_threshold - out_test[i]))]\n",
    "        \n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('jupyter_answers_densenet_{}_0803.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(10 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.833 and 0.842 ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train data f1_score  : 0.9211840609081451\n",
      "f1 score of ab 0 is 0.9603084096823336\n",
      "f1 score of ab 1 is 0.9793386425479889\n",
      "f1 score of ab 2 is 0.9513702179183157\n",
      "f1 score of ab 3 is 0.9822910749256628\n",
      "f1 score of ab 4 is 0.9444799508628059\n",
      "f1 score of ab 5 is 0.9696737288321967\n",
      "f1 score of ab 6 is 0.9584407222873685\n",
      "f1 score of ab 7 is 0.9350094053104944\n",
      "f1 score of ab 8 is 0.9451176555412449\n",
      "f1 score of ab 9 is 0.930177490596017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(penalty=\"l2\",C=1.0)\n",
    "\n",
    "\n",
    "pre_type = \"db6\"#\"db6\"#\"sym\"\n",
    "labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "\n",
    "index = np.arange(6689)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "x_train = np.hstack([train_y,y_pred_proba_train])\n",
    "\n",
    "clf = OneVsRestClassifier(LR)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 20 features per sample; expecting 62",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-6bfe28390548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_proba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_predict_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                 \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_predict_binary\u001b[0;34m(estimator, X)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# probabilities of the positive class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 262\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 20 features per sample; expecting 62"
     ]
    }
   ],
   "source": [
    "out = 0.4 * test[0] + 0.6 * test[1]\n",
    "x_test = np.hstack([out,y_pred_proba])\n",
    "y_pred = clf.predict(x_test)\n",
    "y_pred_proba = clf.predict_proba(x_test)\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "pred = []\n",
    "for j in range(y_pred.shape[0]):\n",
    "    pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "\n",
    "''' ''' \n",
    "for i, val in enumerate(pred):\n",
    "    if val == []:\n",
    "        pass\n",
    "        #for i_p, val_p in enumerate(y_pred_proba[i]):\n",
    "        #    if val_p >= 0.4:\n",
    "        #        pred[i].append(i_p)    # f1 == 0.832\n",
    "                \n",
    "        if y_pred_proba[i][np.argmax(y_pred_proba[i])] >= 0.4:\n",
    "            pred[i] = [np.argmax(y_pred_proba[i])]     # f1 == 0.833\n",
    "\n",
    "\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('jupyter_answers_densenet_{}_0806.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(10 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quarter-final data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quarter_labels= pd.read_csv(\"/media/uuser/data/final_codes/final_run_semi/reference.csv\")\n",
    "quarter_index = np.arange(6500)\n",
    "quarter_y_train = preprocess_y(quarter_labels, quarter_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quarter_train = []\n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_one_net_fold2.csv\").values) \n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_f0819_10net_fold.csv\").values) \n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_attention_10net_fold2.csv\").values) \n",
    "\n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_4block_10net_fold0.csv\").values)\n",
    "# quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_4block_10net_fold1.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_4block_10net_fold2.csv\").values)\n",
    "\n",
    "#quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_one_net_fold0.csv\").values)\n",
    "#quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_one_net_fold1.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_one_net_fold2.csv\").values)\n",
    "\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_10net_fold0.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_10net_fold1.csv\").values)\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"attention_10net_fold2.csv\").values)\n",
    "\n",
    "quarter_train.append(pd.read_csv(path+\"quarter_final/\"+\"densenet_f0819_10net_fold.csv\").values)\n",
    "\n",
    "''' '''\n",
    "# quarter_train_y = thr0*quarter_train[0] + thr1*quarter_train[1] + thr2*quarter_train[2] + \\\n",
    "#                 thr3*quarter_train[3] + thr4*quarter_train[4] + thr5*quarter_train[5]\n",
    "\n",
    "# print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "# quarter_y_pred = np.array([[1 if quarter_train_y[i, j] >= best_threshold[j] else 0 \n",
    "#                             for j in range(quarter_train_y.shape[1])]\n",
    "#           for i in range(len(quarter_train_y))])\n",
    "\n",
    "# Restore\n",
    "# LR_clf = joblib.load(\"LR_ensemble.pkl\")\n",
    "# MLkNN_clf = joblib.load(\"MLkNN_ensemble.pkl\")\n",
    "\n",
    "# quarter_x_train = np.hstack(quarter_train)\n",
    "# quarter_y_pred_LR = LR_clf.predict(quarter_x_train)\n",
    "# quarter_y_pred_MLkNN = MLkNN_clf.predict(quarter_x_train).toarray()\n",
    "\n",
    "# quarter_y_pred_MLkNN[:,7] = quarter_y_pred_LR[:,7]\n",
    "# quarter_y_pred = quarter_y_pred_MLkNN\n",
    "\n",
    "quarter_y_pred = clf.predict(quarter_x_train).toarray()\n",
    "print(\" train data f1_score  :\", f1_score(quarter_y_train, quarter_y_pred, average='macro'))\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(quarter_y_train[:, i], quarter_y_pred[:, i], average='macro')))\n",
    "    \n",
    "print(\" train data hamming_loss  :\", hamming_loss(quarter_y_train, quarter_y_pred)) \n",
    "print(\" train data precision recall f1  :\", prf(quarter_y_train, quarter_y_pred,average=\"samples\"))# 'micro', 'weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6276, 1: 224})"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(quarter_y_train[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6272, 1: 228})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(quarter_y_pred[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8732"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.843 * 10 - 0.498 + 0.8) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " train data f1_score  : 0.9226479088215832\n",
    "f1 score of ab 0 is 0.9576243119144202\n",
    "f1 score of ab 1 is 0.9794953051205606\n",
    "f1 score of ab 2 is 0.951682745472497\n",
    "f1 score of ab 3 is 0.982355844958436\n",
    "f1 score of ab 4 is 0.9477985152051004\n",
    "f1 score of ab 5 is 0.9721827424838709\n",
    "f1 score of ab 6 is 0.958814994471973\n",
    "f1 score of ab 7 is 0.9415063228088438\n",
    "f1 score of ab 8 is 0.9398197061954244\n",
    "f1 score of ab 9 is 0.9297211830048123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
