{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Import Package\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "''' '''\n",
    "# from resnet_ecg.utils import one_hot,get_batches\n",
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "from resnet_ecg.densemodel import Net\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, load_model\n",
    "import keras\n",
    "import pywt\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "'''\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "'''\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "# path of training data\n",
    "path = '/media/jdcloud/'#\"/media/uuser/data/finals/\"#'/media/jdcloud/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.conv_subsample_lengths = [1, 2, 1, 2, 1, 2, 1, 2]\n",
    "        self.conv_filter_length = 32\n",
    "        self.conv_num_filters_start = 12\n",
    "        self.conv_init = \"he_normal\"\n",
    "        self.conv_activation = \"relu\"\n",
    "        self.conv_dropout = 0.5\n",
    "        self.conv_num_skip = 2\n",
    "        self.conv_increase_channels_at = 2\n",
    "        self.batch_size = 32  # 128\n",
    "        self.input_shape = [2560, 12]  # [1280, 1]\n",
    "        self.num_categories = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 0.1\n",
    "        if epoch >= 10 and epoch < 20:\n",
    "            lr = 0.01\n",
    "        if epoch >= 20:\n",
    "            lr = 0.001\n",
    "        # print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "def wavelet(ecg, wavefunc, lv, m, n):  #\n",
    "\n",
    "    coeff = pywt.wavedec(ecg, wavefunc, mode='sym', level=lv)  #\n",
    "    # sgn = lambda x: 1 if x > 0 else -1 if x < 0 else 0\n",
    "\n",
    "    for i in range(m, n + 1):\n",
    "        cD = coeff[i]\n",
    "        for j in range(len(cD)):\n",
    "            Tr = np.sqrt(2 * np.log(len(cD)))\n",
    "            if cD[j] >= Tr:\n",
    "                coeff[i][j] = np.sign(cD[j]) - Tr\n",
    "            else:\n",
    "                coeff[i][j] = 0\n",
    "\n",
    "    denoised_ecg = pywt.waverec(coeff, wavefunc)\n",
    "    return denoised_ecg\n",
    "\n",
    "\n",
    "def wavelet_db6(sig):\n",
    "    \"\"\"\n",
    "    R J, Acharya U R, Min L C. ECG beat classification using PCA, LDA, ICA and discrete\n",
    "     wavelet transform[J].Biomedical Signal Processing and Control, 2013, 8(5): 437-448.\n",
    "    param sig: 1-D numpy Array\n",
    "    return: 1-D numpy Array\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(sig, 'db6', level=9)\n",
    "    coeffs[-1] = np.zeros(len(coeffs[-1]))\n",
    "    coeffs[-2] = np.zeros(len(coeffs[-2]))\n",
    "    coeffs[0] = np.zeros(len(coeffs[0]))\n",
    "    sig_filt = pywt.waverec(coeffs, 'db6')\n",
    "    return sig_filt\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "def read_data_seg(data_path, split=\"Train\", preprocess=False, fs=500, newFs=256, winSecond=10, winNum=10, n_index=0,pre_type=\"sym\"):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    # n_index = 0\n",
    "    n_class = 9\n",
    "    winSize = winSecond * fs\n",
    "    new_winSize = winSecond * newFs\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    # label_path = os.path.join(data_path, \"reference.txt\")\n",
    "    # labels = pd.read_csv(label_path, sep='\\t',header = None)\n",
    "    # labels = pd.read_csv(\"reference.csv\")\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    # print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12  # len(channel_files)\n",
    "    # posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "\n",
    "    X = np.zeros((len(channel_files), new_winSize, n_channels)).astype('float32')\n",
    "    i_ch = 0\n",
    "\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1', 'II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II', 'aVR', 'V2', 'V5']\n",
    "    channel_post_name = ['III', 'aVF', 'V3', 'V6']\n",
    "\n",
    "    for i_ch, fil_ch in enumerate(channel_files[:]):  # tqdm\n",
    "\n",
    "        if i_ch % 1000 == 0:\n",
    "            print(i_ch)\n",
    "\n",
    "        ecg = sio.loadmat(os.path.join(path_signals, fil_ch))\n",
    "        ecg_length = ecg[\"I\"].shape[1]\n",
    "\n",
    "        if ecg_length > fs * winNum * winSecond:\n",
    "            print(\" too long !!!\", ecg_length)\n",
    "            ecg_length = fs * winNum * winSecond\n",
    "        if ecg_length < 4500:\n",
    "            print(\" too short !!!\", ecg_length)\n",
    "            break\n",
    "\n",
    "        slide_steps = int((ecg_length - winSize) / winSecond)\n",
    "\n",
    "        if ecg_length <= 4500:\n",
    "            slide_steps = 0\n",
    "\n",
    "        ecg_channels = np.zeros((new_winSize, n_channels)).astype('float32')\n",
    "\n",
    "        for i_n, ch_name in enumerate(channel_name):\n",
    "\n",
    "            ecg_channels[:, i_n] = signal.resample(ecg[ch_name]\n",
    "                                                   [:, n_index * slide_steps:n_index * slide_steps + winSize].T\n",
    "                                                   , new_winSize).T\n",
    "            if preprocess:\n",
    "                if pre_type == \"sym\":\n",
    "                    ecg_channels[:, i_n] = ecg_preprocessing(ecg_channels[:, i_n].reshape(1, new_winSize), 'sym8', 8, 3,\n",
    "                                                             newFs, removebaseline=False, normalize=False)[0]\n",
    "                elif pre_type == \"db4\":\n",
    "                    ecg_channels[:, i_n] = wavelet(ecg_channels[:, i_n], 'db4', 4, 2, 4)\n",
    "                elif pre_type == \"db6\":\n",
    "                    ecg_channels[:, i_n] = wavelet_db6(ecg_channels[:, i_n])\n",
    "\n",
    "                # ecg_channels[:, i_n] = (ecg_channels[:, i_n]-np.mean(ecg_channels[:, i_n]))/np.std(ecg_channels[:, i_n])\n",
    "            else:\n",
    "                pass\n",
    "                print(\" no preprocess !!! \")\n",
    "\n",
    "        X[i_ch, :, :] = ecg_channels\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_y(labels, y, num_class=10):\n",
    "    bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1, label_nona.shape[0]):\n",
    "            bin_label[i, int(label_nona[j])] = 1\n",
    "    return bin_label\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    # ' Generates data for Keras '\n",
    "\n",
    "    def __init__(self, list_IDs, labels, quarter_labels,batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        # 'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.quarter_labels = quarter_labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # 'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # 'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # 'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size,  *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.n_classes), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            IDsplit = ID.split(\"_\")\n",
    "            if IDsplit[0] == \"quarter\":\n",
    "                X[i,] = np.load(\"/media/uuser/data/ysecgfinals/training_data/\" + ID.split(\"_\")[1] + \".npy\")\n",
    "\n",
    "                y[i,:] = preprocess_y(self.quarter_labels,self.quarter_labels[self.quarter_labels[\"File_name\"] == IDsplit[0]+\"_\"+IDsplit[1]].index)\n",
    "\n",
    "            else:\n",
    "                # Store sample\n",
    "                X[i,] = np.load(\"training_data/\" + ID+\".npy\")\n",
    "                # Store class\n",
    "                y[i,:] = preprocess_y(self.labels,self.labels[self.labels[\"File_name\"] == ID.split(\"_\")[0]].index)\n",
    "\n",
    "        # X_list = [(X[:, i]-np.mean(X[:, i]))/np.std(X[:, i]) for i in range(10)]\n",
    "        X_list = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n",
    "        del X\n",
    "\n",
    "        return X_list, y  # keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "\n",
    "def add_compile(model, config):\n",
    "    optimizer = SGD(lr=config.lr_schedule(0), momentum=0.9)  # Adam()#\n",
    "    model.compile(loss='binary_crossentropy',  # weighted_loss,#'binary_crossentropy',\n",
    "                  optimizer='adam',  # optimizer,#'adam',\n",
    "                  metrics=['accuracy', fmeasure, precision])  # recall\n",
    "    # ['accuracy',fbetaMacro,recallMacro,precisionMacro])\n",
    "    # ['accuracy',fmeasure,recall,precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27453,)\n",
      "(13407,)\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "428/428 [==============================] - 268s 626ms/step - loss: 0.4355 - acc: 0.8911 - fmeasure: 0.4670 - precision: 0.7087\n",
      "Epoch 2/30\n",
      "428/428 [==============================] - 226s 527ms/step - loss: 0.2861 - acc: 0.9126 - fmeasure: 0.6096 - precision: 0.7711\n",
      "Epoch 3/30\n",
      "428/428 [==============================] - 280s 655ms/step - loss: 0.2590 - acc: 0.9182 - fmeasure: 0.6461 - precision: 0.7791\n",
      "Epoch 4/30\n",
      "428/428 [==============================] - 296s 692ms/step - loss: 0.2415 - acc: 0.9241 - fmeasure: 0.6792 - precision: 0.7917\n",
      "Epoch 5/30\n",
      "428/428 [==============================] - 298s 697ms/step - loss: 0.2293 - acc: 0.9289 - fmeasure: 0.7003 - precision: 0.8059\n",
      "Epoch 6/30\n",
      "428/428 [==============================] - 285s 665ms/step - loss: 0.2204 - acc: 0.9315 - fmeasure: 0.7131 - precision: 0.8143\n",
      "Epoch 7/30\n",
      "428/428 [==============================] - 290s 678ms/step - loss: 0.2114 - acc: 0.9341 - fmeasure: 0.7276 - precision: 0.8211\n",
      "Epoch 8/30\n",
      "428/428 [==============================] - 299s 698ms/step - loss: 0.2084 - acc: 0.9356 - fmeasure: 0.7329 - precision: 0.8223\n",
      "Epoch 9/30\n",
      "428/428 [==============================] - 301s 704ms/step - loss: 0.2035 - acc: 0.9373 - fmeasure: 0.7393 - precision: 0.8279\n",
      "Epoch 10/30\n",
      "428/428 [==============================] - 383s 895ms/step - loss: 0.2026 - acc: 0.9381 - fmeasure: 0.7425 - precision: 0.8312\n",
      "Epoch 11/30\n",
      "428/428 [==============================] - 321s 751ms/step - loss: 0.1985 - acc: 0.9402 - fmeasure: 0.7537 - precision: 0.8375\n",
      "Epoch 12/30\n",
      "428/428 [==============================] - 240s 561ms/step - loss: 0.1980 - acc: 0.9404 - fmeasure: 0.7520 - precision: 0.8342\n",
      "Epoch 13/30\n",
      "428/428 [==============================] - 222s 520ms/step - loss: 0.2000 - acc: 0.9408 - fmeasure: 0.7557 - precision: 0.8373\n",
      "Epoch 14/30\n",
      "428/428 [==============================] - 260s 607ms/step - loss: 0.1938 - acc: 0.9425 - fmeasure: 0.7601 - precision: 0.8414\n",
      "Epoch 15/30\n",
      "428/428 [==============================] - 281s 657ms/step - loss: 0.1890 - acc: 0.9435 - fmeasure: 0.7629 - precision: 0.8438\n",
      "Epoch 16/30\n",
      "428/428 [==============================] - 231s 540ms/step - loss: 0.1899 - acc: 0.9426 - fmeasure: 0.7582 - precision: 0.8428\n",
      "Epoch 17/30\n",
      "428/428 [==============================] - 248s 580ms/step - loss: 0.1869 - acc: 0.9449 - fmeasure: 0.7699 - precision: 0.8511\n",
      "Epoch 18/30\n",
      "428/428 [==============================] - 291s 681ms/step - loss: 0.1921 - acc: 0.9432 - fmeasure: 0.7557 - precision: 0.8452\n",
      "Epoch 19/30\n",
      "428/428 [==============================] - 347s 811ms/step - loss: 0.1886 - acc: 0.9448 - fmeasure: 0.7664 - precision: 0.8511\n",
      "Epoch 20/30\n",
      "428/428 [==============================] - 361s 843ms/step - loss: 0.1763 - acc: 0.9494 - fmeasure: 0.7910 - precision: 0.8623\n",
      "Epoch 21/30\n",
      "428/428 [==============================] - 387s 905ms/step - loss: 0.1844 - acc: 0.9455 - fmeasure: 0.7656 - precision: 0.8525\n",
      "Epoch 22/30\n",
      "428/428 [==============================] - 257s 600ms/step - loss: 0.1771 - acc: 0.9483 - fmeasure: 0.7848 - precision: 0.8580\n",
      "Epoch 23/30\n",
      "428/428 [==============================] - 273s 638ms/step - loss: 0.1756 - acc: 0.9487 - fmeasure: 0.7849 - precision: 0.8611\n",
      "Epoch 24/30\n",
      "428/428 [==============================] - 278s 650ms/step - loss: 0.1777 - acc: 0.9481 - fmeasure: 0.7806 - precision: 0.8639\n",
      "Epoch 25/30\n",
      "428/428 [==============================] - 281s 656ms/step - loss: 0.1809 - acc: 0.9469 - fmeasure: 0.7747 - precision: 0.8568\n",
      "Epoch 26/30\n",
      "428/428 [==============================] - 278s 650ms/step - loss: 0.1794 - acc: 0.9484 - fmeasure: 0.7792 - precision: 0.8624\n",
      "Epoch 27/30\n",
      "428/428 [==============================] - 321s 749ms/step - loss: 0.1858 - acc: 0.9462 - fmeasure: 0.7703 - precision: 0.8569\n",
      "Epoch 28/30\n",
      "428/428 [==============================] - 342s 799ms/step - loss: 0.1826 - acc: 0.9477 - fmeasure: 0.7758 - precision: 0.8638\n",
      "Epoch 29/30\n",
      "428/428 [==============================] - 270s 631ms/step - loss: 0.1748 - acc: 0.9497 - fmeasure: 0.7849 - precision: 0.8644\n",
      "Epoch 30/30\n",
      "428/428 [==============================] - 284s 663ms/step - loss: 0.1802 - acc: 0.9479 - fmeasure: 0.7760 - precision: 0.8603\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = path + \"/Train/\"\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "\n",
    "train_files = os.listdir(train_dataset_path)\n",
    "train_files.sort()\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "labels_en = pd.read_csv(path + \"kfold_labels_en.csv\")\n",
    "#data_info = pd.read_csv(path + \"data_info.csv\")\n",
    "\n",
    "input_size = (2560, 12)\n",
    "net_num = 10\n",
    "inputs_list = [Input(shape=input_size) for _ in range(net_num)]\n",
    "net = Net()\n",
    "outputs = net.nnet(inputs_list, 0.5, num_classes=10, attention=False)\n",
    "model = Model(inputs=inputs_list, outputs=outputs)\n",
    "# print(model.summary())\n",
    "\n",
    "raw_IDs = labels_en[\"File_name\"].values.tolist()\n",
    "\n",
    "for j in range(4):\n",
    "    for ids in labels[labels.label1==4][\"File_name\"]:\n",
    "        if ids in raw_IDs:\n",
    "            raw_IDs.append(ids)\n",
    "\n",
    "for j in range(2):\n",
    "    for ids in labels[labels.label1==7][\"File_name\"]:\n",
    "        if ids in raw_IDs:\n",
    "            raw_IDs.append(ids)\n",
    "\n",
    "for j in range(1):\n",
    "    for ids in labels[labels.label1==9][\"File_name\"]:\n",
    "        if ids in raw_IDs:\n",
    "            raw_IDs.append(ids)\n",
    "                        \n",
    "extend_db4_IDs = [i + \"_db4\" for i in raw_IDs]\n",
    "extend_db6_IDs = [i + \"_db6\" for i in raw_IDs]\n",
    "all_IDs = raw_IDs + extend_db4_IDs + extend_db6_IDs\n",
    "\n",
    "train_labels = labels_en[\"label1\"].values\n",
    "all_train_labels = np.hstack((train_labels, train_labels, train_labels))\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': (10, 2560),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 10,\n",
    "          'n_channels': 12,\n",
    "          'shuffle': True}\n",
    "\n",
    "en_amount = 1\n",
    "model_path = './official_densenet_model/'\n",
    "index = np.arange(27453)#20067\n",
    "np.random.shuffle(index)\n",
    "\n",
    "index_train = index[:27453]\n",
    "index_valid = index[14046:]\n",
    "\n",
    "tr_IDs = np.array(all_IDs)[index_train]\n",
    "val_IDs = np.array(all_IDs)[index_valid]\n",
    "\n",
    "print(tr_IDs.shape)\n",
    "print(val_IDs.shape)\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(tr_IDs, labels,quarter_labels=None, **params)\n",
    "#validation_generator = DataGenerator(val_IDs, labels, **params)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=model_path + 'densenet_extend_weights-best_one_fold_all_data.hdf5',\n",
    "                               monitor='val_fmeasure', verbose=1, save_best_only=True,\n",
    "                               save_weights_only=True,\n",
    "                               mode='max')  # val_fmeasure\n",
    "reduce = ReduceLROnPlateau(monitor='val_fmeasure', factor=0.5, patience=2, verbose=1, min_delta=1e-4,\n",
    "                           mode='max')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_fmeasure', patience=5)\n",
    "\n",
    "config = Config()\n",
    "add_compile(model, config)\n",
    "\n",
    "callback_lists = [checkpointer, reduce]\n",
    "\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                              validation_data=None,#validation_generator,\n",
    "                              use_multiprocessing=False,\n",
    "                              epochs=30,\n",
    "                              verbose=1,\n",
    "                              callbacks=None)#callback_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path + 'densenet_extend_weights-best_one_fold_all_data.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_y(labels, y, num_class=10):\n",
    "    bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1, label_nona.shape[0]):\n",
    "            bin_label[i, int(label_nona[j])] = 1\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "'''   '''\n",
    "pre_type = \"sym\"#\"db6\"#\"sym\"\n",
    "\n",
    "labels = pd.read_csv(path + \"REFERENCE.csv\")\n",
    "raw_IDs = labels[\"File_name\"].values.tolist()\n",
    "\n",
    "IDs = {}\n",
    "IDs[\"sym\"] = raw_IDs\n",
    "IDs[\"db4\"] = [i + \"_db4\" for i in raw_IDs]\n",
    "IDs[\"db6\"] = [i + \"_db6\" for i in raw_IDs]\n",
    "\n",
    "#######################\n",
    "input_size = (2560, 12)\n",
    "net_num = 10\n",
    "inputs_list = [Input(shape=input_size) for _ in range(net_num)]\n",
    "net = Net()\n",
    "outputs = net.nnet(inputs_list, 0.5, num_classes=10, attention=False)\n",
    "model = Model(inputs=inputs_list, outputs=outputs) # without attention and maxpooling and separableconv1d   f=0.819\n",
    "\n",
    "model_path = './official_densenet_model/'\n",
    "model_name = \"densenet_extend_weights-best_one_fold_all_data.hdf5\"#'densenet_extend_weights-best_one_fold_0730.hdf5'\n",
    "\n",
    "model.load_weights(model_path + model_name)\n",
    "##########################\n",
    "\n",
    "X = np.empty((6689, 10, 2560, 12))\n",
    "for i, ID in enumerate(IDs[pre_type]):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    X[i,] = np.load(\"training_data/\" + ID + \".npy\")\n",
    "#train_x = [(X[:, i]-np.mean(X[:, i]))/np.std(X[:, i]) for i in range(10)]\n",
    "train_x = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold : [0.5 0.1 0.1 0.3 0.2 0.2 0.4 0.2 0.4 0.2]\n",
      " train data f1_score  : 0.8305535508012308\n",
      "f1 score of ab 0 is 0.9075643559693143\n",
      "f1 score of ab 1 is 0.9613049154153642\n",
      "f1 score of ab 2 is 0.9227571272570245\n",
      "f1 score of ab 3 is 0.9545073322640414\n",
      "f1 score of ab 4 is 0.8570014043977652\n",
      "f1 score of ab 5 is 0.9483618173810205\n",
      "f1 score of ab 6 is 0.8916374838772433\n",
      "f1 score of ab 7 is 0.8517114480801631\n",
      "f1 score of ab 8 is 0.9031904923022938\n",
      "f1 score of ab 9 is 0.852356753434403\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "index = np.arange(6689)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "blend_train = model.predict(train_x)\n",
    "\n",
    "\n",
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(blend_train.shape[1])\n",
    "\n",
    "for i in range(blend_train.shape[1]):\n",
    "    y_prob = np.array(blend_train[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if blend_train[i, j] >= best_threshold[j] else 0 for j in range(blend_train.shape[1])]\n",
    "          for i in range(len(blend_train))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "\n",
    "net_num = 10\n",
    "test_x = [read_data_seg(path, split='Val', preprocess=True, n_index=i, pre_type=pre_type) for i in range(net_num)]\n",
    "\n",
    "out = model.predict(test_x)\n",
    "y_pred_test = np.array(\n",
    "    [[1 if out[i, j] >= best_threshold[j] else 0 for j in range(out.shape[1])] for i in range(len(out))])\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "test_y = y_pred_test\n",
    "\n",
    "y_pred = [[1 if test_y[i, j] >= best_threshold[j] else 0 for j in range(test_y.shape[1])]\n",
    "          for i in range(len(test_y))]\n",
    "pred = []\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(10) if y_pred[j][i] == 1])\n",
    "\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('jupyter_answers_densenet_{}_0809.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8', 'label9', 'label10'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(10 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
