{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from utils import extract_basic_features\n",
    "\n",
    "#import wfdb\n",
    "import os\n",
    "#import wfdb.processing as wp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "#from utils import find_noise_features, extract_basic_features\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import random as rn\n",
    "#from lightgbm import LGBMClassifier\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "import scipy.io as sio\n",
    "\n",
    "#from resnet_ecg.utils import one_hot,get_batches\n",
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler,EarlyStopping,ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "path = '/media/jdcloud/'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session )\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.conv_subsample_lengths = [1, 2, 1, 2, 1, 2, 1, 2]\n",
    "        self.conv_filter_length = 32\n",
    "        self.conv_num_filters_start = 12\n",
    "        self.conv_init = \"he_normal\"\n",
    "        self.conv_activation = \"relu\"\n",
    "        self.conv_dropout = 0.5\n",
    "        self.conv_num_skip = 2\n",
    "        self.conv_increase_channels_at = 2\n",
    "        self.batch_size = 32#128\n",
    "        self.input_shape = [2560, 12]#[1280, 1]\n",
    "        self.num_categories = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 0.1\n",
    "        if epoch >= 10 and epoch < 20:\n",
    "            lr = 0.01\n",
    "        if epoch >= 20:\n",
    "            lr = 0.001\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "\n",
    "def read_data_seg(data_path, split=\"Train\", preprocess=False, fs=500, newFs=256, winSecond=10, winNum=10, n_index=0):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    # n_index = 0\n",
    "    n_class = 9\n",
    "    winSize = winSecond * fs\n",
    "    new_winSize = winSecond * newFs\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    # label_path = os.path.join(data_path, \"reference.txt\")\n",
    "    # labels = pd.read_csv(label_path, sep='\\t',header = None)\n",
    "    # labels = pd.read_csv(\"reference.csv\")\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    # print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12  # len(channel_files)\n",
    "    # posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "\n",
    "    X = np.zeros((len(channel_files), new_winSize, n_channels)).astype('float32') \n",
    "    i_ch = 0\n",
    "\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1', 'II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II', 'aVR', 'V2', 'V5']\n",
    "    channel_post_name = ['III', 'aVF', 'V3', 'V6']\n",
    "\n",
    "    for i_ch, fil_ch in enumerate(channel_files[:]):  # tqdm\n",
    "        \n",
    "        if i_ch % 2000 == 0:\n",
    "            print(i_ch)\n",
    "            \n",
    "        ecg = sio.loadmat(os.path.join(path_signals, fil_ch))\n",
    "        ecg_length = ecg[\"I\"].shape[1]\n",
    "\n",
    "        if ecg_length > fs * winNum * winSecond:\n",
    "            print(\" too long !!!\", ecg_length)\n",
    "            ecg_length = fs * winNum * winSecond\n",
    "        if ecg_length < 4500:\n",
    "            print(\" too short !!!\", ecg_length)\n",
    "            break\n",
    "\n",
    "        slide_steps = int((ecg_length - winSize) / winSecond)\n",
    "\n",
    "        if ecg_length <= 4500:\n",
    "            slide_steps = 0\n",
    "\n",
    "        ecg_channels = np.zeros((new_winSize, n_channels)).astype('float32') \n",
    "\n",
    "        for i_n, ch_name in enumerate(channel_name):\n",
    "\n",
    "            ecg_channels[:, i_n] = signal.resample(ecg[ch_name]\n",
    "                                                   [:, n_index * slide_steps:n_index * slide_steps + winSize].T\n",
    "                                                   , new_winSize).T\n",
    "            if preprocess:\n",
    "                data = ecg_preprocessing(ecg_channels[:, i_n].reshape(1, new_winSize), 'sym8', 8, 3, newFs)\n",
    "                ecg_channels[:, i_n] = data[0]\n",
    "            else:\n",
    "                pass\n",
    "                ecg_channels[:, i_n] = ecg_channels[:, i_n]\n",
    "\n",
    "        X[i_ch, :, :] = ecg_channels\n",
    "\n",
    "    return X\n",
    "\n",
    "def read_train_data(path):\n",
    "\n",
    "    ecg12_seg0 = read_data_seg(path, n_index=0)\n",
    "    ecg12_seg1 = read_data_seg(path, n_index=1)\n",
    "    ecg12_seg2 = read_data_seg(path, n_index=2)\n",
    "    ecg12_seg3 = read_data_seg(path, n_index=3)\n",
    "    ecg12_seg4 = read_data_seg(path, n_index=4)\n",
    "\n",
    "    ecg12_seg5 = read_data_seg(path, n_index=5)\n",
    "    ecg12_seg6 = read_data_seg(path, n_index=6)\n",
    "    ecg12_seg7 = read_data_seg(path, n_index=7)\n",
    "    ecg12_seg8 = read_data_seg(path, n_index=8)\n",
    "    ecg12_seg9 = read_data_seg(path, n_index=9)\n",
    "\n",
    "    X = [ecg12_seg0, ecg12_seg1, ecg12_seg2, ecg12_seg3,\n",
    "         ecg12_seg4, ecg12_seg5, ecg12_seg6, ecg12_seg7,\n",
    "         ecg12_seg8, ecg12_seg9,\n",
    "           ]\n",
    "\n",
    "    del ecg12_seg0, ecg12_seg1, ecg12_seg2, ecg12_seg3, ecg12_seg4\n",
    "    del ecg12_seg5, ecg12_seg6, ecg12_seg7, ecg12_seg8, ecg12_seg9\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return X\n",
    "\n",
    "def read_test_data(path):\n",
    "\n",
    "    test_x_seg0 = read_data_seg(path, split='Val', n_index=0)\n",
    "    test_x_seg1 = read_data_seg(path, split='Val', n_index=1)\n",
    "    test_x_seg2 = read_data_seg(path, split='Val', n_index=2)\n",
    "    test_x_seg3 = read_data_seg(path, split='Val', n_index=3)\n",
    "    test_x_seg4 = read_data_seg(path, split='Val', n_index=4)\n",
    "\n",
    "    test_x_seg5 = read_data_seg(path, split='Val', n_index=5)\n",
    "    test_x_seg6 = read_data_seg(path, split='Val', n_index=6)\n",
    "    test_x_seg7 = read_data_seg(path, split='Val', n_index=7)\n",
    "    test_x_seg8 = read_data_seg(path, split='Val', n_index=8)\n",
    "    test_x_seg9 = read_data_seg(path, split='Val', n_index=9)\n",
    "\n",
    "    test_x = [test_x_seg0, test_x_seg1, test_x_seg2, test_x_seg3, test_x_seg4,\n",
    "              test_x_seg5, test_x_seg6, test_x_seg7, test_x_seg8, test_x_seg9,\n",
    "             ]\n",
    "\n",
    "    del test_x_seg0, test_x_seg1, test_x_seg2, test_x_seg3, test_x_seg4\n",
    "    del test_x_seg5, test_x_seg6, test_x_seg7, test_x_seg8, test_x_seg9\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return test_x\n",
    "\n",
    "def preprocess_y(labels,y,num_class=9):\n",
    "    bin_label = np.zeros((len(y),num_class)).astype('int8') \n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1,label_nona.shape[0]):\n",
    "            bin_label[i,int(label_nona[j])]=1\n",
    "    return bin_label\n",
    "\n",
    "\n",
    "def add_compile(model, config):\n",
    "    optimizer = SGD(lr=config.lr_schedule(0), momentum=0.9)  # Adam()#\n",
    "    model.compile(loss='binary_crossentropy',  # weighted_loss,#'binary_crossentropy',\n",
    "                  optimizer='adam',  # optimizer,#'adam',\n",
    "                  metrics=['accuracy', fmeasure, precision])#recall\n",
    "    # ['accuracy',fbetaMacro,recallMacro,precisionMacro])\n",
    "    # ['accuracy',fmeasure,recall,precision])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_dataset_path = path + \"/Train/\"\n",
    "    val_dataset_path = path + \"/Val/\"\n",
    "\n",
    "    train_files = os.listdir(train_dataset_path)\n",
    "    train_files.sort()\n",
    "    val_files = os.listdir(val_dataset_path)\n",
    "    val_files.sort()\n",
    "\n",
    "    labels = pd.read_csv(path+\"reference.csv\")\n",
    "    \n",
    "    labels_en = pd.read_csv(path + \"kfold_labels_en.csv\")\n",
    "    \n",
    "    #print(labels.head())\n",
    "\n",
    "    bin_label = np.zeros((6500,9))\n",
    "    for i in range(labels.shape[0]):\n",
    "        label_nona = labels.loc[i].dropna()\n",
    "        for j in range(1,label_nona.shape[0]):\n",
    "            bin_label[i,int(label_nona[j])]=1\n",
    "\n",
    "    cv_pred_all = 0\n",
    "    en_amount = 1\n",
    "\n",
    "    #labels_en = pd.read_csv(path + \"kfold_labels_en.csv\")\n",
    "    #print(labels_en.shape)\n",
    "    #print(labels_en.head())\n",
    "\n",
    "    #data_info = pd.read_csv(path + \"data_info.csv\")\n",
    "    #print(data_info.head())\n",
    "\n",
    "    train_index = np.arange(6500).astype('int16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(571, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels.label1==6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels.label2==6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels.label3==6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels.label4==6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels.label8==6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lab = 6.\n",
    "for i in range(labels.shape[0]):\n",
    "    if labels.iloc[i][\"label2\"] == lab:\n",
    "        labels.loc[i,\"label1\"] = lab\n",
    "    if labels.iloc[i][\"label3\"] == lab:\n",
    "        labels.loc[i,\"label1\"] = lab\n",
    "    if labels.iloc[i][\"label4\"] == lab:\n",
    "        labels.loc[i,\"label1\"] = lab\n",
    "    if labels.iloc[i][\"label5\"] == lab:\n",
    "        labels.loc[i,\"label1\"] = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels.label1==6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels.replace(1.,0.,inplace=True)\n",
    "labels.replace(2.,0.,inplace=True)\n",
    "labels.replace(3.,0.,inplace=True)\n",
    "labels.replace(4.,0.,inplace=True)\n",
    "labels.replace(5.,0.,inplace=True)\n",
    "labels.replace(6.,1.,inplace=True)\n",
    "labels.replace(7.,0.,inplace=True)\n",
    "labels.replace(8.,0.,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels[\"label1\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index shape : (6500,)\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index = train_index.astype(np.int16)\n",
    "\n",
    "train_index.sort()\n",
    "\n",
    "print(\"train_index shape :\",train_index.shape)\n",
    "#print(train_index)\n",
    "\n",
    "ecg12_seg0 = read_data_seg(path, n_index=0) \n",
    "ecg12_seg1 = read_data_seg(path, n_index=1) \n",
    "ecg12_seg2 = read_data_seg(path, n_index=2) \n",
    "ecg12_seg3 = read_data_seg(path, n_index=3) \n",
    "ecg12_seg4 = read_data_seg(path, n_index=4) \n",
    "\n",
    "ecg12_seg5 = read_data_seg(path, n_index=5)\n",
    "ecg12_seg6 = read_data_seg(path, n_index=6)\n",
    "ecg12_seg7 = read_data_seg(path, n_index=7)\n",
    "ecg12_seg8 = read_data_seg(path, n_index=8)\n",
    "ecg12_seg9 = read_data_seg(path, n_index=9)\n",
    "#train_x = np.array(read_train_data(path),dtype=np.float32)\n",
    "#test_x = read_test_data(path)\n",
    "\n",
    "test_x_seg0 = read_data_seg(path, split='Val', n_index=0)\n",
    "test_x_seg1 = read_data_seg(path, split='Val', n_index=1)\n",
    "test_x_seg2 = read_data_seg(path, split='Val', n_index=2)\n",
    "test_x_seg3 = read_data_seg(path, split='Val', n_index=3)\n",
    "test_x_seg4 = read_data_seg(path, split='Val', n_index=4)\n",
    "\n",
    "test_x_seg5 = read_data_seg(path, split='Val', n_index=5)\n",
    "test_x_seg6 = read_data_seg(path, split='Val', n_index=6)\n",
    "test_x_seg7 = read_data_seg(path, split='Val', n_index=7)\n",
    "test_x_seg8 = read_data_seg(path, split='Val', n_index=8)\n",
    "test_x_seg9 = read_data_seg(path, split='Val', n_index=9)\n",
    "\n",
    "test_x = [test_x_seg0, test_x_seg1, test_x_seg2, test_x_seg3, test_x_seg4,\n",
    "          test_x_seg5, test_x_seg6, test_x_seg7, test_x_seg8, test_x_seg9,\n",
    "         ]\n",
    "\n",
    "del test_x_seg0, test_x_seg1, test_x_seg2, test_x_seg3, test_x_seg4\n",
    "del test_x_seg5, test_x_seg6, test_x_seg7, test_x_seg8, test_x_seg9\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/uuser/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2560, 12)     444         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 2560, 12)     444         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 2560, 12)     444         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 2560, 12)     444         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 2560, 12)     444         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, 2560, 12)     444         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, 2560, 12)     444         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_106 (Conv1D)             (None, 2560, 12)     444         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, 2560, 12)     444         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_136 (Conv1D)             (None, 2560, 12)     444         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 2560, 12)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 2560, 12)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, 2560, 12)     0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, 2560, 12)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, 2560, 12)     0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)      (None, 2560, 12)     0           conv1d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_103 (LeakyReLU)     (None, 2560, 12)     0           conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_120 (LeakyReLU)     (None, 2560, 12)     0           conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_137 (LeakyReLU)     (None, 2560, 12)     0           conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_154 (LeakyReLU)     (None, 2560, 12)     0           conv1d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2560, 12)     444         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_107 (Conv1D)             (None, 2560, 12)     444         leaky_re_lu_120[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, 2560, 12)     444         leaky_re_lu_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_137 (Conv1D)             (None, 2560, 12)     444         leaky_re_lu_154[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 2560, 12)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 2560, 12)     0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, 2560, 12)     0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, 2560, 12)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, 2560, 12)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)      (None, 2560, 12)     0           conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_104 (LeakyReLU)     (None, 2560, 12)     0           conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_121 (LeakyReLU)     (None, 2560, 12)     0           conv1d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_138 (LeakyReLU)     (None, 2560, 12)     0           conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_155 (LeakyReLU)     (None, 2560, 12)     0           conv1d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1280, 12)     3468        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, 1280, 12)     3468        leaky_re_lu_121[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, 1280, 12)     3468        leaky_re_lu_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_138 (Conv1D)             (None, 1280, 12)     3468        leaky_re_lu_155[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 1280, 12)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 1280, 12)     0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, 1280, 12)     0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 1280, 12)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 1280, 12)     0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)      (None, 1280, 12)     0           conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_105 (LeakyReLU)     (None, 1280, 12)     0           conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_122 (LeakyReLU)     (None, 1280, 12)     0           conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_139 (LeakyReLU)     (None, 1280, 12)     0           conv1d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_156 (LeakyReLU)     (None, 1280, 12)     0           conv1d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1280, 12)     0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1280, 12)     0           leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_122[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_139[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_156[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1280, 12)     444         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1280, 12)     444         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 1280, 12)     444         dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 1280, 12)     444         dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 1280, 12)     444         dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, 1280, 12)     444         dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 1280, 12)     444         dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, 1280, 12)     444         dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, 1280, 12)     444         dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_139 (Conv1D)             (None, 1280, 12)     444         dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 1280, 12)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 1280, 12)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, 1280, 12)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 1280, 12)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, 1280, 12)     0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_89 (LeakyReLU)      (None, 1280, 12)     0           conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_106 (LeakyReLU)     (None, 1280, 12)     0           conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_123 (LeakyReLU)     (None, 1280, 12)     0           conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_140 (LeakyReLU)     (None, 1280, 12)     0           conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_157 (LeakyReLU)     (None, 1280, 12)     0           conv1d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1280, 12)     444         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, 1280, 12)     444         leaky_re_lu_123[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, 1280, 12)     444         leaky_re_lu_140[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_140 (Conv1D)             (None, 1280, 12)     444         leaky_re_lu_157[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1280, 12)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 1280, 12)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, 1280, 12)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 1280, 12)     0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)      (None, 1280, 12)     0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_90 (LeakyReLU)      (None, 1280, 12)     0           conv1d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_107 (LeakyReLU)     (None, 1280, 12)     0           conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_124 (LeakyReLU)     (None, 1280, 12)     0           conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_141 (LeakyReLU)     (None, 1280, 12)     0           conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_158 (LeakyReLU)     (None, 1280, 12)     0           conv1d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 640, 12)      3468        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, 640, 12)      3468        leaky_re_lu_124[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, 640, 12)      3468        leaky_re_lu_141[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_141 (Conv1D)             (None, 640, 12)      3468        leaky_re_lu_158[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 640, 12)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 640, 12)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 640, 12)      0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 640, 12)      0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)      (None, 640, 12)      0           conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_91 (LeakyReLU)      (None, 640, 12)      0           conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_108 (LeakyReLU)     (None, 640, 12)      0           conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_125 (LeakyReLU)     (None, 640, 12)      0           conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_142 (LeakyReLU)     (None, 640, 12)      0           conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_159 (LeakyReLU)     (None, 640, 12)      0           conv1d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 640, 12)      0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 640, 12)      0           leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 640, 12)      0           leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 640, 12)      0           leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 640, 12)      0           leaky_re_lu_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 640, 12)      0           leaky_re_lu_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 640, 12)      0           leaky_re_lu_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 640, 12)      0           leaky_re_lu_125[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 640, 12)      0           leaky_re_lu_142[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 640, 12)      0           leaky_re_lu_159[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 640, 12)      444         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 640, 12)      444         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 640, 12)      444         dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 640, 12)      444         dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 640, 12)      444         dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 640, 12)      444         dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 640, 12)      444         dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, 640, 12)      444         dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_127 (Conv1D)             (None, 640, 12)      444         dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_142 (Conv1D)             (None, 640, 12)      444         dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 640, 12)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 640, 12)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 640, 12)      0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, 640, 12)      0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)      (None, 640, 12)      0           conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_92 (LeakyReLU)      (None, 640, 12)      0           conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_109 (LeakyReLU)     (None, 640, 12)      0           conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_126 (LeakyReLU)     (None, 640, 12)      0           conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_143 (LeakyReLU)     (None, 640, 12)      0           conv1d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_160 (LeakyReLU)     (None, 640, 12)      0           conv1d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 640, 12)      444         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, 640, 12)      444         leaky_re_lu_126[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_128 (Conv1D)             (None, 640, 12)      444         leaky_re_lu_143[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_143 (Conv1D)             (None, 640, 12)      444         leaky_re_lu_160[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 640, 12)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 640, 12)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 640, 12)      0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, 640, 12)      0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)      (None, 640, 12)      0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_93 (LeakyReLU)      (None, 640, 12)      0           conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_110 (LeakyReLU)     (None, 640, 12)      0           conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_127 (LeakyReLU)     (None, 640, 12)      0           conv1d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_144 (LeakyReLU)     (None, 640, 12)      0           conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_161 (LeakyReLU)     (None, 640, 12)      0           conv1d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 320, 12)      3468        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, 320, 12)      3468        leaky_re_lu_127[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_129 (Conv1D)             (None, 320, 12)      3468        leaky_re_lu_144[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_144 (Conv1D)             (None, 320, 12)      3468        leaky_re_lu_161[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 320, 12)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 320, 12)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 320, 12)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, 320, 12)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)      (None, 320, 12)      0           conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_94 (LeakyReLU)      (None, 320, 12)      0           conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_111 (LeakyReLU)     (None, 320, 12)      0           conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_128 (LeakyReLU)     (None, 320, 12)      0           conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_145 (LeakyReLU)     (None, 320, 12)      0           conv1d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_162 (LeakyReLU)     (None, 320, 12)      0           conv1d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 320, 12)      0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 320, 12)      0           leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 320, 12)      0           leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 320, 12)      0           leaky_re_lu_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 320, 12)      0           leaky_re_lu_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 320, 12)      0           leaky_re_lu_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 320, 12)      0           leaky_re_lu_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 320, 12)      0           leaky_re_lu_128[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 320, 12)      0           leaky_re_lu_145[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 320, 12)      0           leaky_re_lu_162[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 320, 12)      444         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 320, 12)      444         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 320, 12)      444         dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 320, 12)      444         dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, 320, 12)      444         dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 320, 12)      444         dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 320, 12)      444         dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, 320, 12)      444         dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_130 (Conv1D)             (None, 320, 12)      444         dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, 320, 12)      444         dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 320, 12)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 320, 12)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 320, 12)      0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 320, 12)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)      (None, 320, 12)      0           conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_95 (LeakyReLU)      (None, 320, 12)      0           conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_112 (LeakyReLU)     (None, 320, 12)      0           conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_129 (LeakyReLU)     (None, 320, 12)      0           conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_146 (LeakyReLU)     (None, 320, 12)      0           conv1d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_163 (LeakyReLU)     (None, 320, 12)      0           conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, 320, 12)      444         leaky_re_lu_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, 320, 12)      444         leaky_re_lu_129[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_131 (Conv1D)             (None, 320, 12)      444         leaky_re_lu_146[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, 320, 12)      444         leaky_re_lu_163[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 320, 12)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 320, 12)      0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 320, 12)      0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 320, 12)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)      (None, 320, 12)      0           conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_96 (LeakyReLU)      (None, 320, 12)      0           conv1d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_113 (LeakyReLU)     (None, 320, 12)      0           conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_130 (LeakyReLU)     (None, 320, 12)      0           conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_147 (LeakyReLU)     (None, 320, 12)      0           conv1d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_164 (LeakyReLU)     (None, 320, 12)      0           conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, 160, 12)      3468        leaky_re_lu_113[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, 160, 12)      3468        leaky_re_lu_130[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_132 (Conv1D)             (None, 160, 12)      3468        leaky_re_lu_147[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_147 (Conv1D)             (None, 160, 12)      3468        leaky_re_lu_164[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 160, 12)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 160, 12)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 160, 12)      0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 160, 12)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)      (None, 160, 12)      0           conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_97 (LeakyReLU)      (None, 160, 12)      0           conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_114 (LeakyReLU)     (None, 160, 12)      0           conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_131 (LeakyReLU)     (None, 160, 12)      0           conv1d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_148 (LeakyReLU)     (None, 160, 12)      0           conv1d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_165 (LeakyReLU)     (None, 160, 12)      0           conv1d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 160, 12)      0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 160, 12)      0           leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 160, 12)      0           leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 160, 12)      0           leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 160, 12)      0           leaky_re_lu_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 160, 12)      0           leaky_re_lu_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 160, 12)      0           leaky_re_lu_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 160, 12)      0           leaky_re_lu_131[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 160, 12)      0           leaky_re_lu_148[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 160, 12)      0           leaky_re_lu_165[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 160, 12)      444         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 160, 12)      444         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 160, 12)      444         dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 160, 12)      444         dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_73 (Conv1D)              (None, 160, 12)      444         dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, 160, 12)      444         dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, 160, 12)      444         dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, 160, 12)      444         dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_133 (Conv1D)             (None, 160, 12)      444         dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_148 (Conv1D)             (None, 160, 12)      444         dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 160, 12)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 160, 12)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, 160, 12)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 160, 12)      0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_81 (LeakyReLU)      (None, 160, 12)      0           conv1d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_98 (LeakyReLU)      (None, 160, 12)      0           conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_115 (LeakyReLU)     (None, 160, 12)      0           conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_132 (LeakyReLU)     (None, 160, 12)      0           conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_149 (LeakyReLU)     (None, 160, 12)      0           conv1d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_166 (LeakyReLU)     (None, 160, 12)      0           conv1d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_74 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_104 (Conv1D)             (None, 160, 12)      444         leaky_re_lu_115[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, 160, 12)      444         leaky_re_lu_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_134 (Conv1D)             (None, 160, 12)      444         leaky_re_lu_149[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_149 (Conv1D)             (None, 160, 12)      444         leaky_re_lu_166[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 160, 12)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 160, 12)      0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, 160, 12)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 160, 12)      0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_82 (LeakyReLU)      (None, 160, 12)      0           conv1d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_99 (LeakyReLU)      (None, 160, 12)      0           conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_116 (LeakyReLU)     (None, 160, 12)      0           conv1d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_133 (LeakyReLU)     (None, 160, 12)      0           conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_150 (LeakyReLU)     (None, 160, 12)      0           conv1d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_167 (LeakyReLU)     (None, 160, 12)      0           conv1d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, 80, 12)       6924        leaky_re_lu_116[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, 80, 12)       6924        leaky_re_lu_133[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_135 (Conv1D)             (None, 80, 12)       6924        leaky_re_lu_150[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_150 (Conv1D)             (None, 80, 12)       6924        leaky_re_lu_167[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 80, 12)       0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 80, 12)       0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, 80, 12)       0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, 80, 12)       0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_83 (LeakyReLU)      (None, 80, 12)       0           conv1d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_100 (LeakyReLU)     (None, 80, 12)       0           conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_117 (LeakyReLU)     (None, 80, 12)       0           conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_134 (LeakyReLU)     (None, 80, 12)       0           conv1d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_151 (LeakyReLU)     (None, 80, 12)       0           conv1d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_168 (LeakyReLU)     (None, 80, 12)       0           conv1d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 80, 12)       0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 80, 12)       0           leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 80, 12)       0           leaky_re_lu_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 80, 12)       0           leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 80, 12)       0           leaky_re_lu_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 80, 12)       0           leaky_re_lu_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 80, 12)       0           leaky_re_lu_117[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 80, 12)       0           leaky_re_lu_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 80, 12)       0           leaky_re_lu_151[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 80, 12)       0           leaky_re_lu_168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 80, 24)       2496        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 80, 24)       2496        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 80, 24)       2496        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 80, 24)       2496        dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 80, 24)       2496        dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 80, 24)       2496        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 80, 24)       2496        dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 80, 24)       2496        dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 80, 24)       2496        dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 80, 24)       2496        dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 80, 24)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 80, 24)       0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, 80, 24)       0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, 80, 24)       0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_84 (LeakyReLU)      (None, 80, 24)       0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_101 (LeakyReLU)     (None, 80, 24)       0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_118 (LeakyReLU)     (None, 80, 24)       0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_135 (LeakyReLU)     (None, 80, 24)       0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_152 (LeakyReLU)     (None, 80, 24)       0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_169 (LeakyReLU)     (None, 80, 24)       0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 80, 24)       0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 80, 24)       0           leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 80, 24)       0           leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 80, 24)       0           leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 80, 24)       0           leaky_re_lu_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 80, 24)       0           leaky_re_lu_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 80, 24)       0           leaky_re_lu_118[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 80, 24)       0           leaky_re_lu_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 80, 24)       0           leaky_re_lu_152[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 80, 24)       0           leaky_re_lu_169[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_1 (Atten (None, 24)           624         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_2 (Atten (None, 24)           624         dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_3 (Atten (None, 24)           624         dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_4 (Atten (None, 24)           624         dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_5 (Atten (None, 24)           624         dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_6 (Atten (None, 24)           624         dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_7 (Atten (None, 24)           624         dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_8 (Atten (None, 24)           624         dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_9 (Atten (None, 24)           624         dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_10 (Atte (None, 24)           624         dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 24)        0           attention_with_context_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 24)        0           attention_with_context_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 24)        0           attention_with_context_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 24)        0           attention_with_context_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 24)        0           attention_with_context_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 24)        0           attention_with_context_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 24)        0           attention_with_context_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 24)        0           attention_with_context_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 24)        0           attention_with_context_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 24)        0           attention_with_context_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 24)        96          reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 24)        96          reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 24)        96          reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 24)        96          reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 24)        96          reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 24)        96          reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 1, 24)        96          reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1, 24)        96          reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 24)        96          reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1, 24)        96          reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_102 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_119 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_136 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_153 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_170 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1, 24)        0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1, 24)        0           leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 1, 24)        0           leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 1, 24)        0           leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 1, 24)        0           leaky_re_lu_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 1, 24)        0           leaky_re_lu_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 1, 24)        0           leaky_re_lu_119[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 1, 24)        0           leaky_re_lu_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 1, 24)        0           leaky_re_lu_153[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 1, 24)        0           leaky_re_lu_170[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 24)       0           dropout_7[0][0]                  \n",
      "                                                                 dropout_14[0][0]                 \n",
      "                                                                 dropout_21[0][0]                 \n",
      "                                                                 dropout_28[0][0]                 \n",
      "                                                                 dropout_35[0][0]                 \n",
      "                                                                 dropout_42[0][0]                 \n",
      "                                                                 dropout_49[0][0]                 \n",
      "                                                                 dropout_56[0][0]                 \n",
      "                                                                 dropout_63[0][0]                 \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 10, 24)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 240)          0           dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            241         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 284,761\n",
      "Trainable params: 284,281\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from resnet_ecg import attentionmodel_ctwo\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model,load_model\n",
    "\n",
    "'''   '''\n",
    "inputs0 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs1 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs2 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs3 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs4 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs5 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs6 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs7 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs8 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs9 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "\n",
    "inputs_list = [inputs0,inputs1,inputs2,inputs3,inputs4,inputs5,inputs6,inputs7,inputs8,inputs9]\n",
    "\n",
    "outputs = attentionmodel_ctwo.build_network(inputs_list,0.5,num_classes=1,block_size=4,relu=False)\n",
    "\n",
    "model = Model(inputs =inputs_list,outputs=outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = labels['label1'].values\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = np.bincount(list(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_for_0 = 1./counts[0]\n",
    "weight_for_1 = 1./counts[1]\n",
    "\n",
    "class_weight = {0:weight_for_0,1:weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[[1,2,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from resnet_ecg.utils import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.utils.to_categorical(train_label,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "fold:  1  training\n",
      "Learning rate:  0.1\n",
      "Train on 4333 samples, validate on 2167 samples\n",
      "Epoch 1/20\n",
      "4333/4333 [==============================] - 59s 14ms/step - loss: 0.3630 - acc: 0.8948 - fmeasure: 0.0052 - precision: 0.0185 - val_loss: 0.7502 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.89476, saving model to ./tst_model/tst_er_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 2/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3578 - acc: 0.8955 - fmeasure: 0.0033 - precision: 0.0074 - val_loss: 0.4657 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: acc improved from 0.89476 to 0.89545, saving model to ./tst_model/tst_er_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 3/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3634 - acc: 0.8957 - fmeasure: 0.0059 - precision: 0.0148 - val_loss: 0.9014 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00003: acc improved from 0.89545 to 0.89568, saving model to ./tst_model/tst_er_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 4/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3597 - acc: 0.8961 - fmeasure: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.3555 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00004: acc improved from 0.89568 to 0.89615, saving model to ./tst_model/tst_er_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 5/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3496 - acc: 0.8961 - fmeasure: 0.0062 - precision: 0.0197 - val_loss: 0.4107 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.89615\n",
      "Epoch 6/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3581 - acc: 0.8955 - fmeasure: 0.0107 - precision: 0.0222 - val_loss: 0.7996 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.89615\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/20\n",
      "4333/4333 [==============================] - 21s 5ms/step - loss: 0.3578 - acc: 0.8961 - fmeasure: 0.0030 - precision: 0.0104 - val_loss: 0.5719 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.89615\n",
      "Epoch 8/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3453 - acc: 0.8957 - fmeasure: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.3630 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.89615\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3425 - acc: 0.8964 - fmeasure: 0.0202 - precision: 0.0542 - val_loss: 0.3262 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00009: acc improved from 0.89615 to 0.89638, saving model to ./tst_model/tst_er_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 10/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3423 - acc: 0.8957 - fmeasure: 0.0037 - precision: 0.0148 - val_loss: 0.3896 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.89638\n",
      "Epoch 11/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3508 - acc: 0.8952 - fmeasure: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.4533 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00011: acc did not improve from 0.89638\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3402 - acc: 0.8968 - fmeasure: 0.0095 - precision: 0.0295 - val_loss: 0.3785 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00012: acc improved from 0.89638 to 0.89684, saving model to ./tst_model/tst_er_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 13/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3486 - acc: 0.8959 - fmeasure: 0.0134 - precision: 0.0443 - val_loss: 0.4069 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00013: acc did not improve from 0.89684\n",
      "Epoch 14/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3360 - acc: 0.8959 - fmeasure: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.3846 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00014: acc did not improve from 0.89684\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3359 - acc: 0.8959 - fmeasure: 0.0086 - precision: 0.0222 - val_loss: 0.3958 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00015: acc did not improve from 0.89684\n",
      "Epoch 16/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3365 - acc: 0.8971 - fmeasure: 0.0099 - precision: 0.0443 - val_loss: 0.3898 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00016: acc improved from 0.89684 to 0.89707, saving model to ./tst_model/tst_er_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 17/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3419 - acc: 0.8961 - fmeasure: 0.0247 - precision: 0.0687 - val_loss: 0.3798 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00017: acc did not improve from 0.89707\n",
      "Epoch 18/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3324 - acc: 0.8964 - fmeasure: 0.0114 - precision: 0.0399 - val_loss: 0.3417 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00018: acc did not improve from 0.89707\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 19/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3370 - acc: 0.8961 - fmeasure: 0.0037 - precision: 0.0148 - val_loss: 0.3406 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00019: acc did not improve from 0.89707\n",
      "Epoch 20/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3311 - acc: 0.8966 - fmeasure: 0.0033 - precision: 0.0148 - val_loss: 0.3423 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00020: acc did not improve from 0.89707\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "fold:  2  training\n",
      "Learning rate:  0.1\n",
      "Train on 4333 samples, validate on 2167 samples\n",
      "Epoch 1/20\n",
      "4333/4333 [==============================] - 64s 15ms/step - loss: 0.3489 - acc: 0.8955 - fmeasure: 0.0134 - precision: 0.0317 - val_loss: 0.3225 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.89545, saving model to ./tst_model/tst_er_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 2/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3411 - acc: 0.8961 - fmeasure: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.3339 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: acc improved from 0.89545 to 0.89615, saving model to ./tst_model/tst_er_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 3/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3509 - acc: 0.8948 - fmeasure: 0.0132 - precision: 0.0448 - val_loss: 0.6062 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00003: acc did not improve from 0.89615\n",
      "Epoch 4/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3366 - acc: 0.8955 - fmeasure: 0.0195 - precision: 0.0628 - val_loss: 1.3505 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.89615\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3454 - acc: 0.8945 - fmeasure: 0.0054 - precision: 0.0295 - val_loss: 0.3814 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.89615\n",
      "Epoch 6/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3361 - acc: 0.8957 - fmeasure: 0.0180 - precision: 0.0480 - val_loss: 0.3833 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.89615\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3280 - acc: 0.8961 - fmeasure: 0.0145 - precision: 0.0591 - val_loss: 0.3296 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.89615\n",
      "Epoch 8/20\n",
      "4333/4333 [==============================] - 21s 5ms/step - loss: 0.3370 - acc: 0.8968 - fmeasure: 0.0207 - precision: 0.0812 - val_loss: 0.3376 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00008: acc improved from 0.89615 to 0.89684, saving model to ./tst_model/tst_er_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 9/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3269 - acc: 0.8957 - fmeasure: 0.0261 - precision: 0.0812 - val_loss: 0.3269 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.89684\n",
      "Epoch 10/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3227 - acc: 0.8968 - fmeasure: 0.0348 - precision: 0.1108 - val_loss: 0.4035 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.89684\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3275 - acc: 0.8936 - fmeasure: 0.0278 - precision: 0.0714 - val_loss: 0.4252 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00011: acc did not improve from 0.89684\n",
      "Epoch 12/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3215 - acc: 0.8968 - fmeasure: 0.0153 - precision: 0.0517 - val_loss: 0.3785 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00012: acc did not improve from 0.89684\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 13/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3103 - acc: 0.8961 - fmeasure: 0.0155 - precision: 0.0566 - val_loss: 0.3527 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00013: acc did not improve from 0.89684\n",
      "Epoch 14/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3242 - acc: 0.8941 - fmeasure: 0.0264 - precision: 0.0990 - val_loss: 0.3318 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00014: acc did not improve from 0.89684\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3146 - acc: 0.8952 - fmeasure: 0.0143 - precision: 0.0298 - val_loss: 0.3356 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00015: acc did not improve from 0.89684\n",
      "Epoch 16/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3170 - acc: 0.8964 - fmeasure: 0.0221 - precision: 0.0763 - val_loss: 0.3258 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00016: acc did not improve from 0.89684\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 17/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3121 - acc: 0.8957 - fmeasure: 0.0186 - precision: 0.0517 - val_loss: 0.3275 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00017: acc did not improve from 0.89684\n",
      "Epoch 18/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3154 - acc: 0.8945 - fmeasure: 0.0215 - precision: 0.0812 - val_loss: 0.3376 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00018: acc did not improve from 0.89684\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 19/20\n",
      "4333/4333 [==============================] - 23s 5ms/step - loss: 0.3102 - acc: 0.8975 - fmeasure: 0.0557 - precision: 0.1600 - val_loss: 0.3431 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00019: acc improved from 0.89684 to 0.89753, saving model to ./tst_model/tst_er_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 20/20\n",
      "4333/4333 [==============================] - 22s 5ms/step - loss: 0.3146 - acc: 0.8961 - fmeasure: 0.0390 - precision: 0.1403 - val_loss: 0.3426 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00020: acc did not improve from 0.89753\n",
      "fold:  3  training\n",
      "Learning rate:  0.1\n",
      "Train on 4334 samples, validate on 2166 samples\n",
      "Epoch 1/20\n",
      "4334/4334 [==============================] - 76s 17ms/step - loss: 0.3401 - acc: 0.8957 - fmeasure: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.3170 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.89571, saving model to ./tst_model/tst_er_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 2/20\n",
      "4334/4334 [==============================] - 23s 5ms/step - loss: 0.3369 - acc: 0.8957 - fmeasure: 0.0101 - precision: 0.0332 - val_loss: 0.3556 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.89571\n",
      "Epoch 3/20\n",
      "4334/4334 [==============================] - 23s 5ms/step - loss: 0.3424 - acc: 0.8948 - fmeasure: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.3132 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00003: acc did not improve from 0.89571\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/20\n",
      "4334/4334 [==============================] - 23s 5ms/step - loss: 0.3299 - acc: 0.8948 - fmeasure: 0.0234 - precision: 0.0492 - val_loss: 0.3448 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.89571\n",
      "Epoch 5/20\n",
      "4334/4334 [==============================] - 23s 5ms/step - loss: 0.3329 - acc: 0.8962 - fmeasure: 0.0049 - precision: 0.0148 - val_loss: 0.3212 - val_acc: 0.8961 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00005: acc improved from 0.89571 to 0.89617, saving model to ./tst_model/tst_er_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 6/20\n",
      "4334/4334 [==============================] - 23s 5ms/step - loss: 0.3276 - acc: 0.8939 - fmeasure: 0.0076 - precision: 0.0222 - val_loss: 0.3103 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.89617\n",
      "Epoch 7/20\n",
      "4334/4334 [==============================] - 24s 6ms/step - loss: 0.3351 - acc: 0.8957 - fmeasure: 0.0285 - precision: 0.0788 - val_loss: 0.3364 - val_acc: 0.8961 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.89617\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/20\n",
      "4334/4334 [==============================] - 23s 5ms/step - loss: 0.3299 - acc: 0.8964 - fmeasure: 0.0232 - precision: 0.0591 - val_loss: 0.3251 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00008: acc improved from 0.89617 to 0.89640, saving model to ./tst_model/tst_er_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 9/20\n",
      "4334/4334 [==============================] - 22s 5ms/step - loss: 0.3185 - acc: 0.8978 - fmeasure: 0.0538 - precision: 0.1464 - val_loss: 0.2991 - val_acc: 0.8966 - val_fmeasure: 0.0052 - val_precision: 0.0591\n",
      "\n",
      "Epoch 00009: acc improved from 0.89640 to 0.89778, saving model to ./tst_model/tst_er_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 10/20\n",
      "4334/4334 [==============================] - 21s 5ms/step - loss: 0.3242 - acc: 0.8962 - fmeasure: 0.0359 - precision: 0.1255 - val_loss: 0.3050 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.89778\n",
      "Epoch 11/20\n",
      "4334/4334 [==============================] - 21s 5ms/step - loss: 0.3281 - acc: 0.8941 - fmeasure: 0.0201 - precision: 0.0541 - val_loss: 0.2980 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00011: acc did not improve from 0.89778\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/20\n",
      "4334/4334 [==============================] - 22s 5ms/step - loss: 0.3131 - acc: 0.8962 - fmeasure: 0.0303 - precision: 0.1108 - val_loss: 0.2963 - val_acc: 0.8966 - val_fmeasure: 0.0060 - val_precision: 0.0591\n",
      "\n",
      "Epoch 00012: acc did not improve from 0.89778\n",
      "Epoch 13/20\n",
      "4334/4334 [==============================] - 22s 5ms/step - loss: 0.3116 - acc: 0.8985 - fmeasure: 0.0332 - precision: 0.1034 - val_loss: 0.3107 - val_acc: 0.8966 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00013: acc improved from 0.89778 to 0.89848, saving model to ./tst_model/tst_er_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 14/20\n",
      "4334/4334 [==============================] - 22s 5ms/step - loss: 0.3159 - acc: 0.8959 - fmeasure: 0.0267 - precision: 0.0960 - val_loss: 0.3015 - val_acc: 0.8966 - val_fmeasure: 0.0015 - val_precision: 0.0295\n",
      "\n",
      "Epoch 00014: acc did not improve from 0.89848\n",
      "Epoch 15/20\n",
      "4334/4334 [==============================] - 21s 5ms/step - loss: 0.3106 - acc: 0.8976 - fmeasure: 0.0798 - precision: 0.2075 - val_loss: 0.3117 - val_acc: 0.8966 - val_fmeasure: 0.0015 - val_precision: 0.0295\n",
      "\n",
      "Epoch 00015: acc did not improve from 0.89848\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 16/20\n",
      "4334/4334 [==============================] - 22s 5ms/step - loss: 0.3164 - acc: 0.8964 - fmeasure: 0.0547 - precision: 0.1883 - val_loss: 0.2972 - val_acc: 0.8961 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00016: acc did not improve from 0.89848\n",
      "Epoch 17/20\n",
      "4334/4334 [==============================] - 23s 5ms/step - loss: 0.3013 - acc: 0.8976 - fmeasure: 0.0793 - precision: 0.1718 - val_loss: 0.3068 - val_acc: 0.8961 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00017: acc did not improve from 0.89848\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/20\n",
      "4334/4334 [==============================] - 22s 5ms/step - loss: 0.3007 - acc: 0.8969 - fmeasure: 0.0563 - precision: 0.1760 - val_loss: 0.2981 - val_acc: 0.8961 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00018: acc did not improve from 0.89848\n",
      "Epoch 19/20\n",
      "4334/4334 [==============================] - 22s 5ms/step - loss: 0.3057 - acc: 0.8971 - fmeasure: 0.0587 - precision: 0.1747 - val_loss: 0.2974 - val_acc: 0.8961 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00019: acc did not improve from 0.89848\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 20/20\n",
      "4334/4334 [==============================] - 22s 5ms/step - loss: 0.3005 - acc: 0.8973 - fmeasure: 0.0367 - precision: 0.1108 - val_loss: 0.2975 - val_acc: 0.8961 - val_fmeasure: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00020: acc did not improve from 0.89848\n"
     ]
    }
   ],
   "source": [
    "#print(\"train_x shape :\", train_x.shape)\n",
    "       \n",
    "model_path = './tst_model/'\n",
    "\n",
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 3\n",
    "    n_classes = 1#2\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "    kf = kfold.split(train_index, train_label)\n",
    "\n",
    "    blend_train = np.zeros((6500, n_classes)).astype('float32') #len(train_x)\n",
    "    blend_test = np.zeros((500, n_fold, n_classes)).astype('float32') #len(test_x)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i+1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        index_tr = train_index[index_train]\n",
    "        index_vld = train_index[index_valid]\n",
    "\n",
    "        '''  '''\n",
    "        \n",
    "        X_tr = [ecg12_seg0[index_tr], ecg12_seg1[index_tr], ecg12_seg2[index_tr], ecg12_seg3[index_tr],\n",
    "                ecg12_seg4[index_tr], ecg12_seg5[index_tr], ecg12_seg6[index_tr], ecg12_seg7[index_tr],\n",
    "                ecg12_seg8[index_tr], ecg12_seg9[index_tr],\n",
    "               ]\n",
    "\n",
    "        X_vld = [ecg12_seg0[index_vld], ecg12_seg1[index_vld], ecg12_seg2[index_vld], ecg12_seg3[index_vld],\n",
    "                 ecg12_seg4[index_vld], ecg12_seg5[index_vld], ecg12_seg6[index_vld], ecg12_seg7[index_vld],\n",
    "                 ecg12_seg8[index_vld], ecg12_seg9[index_vld],\n",
    "               ]\n",
    "        \n",
    "        #X_tr = train_x[index_tr]\n",
    "        #X_vld = train_x[index_vld]\n",
    "        #print(index_tr)\n",
    "\n",
    "        y_tr = train_label[index_tr]#keras.utils.to_categorical(train_label[index_tr] ,num_classes=2) #preprocess_y(labels,index_tr)\n",
    "        y_vld = train_label[index_vld]#keras.utils.to_categorical(train_label[index_vld] ,num_classes=2)   #preprocess_y(labels,index_vld)\n",
    "\n",
    "        #print(y_tr.shape)\n",
    "        #print(y_vld.shape)\n",
    "        #print(y_tr[:10])\n",
    "        #print(y_vld[:10])\n",
    "\n",
    "        checkpointer = ModelCheckpoint(filepath=model_path+'tst_er_attention_weights-best_k{}_r{}.hdf5'.format(seed,i),\n",
    "                                       monitor='acc', verbose=1, save_best_only=True,\n",
    "                                       save_weights_only=True,\n",
    "                                       mode='max')  # val_fmeasure  val_loss\n",
    "        reduce = ReduceLROnPlateau(monitor='acc', factor=0.5, patience=2, verbose=1, min_delta=1e-4,\n",
    "                                   mode='max')\n",
    "\n",
    "        config = Config()\n",
    "        add_compile(model, config)\n",
    "        ''' \n",
    "        model_name = 'resnet21.h5'\n",
    "        earlystop = EarlyStopping(\n",
    "            monitor='val_fmeasure',  # 'val_categorical_accuracy',\n",
    "            patience=10,\n",
    "        )\n",
    "        checkpoint = ModelCheckpoint(filepath=model_name,\n",
    "                                     monitor='val_categorical_accuracy', mode='max',\n",
    "                                     save_best_only='True')\n",
    "\n",
    "        lr_scheduler = LearningRateScheduler(config.lr_schedule)\n",
    "        '''\n",
    "        callback_lists = [checkpointer, reduce]  # [checkpointer,lr_scheduler]#\n",
    "        # [checkpointer,earlystop,lr_scheduler]\n",
    "        # [checkpoint, earlystop,lr_scheduler]\n",
    "\n",
    "        history = model.fit(x=X_tr, y=y_tr, batch_size=64, epochs=20,  #class_weight=class_weight,#'auto',\n",
    "                            verbose=1, validation_data=(X_vld, y_vld), callbacks=callback_lists)\n",
    "\n",
    "        # Evaluate best trained model\n",
    "        #model.load_weights(model_path+'densenet_weights-best_k{}_r{}.hdf5'.format(seed, i))\n",
    "\n",
    "        test_y = model.predict(test_x)\n",
    "        val_y = model.predict(X_vld)\n",
    "        \n",
    "        del X_tr\n",
    "        del X_vld\n",
    "        \n",
    "        #K.clear_session()\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        #config = tf.ConfigProto()\n",
    "        #config.gpu_options.allow_growth=True\n",
    "        #sess = tf.Session(config=config)\n",
    "        #K.set_session(sess)\n",
    "\n",
    "        blend_train[index_vld, :] = val_y\n",
    "        blend_test[:, i, :] = test_y\n",
    "\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "fold:  1  training\n",
      "fold:  2  training\n"
     ]
    }
   ],
   "source": [
    "#print(\"train_x shape :\", train_x.shape)\n",
    "       \n",
    "model_path = './tst_model/'#'./model/'\n",
    "\n",
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 2#\n",
    "    n_classes = 1#2\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "    kf = kfold.split(train_index, train_label)\n",
    "\n",
    "    blend_train = np.zeros((6500, n_classes)).astype('float32') #len(train_x)\n",
    "    blend_test = np.zeros((500, n_fold, n_classes)).astype('float32') #len(test_x)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i+1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        index_tr = train_index[index_train]\n",
    "        index_vld = train_index[index_valid]\n",
    "\n",
    "        X_vld = [ecg12_seg0[index_vld], ecg12_seg1[index_vld], ecg12_seg2[index_vld], ecg12_seg3[index_vld],\n",
    "                 ecg12_seg4[index_vld], ecg12_seg5[index_vld], ecg12_seg6[index_vld], ecg12_seg7[index_vld],\n",
    "                 ecg12_seg8[index_vld], ecg12_seg9[index_vld],\n",
    "               ]\n",
    "\n",
    "        #y_vld = keras.utils.to_categorical(train_label[index_vld] ,num_classes=2)#preprocess_y(labels,index_vld)\n",
    "\n",
    "        # Evaluate best trained model\n",
    "        model.load_weights(model_path+'tst_er_attention_weights-best_k{}_r{}.hdf5'.format(seed, i+1))\n",
    "\n",
    "        test_y = model.predict(test_x)\n",
    "        val_y = model.predict(X_vld)\n",
    "\n",
    "        del X_vld\n",
    "\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "\n",
    "        blend_train[index_vld, :] = val_y\n",
    "        blend_test[:, i, :] = test_y\n",
    "\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold:  [0.1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_y = blend_train\n",
    "\n",
    "threshold = np.arange(0.1,0.9,0.1)\n",
    "\n",
    "out = x_tr_y\n",
    "#y_test = keras.utils.to_categorical(train_label,num_classes=2)#y_tr\n",
    "y_test = train_label\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(out.shape[1])\n",
    "for i in range(out.shape[1]):\n",
    "    y_prob = np.array(out[:,i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
    "        #acc.append( matthews_corrcoef(y_test[:,i],y_pred))\n",
    "        acc.append(f1_score(y_test[:],y_pred,average='macro'))\n",
    "    acc   = np.array(acc)\n",
    "    index = np.where(acc==acc.max()) \n",
    "    accuracies.append(acc.max()) \n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "    \n",
    "print(\"best_threshold: \",best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if out[i]>=best_threshold[j] else 0 for j in range(1) for i in range(len(y_test))]])\n",
    "\n",
    "y_pred \n",
    "\n",
    "y_test\n",
    "\n",
    "#best_threshold:  [0.7 0.4 0.5 0.4 0.3 0.2 0.3 0.4 0.4]\n",
    "#0.022393162393162393\n",
    "\n",
    "#best_threshold:  [0.7 0.4 0.5 0.4 0.4 0.2 0.4 0.4 0.5]\n",
    "#0.022615384615384617\n",
    "\n",
    "#hamming_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5390, 1110])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5994092386794592"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred[0],average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-062c63e6d1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1 score of ab {} is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_test[:,i],y_pred[:,i],average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_yy = 0.2*blend_test[:,0,:]+0.8*blend_test[:,1,:]#blend_test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "classes = [0,1]#,2,3,4,5,6,7,8]\n",
    "\n",
    "test_y = test_yy\n",
    "\n",
    "y_pred = [[1 if test_y[i,j]>=best_threshold[j] else 0 for j in range(test_y.shape[1])] \n",
    "          for i in range(len(test_y))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "classes = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "test_y = test_yy\n",
    "\n",
    "y_pred = [[1 if test_y[i]>=0.1 else 0 for j in range(1)] \n",
    "          for i in range(len(test_y))]\n",
    "\n",
    "\n",
    "y_pred = [[0,0,0,0,0,0]+i+[0,0] for i in y_pred]\n",
    "\n",
    "\n",
    "pred=[]\n",
    "for j in range(len(y_pred)):\n",
    "    pred.append([classes[i] for i in range(9) if y_pred[j][i] == 1])\n",
    "\n",
    "with open('answers_attention_no_er.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name) \n",
    "            \n",
    "            result = pred[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predd = [[i] for i in np.argmax(test_yy,axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('answers_attention_no.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name) \n",
    "            \n",
    "            result = predd[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
