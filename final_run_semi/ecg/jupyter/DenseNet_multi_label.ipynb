{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from resnet_ecg.utils import one_hot,get_batches\n",
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "from resnet_ecg.densenet import DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import extract_basic_features\n",
    "\n",
    "import wfdb\n",
    "import os\n",
    "import wfdb.processing as wp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from utils import find_noise_features, extract_basic_features\n",
    "import shutil\n",
    "\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy.io as sio\n",
    "train_dataset_path = os.getcwd()+\"/Train/\"\n",
    "val_dataset_path = os.getcwd()+\"/Val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files = os.listdir(train_dataset_path)\n",
    "train_files.sort()\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "      <th>label6</th>\n",
       "      <th>label7</th>\n",
       "      <th>label8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN0001</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN0002</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN0003</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN0004</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN0005</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File_name  label1  label2  label3  label4  label5  label6  label7  label8\n",
       "0  TRAIN0001       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "1  TRAIN0002       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "2  TRAIN0003       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "3  TRAIN0004       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "4  TRAIN0005       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"reference.csv\")\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data_seg(data_path, split = \"Train\",preprocess=False):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 2560\n",
    "\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    #label_path = os.path.join(data_path, \"reference.txt\")\n",
    "    #labels = pd.read_csv(label_path, sep='\\t',header = None)\n",
    "    #labels = pd.read_csv(\"reference.csv\")\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    #print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12#len(channel_files)\n",
    "    #posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "    X = np.zeros((len(channel_files), n_steps, n_channels))\n",
    "    i_ch = 0\n",
    "    \n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1','II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II','aVR','V2','V5']\n",
    "    channel_post_name = ['III','aVF','V3','V6']\n",
    "    \n",
    "    for i_ch,fil_ch in tqdm(enumerate(channel_files[:])):\n",
    "        #channel_name = fil_ch[:-posix]\n",
    "        #dat_ = pd.read_csv(os.path.join(path_signals,fil_ch), delim_whitespace = True, header = None)\n",
    "        #print(fil_ch)\n",
    "        \n",
    "        ecg = sio.loadmat(os.path.join(path_signals,fil_ch))\n",
    "        \n",
    "        ecg_channels = np.zeros((n_steps, n_channels))\n",
    "          \n",
    "        for i_n,ch_name in enumerate(channel_name):\n",
    "            \n",
    "            if ch_name in channel_mid_name:\n",
    "                mid_ind = int(ecg[ch_name].T.shape[0]/2)\n",
    "                ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[mid_ind-2500:mid_ind+2500],2560).T \n",
    "            elif ch_name in channel_post_name:\n",
    "                ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[-5000:],2560).T\n",
    "            else:\n",
    "                ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[:5000],2560).T\n",
    "                \n",
    "            #ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T,2560).T\n",
    "\n",
    "            if preprocess:\n",
    "                data = ecg_preprocessing(ecg_channels[:,i_n].reshape(1,2560), 'sym8', 8, 3, 256)\n",
    "\n",
    "                ecg_channels[:,i_n] = data[0]#ecg['data']\n",
    "            else:\n",
    "                pass\n",
    "                #ecg_channels[:,i_n] = ecg_channels[:,i_n]\n",
    "                \n",
    "        X[i_ch,:,:] = ecg_channels\n",
    "\n",
    "    # Return \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data_sample(data_path, split = \"Train\",preprocess=False):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 2560\n",
    "\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    #label_path = os.path.join(data_path, \"reference.txt\")\n",
    "    #labels = pd.read_csv(label_path, sep='\\t',header = None)\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    #print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12#len(channel_files)\n",
    "    #posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "    X = np.zeros((len(channel_files), n_steps, n_channels))\n",
    "    i_ch = 0\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1',  'II', 'aVR', 'V3', 'III', 'V5']\n",
    "    \n",
    "    #shape_ecg = []\n",
    "    \n",
    "    for i_ch,fil_ch in tqdm(enumerate(channel_files[:])):\n",
    "        #channel_name = fil_ch[:-posix]\n",
    "        #dat_ = pd.read_csv(os.path.join(path_signals,fil_ch), delim_whitespace = True, header = None)\n",
    "        #print(fil_ch)\n",
    "        \n",
    "        ecg = sio.loadmat(os.path.join(path_signals,fil_ch))\n",
    "        \n",
    "        \n",
    "        \n",
    "        ecg_channels = np.zeros((n_steps, n_channels))\n",
    "        \n",
    "        for i_n,ch_name in enumerate(channel_name[:]):\n",
    "            \n",
    "            ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T,2560).T\n",
    "            \n",
    "            #shape_ecg.append(ecg[ch_name].shape[1])\n",
    "            \n",
    "            if preprocess:\n",
    "                data = ecg_preprocessing(ecg_channels[:,i_n].reshape(1,2560), 'sym8', 8, 3, 256)\n",
    "\n",
    "                ecg_channels[:,i_n] = data[0]#ecg['data']\n",
    "            else:\n",
    "                pass\n",
    "                #ecg_channels[:,i_n] = ecg_channels[:,i_n]\n",
    "                \n",
    "        X[i_ch,:,:] = ecg_channels\n",
    "\n",
    "    # Return \n",
    "    return X#shape_ecg#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6500it [01:45, 61.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x = read_data_sample(os.getcwd(),preprocess=True)#read_data_sample(os.getcwd(),preprocess=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum1_label=0\n",
    "sum2_label=0\n",
    "sum3_label=0\n",
    "sum4_label=0\n",
    "sum5_label=0\n",
    "sum6_label=0\n",
    "sum7_label=0\n",
    "bin_label = np.zeros((6500,9))\n",
    "for i in range(labels.shape[0]):\n",
    "    label_nona = labels.loc[i].dropna()\n",
    "    if label_nona.shape[0] == 2:\n",
    "        sum1_label+=1\n",
    "    if label_nona.shape[0] == 3:\n",
    "        sum2_label+=1\n",
    "    if label_nona.shape[0] == 4:\n",
    "        sum3_label+=1\n",
    "    if label_nona.shape[0] == 5:\n",
    "        sum4_label+=1\n",
    "    if label_nona.shape[0] == 6:\n",
    "        sum5_label+=1\n",
    "    if label_nona.shape[0] == 7:\n",
    "        sum6_label+=1\n",
    "    if label_nona.shape[0] == 8:\n",
    "        sum7_label+=1\n",
    "    for j in range(1,label_nona.shape[0]):\n",
    "        bin_label[i,int(label_nona[j])]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 2560, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler,EarlyStopping,ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.conv_subsample_lengths = [1, 2, 1, 2, 1, 2, 1, 2]\n",
    "        self.conv_filter_length = 32\n",
    "        self.conv_num_filters_start = 12\n",
    "        self.conv_init = \"he_normal\"\n",
    "        self.conv_activation = \"relu\"\n",
    "        self.conv_dropout = 0.5\n",
    "        self.conv_num_skip = 2\n",
    "        self.conv_increase_channels_at = 2\n",
    "        self.batch_size = 32#128\n",
    "        self.input_shape = [2560, 12]#[1280, 1]\n",
    "        self.num_categories = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 0.1\n",
    "        if epoch >= 20 and epoch < 40:\n",
    "            lr = 0.01\n",
    "        if epoch >= 40:\n",
    "            lr = 0.001\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DenseNet 0.0.1\n",
      "#############################################\n",
      "Dense blocks: 3\n",
      "Layers per dense block: [2, 2, 2]\n",
      "#############################################\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "block::: 0\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/uuser/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "block::: 1\n",
      "nb_channels:: Tensor(\"concatenate_6/concat:0\", shape=(?, 160, 60), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "KTF.set_session(session )\n",
    "\n",
    "model = DenseNet(input_shape=(2560,12), nb_classes=9, depth=10, growth_rate=16,\n",
    "                          dropout_rate=0.1, bottleneck=False, compression=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2560, 16)     6144        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2560, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 2560, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2560, 16)     8192        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2560, 16)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2560, 32)     0           conv1d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 2560, 32)     128         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2560, 32)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2560, 16)     16384       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2560, 16)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2560, 48)     0           conv1d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2560, 48)     192         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 2560, 48)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1280, 24)     36864       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1280, 24)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePoo (None, 640, 24)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 640, 24)      96          average_pooling1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 640, 24)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 640, 16)      12288       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 640, 16)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 640, 40)      0           average_pooling1d_1[0][0]        \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 640, 40)      160         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 640, 40)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 640, 16)      20480       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 640, 16)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 640, 56)      0           average_pooling1d_1[0][0]        \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 640, 56)      224         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 640, 56)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 320, 28)      50176       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 320, 28)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePoo (None, 160, 28)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 160, 28)      112         average_pooling1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 160, 28)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 160, 64)      57344       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 160, 64)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 160, 64)      256         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 160, 64)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 160, 16)      32768       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 160, 16)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 160, 44)      0           average_pooling1d_2[0][0]        \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 160, 44)      176         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 160, 44)      0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 160, 64)      90112       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 160, 64)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 160, 64)      256         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 160, 64)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 160, 16)      32768       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 160, 16)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 160, 60)      0           average_pooling1d_2[0][0]        \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 160, 60)      240         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 160, 60)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 60)           0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 9)            549         global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 365,973\n",
      "Trainable params: 365,021\n",
      "Non-trainable params: 952\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "    \n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(train_x, labels['label1'], \n",
    "                                        stratify = labels['label1'], #test_size=0.2,\n",
    "                                        shuffle=True, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4788, 208]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_tr.index.tolist()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_y(y,num_class=9):\n",
    "    bin_label = np.zeros((y.shape[0],num_class))\n",
    "    for i in range(y.shape[0]):\n",
    "        label_nona = labels.loc[lab_tr.index.tolist()[i]].dropna()\n",
    "        for j in range(1,label_nona.shape[0]):\n",
    "            bin_label[i,int(label_nona[j])]=1\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_tr = preprocess_y(lab_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_vld = preprocess_y(lab_vld) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights_best_simple_model_1.hdf5', \n",
    "                            monitor='val_fmeasure',verbose=1, save_best_only=True, mode='max')\n",
    "reduce = ReduceLROnPlateau(monitor='val_fmeasure',factor=0.5,patience=2,verbose=1,min_delta=1e-4,mode='max')\n",
    "''' \n",
    "model.compile(optimizer = 'adam',\n",
    "           loss='binary_crossentropy',\n",
    "           metrics=['accuracy',fmeasure,recall,precision])\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "       validation_data = val_generator,\n",
    "       epochs=epochs,\n",
    "       callbacks=[checkpointer,reduce],\n",
    "       verbose=1)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "#2156+224+672+654+180+826+534+504+1953\n",
    "\n",
    "def weighted_loss(labels,logits):\n",
    "    logits_log = (-1) * K.log(logits)\n",
    "    num_class = logits.shape[-1]\n",
    "    \n",
    "    y_0 = 0.8 * labels[:,0:1]\n",
    "    y_1 = 1.2 * labels[:,1:2]\n",
    "    y_2 = 1.0 * labels[:,2:3]\n",
    "    y_3 = 1.0 * labels[:,3:4]\n",
    "    y_4 = 1.2 * labels[:,4:5]\n",
    "    y_5 = 1.0 * labels[:,5:6]\n",
    "    y_6 = 1.0 * labels[:,6:7]\n",
    "    y_7 = 1.0 * labels[:,7:8]\n",
    "    y_8 = 0.8 * labels[:,8:]\n",
    "    \n",
    "    weight_labels = K.concatenate([y_0,y_1,y_2,y_3,y_4,y_5,y_6,y_7,y_8],axis=1)\n",
    "    loss = K.sum(logits_log * weight_labels,axis=1)\n",
    "    \n",
    "    loss = K.mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_compile(model, config):\n",
    "    \n",
    "    optimizer = SGD(lr=config.lr_schedule(0), momentum=0.9)#Adam()#\n",
    "    model.compile(loss='binary_crossentropy',#weighted_loss,#'binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy',fmeasure,recall,precision])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cw = {0:1,1:3,2:3,3:2,4:9,5:3,6:3,7:9,8:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.1\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4875 samples, validate on 1625 samples\n",
      "Epoch 1/50\n",
      "4875/4875 [==============================] - 15s 3ms/step - loss: 0.3456 - acc: 0.8913 - fmeasure: 0.5996 - recall: 0.5438 - precision: 0.7120 - val_loss: 0.5880 - val_acc: 0.8225 - val_fmeasure: 0.2159 - val_recall: 0.1858 - val_precision: 0.2593\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.21587, saving model to weights_best_simple_model_1.hdf5\n",
      "Epoch 2/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.2303 - acc: 0.9252 - fmeasure: 0.6557 - recall: 0.5444 - precision: 0.8292 - val_loss: 0.7660 - val_acc: 0.8213 - val_fmeasure: 0.2306 - val_recall: 0.2033 - val_precision: 0.2676\n",
      "\n",
      "Epoch 00002: val_fmeasure improved from 0.21587 to 0.23059, saving model to weights_best_simple_model_1.hdf5\n",
      "Epoch 3/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.2024 - acc: 0.9307 - fmeasure: 0.6873 - recall: 0.5811 - precision: 0.8464 - val_loss: 0.7402 - val_acc: 0.8194 - val_fmeasure: 0.2384 - val_recall: 0.2144 - val_precision: 0.2697\n",
      "\n",
      "Epoch 00003: val_fmeasure improved from 0.23059 to 0.23839, saving model to weights_best_simple_model_1.hdf5\n",
      "Epoch 4/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1790 - acc: 0.9378 - fmeasure: 0.7295 - recall: 0.6400 - precision: 0.8525 - val_loss: 0.6761 - val_acc: 0.8203 - val_fmeasure: 0.2127 - val_recall: 0.1838 - val_precision: 0.2546\n",
      "\n",
      "Epoch 00004: val_fmeasure did not improve from 0.23839\n",
      "Epoch 5/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1688 - acc: 0.9404 - fmeasure: 0.7444 - recall: 0.6617 - precision: 0.8541 - val_loss: 0.6820 - val_acc: 0.8137 - val_fmeasure: 0.1833 - val_recall: 0.1583 - val_precision: 0.2203\n",
      "\n",
      "Epoch 00005: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1477 - acc: 0.9486 - fmeasure: 0.7839 - recall: 0.7115 - precision: 0.8767 - val_loss: 0.7226 - val_acc: 0.8115 - val_fmeasure: 0.2267 - val_recall: 0.2093 - val_precision: 0.2484\n",
      "\n",
      "Epoch 00006: val_fmeasure did not improve from 0.23839\n",
      "Epoch 7/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1375 - acc: 0.9527 - fmeasure: 0.8049 - recall: 0.7425 - precision: 0.8822 - val_loss: 0.7769 - val_acc: 0.8104 - val_fmeasure: 0.2159 - val_recall: 0.1982 - val_precision: 0.2388\n",
      "\n",
      "Epoch 00007: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1266 - acc: 0.9569 - fmeasure: 0.8238 - recall: 0.7677 - precision: 0.8923 - val_loss: 0.7855 - val_acc: 0.8077 - val_fmeasure: 0.2300 - val_recall: 0.2178 - val_precision: 0.2448\n",
      "\n",
      "Epoch 00008: val_fmeasure did not improve from 0.23839\n",
      "Epoch 9/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1210 - acc: 0.9585 - fmeasure: 0.8313 - recall: 0.7811 - precision: 0.8916 - val_loss: 0.8110 - val_acc: 0.8046 - val_fmeasure: 0.2119 - val_recall: 0.1988 - val_precision: 0.2281\n",
      "\n",
      "Epoch 00009: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1120 - acc: 0.9621 - fmeasure: 0.8465 - recall: 0.7982 - precision: 0.9039 - val_loss: 0.8122 - val_acc: 0.8062 - val_fmeasure: 0.2195 - val_recall: 0.2061 - val_precision: 0.2360\n",
      "\n",
      "Epoch 00010: val_fmeasure did not improve from 0.23839\n",
      "Epoch 11/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1094 - acc: 0.9635 - fmeasure: 0.8536 - recall: 0.8113 - precision: 0.9028 - val_loss: 0.8414 - val_acc: 0.8060 - val_fmeasure: 0.2217 - val_recall: 0.2092 - val_precision: 0.2373\n",
      "\n",
      "Epoch 00011: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 12/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1061 - acc: 0.9641 - fmeasure: 0.8551 - recall: 0.8070 - precision: 0.9121 - val_loss: 0.8400 - val_acc: 0.8038 - val_fmeasure: 0.2191 - val_recall: 0.2086 - val_precision: 0.2319\n",
      "\n",
      "Epoch 00012: val_fmeasure did not improve from 0.23839\n",
      "Epoch 13/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1051 - acc: 0.9659 - fmeasure: 0.8637 - recall: 0.8201 - precision: 0.9145 - val_loss: 0.8537 - val_acc: 0.8024 - val_fmeasure: 0.2152 - val_recall: 0.2050 - val_precision: 0.2275\n",
      "\n",
      "Epoch 00013: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 14/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.1022 - acc: 0.9657 - fmeasure: 0.8629 - recall: 0.8225 - precision: 0.9100 - val_loss: 0.8525 - val_acc: 0.8015 - val_fmeasure: 0.2126 - val_recall: 0.2028 - val_precision: 0.2244\n",
      "\n",
      "Epoch 00014: val_fmeasure did not improve from 0.23839\n",
      "Epoch 15/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0999 - acc: 0.9675 - fmeasure: 0.8707 - recall: 0.8305 - precision: 0.9176 - val_loss: 0.8569 - val_acc: 0.8013 - val_fmeasure: 0.2139 - val_recall: 0.2045 - val_precision: 0.2254\n",
      "\n",
      "Epoch 00015: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 16/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0994 - acc: 0.9672 - fmeasure: 0.8693 - recall: 0.8283 - precision: 0.9166 - val_loss: 0.8715 - val_acc: 0.8014 - val_fmeasure: 0.2127 - val_recall: 0.2029 - val_precision: 0.2246\n",
      "\n",
      "Epoch 00016: val_fmeasure did not improve from 0.23839\n",
      "Epoch 17/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0994 - acc: 0.9673 - fmeasure: 0.8693 - recall: 0.8267 - precision: 0.9189 - val_loss: 0.8707 - val_acc: 0.8011 - val_fmeasure: 0.2128 - val_recall: 0.2036 - val_precision: 0.2243\n",
      "\n",
      "Epoch 00017: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 18/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0982 - acc: 0.9674 - fmeasure: 0.8695 - recall: 0.8277 - precision: 0.9184 - val_loss: 0.8686 - val_acc: 0.8018 - val_fmeasure: 0.2138 - val_recall: 0.2041 - val_precision: 0.2259\n",
      "\n",
      "Epoch 00018: val_fmeasure did not improve from 0.23839\n",
      "Epoch 19/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0971 - acc: 0.9689 - fmeasure: 0.8757 - recall: 0.8346 - precision: 0.9236 - val_loss: 0.8657 - val_acc: 0.8016 - val_fmeasure: 0.2141 - val_recall: 0.2045 - val_precision: 0.2259\n",
      "\n",
      "Epoch 00019: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 20/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0989 - acc: 0.9668 - fmeasure: 0.8672 - recall: 0.8261 - precision: 0.9149 - val_loss: 0.8725 - val_acc: 0.8016 - val_fmeasure: 0.2137 - val_recall: 0.2040 - val_precision: 0.2256\n",
      "\n",
      "Epoch 00020: val_fmeasure did not improve from 0.23839\n",
      "Epoch 21/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0970 - acc: 0.9679 - fmeasure: 0.8715 - recall: 0.8292 - precision: 0.9204 - val_loss: 0.8754 - val_acc: 0.8017 - val_fmeasure: 0.2147 - val_recall: 0.2050 - val_precision: 0.2267\n",
      "\n",
      "Epoch 00021: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 22/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0980 - acc: 0.9677 - fmeasure: 0.8701 - recall: 0.8264 - precision: 0.9210 - val_loss: 0.8728 - val_acc: 0.8016 - val_fmeasure: 0.2133 - val_recall: 0.2035 - val_precision: 0.2253\n",
      "\n",
      "Epoch 00022: val_fmeasure did not improve from 0.23839\n",
      "Epoch 23/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0983 - acc: 0.9678 - fmeasure: 0.8716 - recall: 0.8296 - precision: 0.9205 - val_loss: 0.8781 - val_acc: 0.8014 - val_fmeasure: 0.2135 - val_recall: 0.2040 - val_precision: 0.2251\n",
      "\n",
      "Epoch 00023: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 24/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0979 - acc: 0.9670 - fmeasure: 0.8675 - recall: 0.8249 - precision: 0.9174 - val_loss: 0.8793 - val_acc: 0.8013 - val_fmeasure: 0.2134 - val_recall: 0.2040 - val_precision: 0.2249\n",
      "\n",
      "Epoch 00024: val_fmeasure did not improve from 0.23839\n",
      "Epoch 25/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0971 - acc: 0.9685 - fmeasure: 0.8747 - recall: 0.8349 - precision: 0.9210 - val_loss: 0.8775 - val_acc: 0.8013 - val_fmeasure: 0.2134 - val_recall: 0.2039 - val_precision: 0.2250\n",
      "\n",
      "Epoch 00025: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 26/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0943 - acc: 0.9697 - fmeasure: 0.8791 - recall: 0.8378 - precision: 0.9271 - val_loss: 0.8739 - val_acc: 0.8016 - val_fmeasure: 0.2146 - val_recall: 0.2050 - val_precision: 0.2262\n",
      "\n",
      "Epoch 00026: val_fmeasure did not improve from 0.23839\n",
      "Epoch 27/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0959 - acc: 0.9690 - fmeasure: 0.8761 - recall: 0.8334 - precision: 0.9261 - val_loss: 0.8761 - val_acc: 0.8012 - val_fmeasure: 0.2129 - val_recall: 0.2035 - val_precision: 0.2244\n",
      "\n",
      "Epoch 00027: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 28/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0975 - acc: 0.9684 - fmeasure: 0.8734 - recall: 0.8318 - precision: 0.9216 - val_loss: 0.8795 - val_acc: 0.8013 - val_fmeasure: 0.2130 - val_recall: 0.2035 - val_precision: 0.2247\n",
      "\n",
      "Epoch 00028: val_fmeasure did not improve from 0.23839\n",
      "Epoch 29/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0969 - acc: 0.9683 - fmeasure: 0.8731 - recall: 0.8320 - precision: 0.9206 - val_loss: 0.8793 - val_acc: 0.8014 - val_fmeasure: 0.2135 - val_recall: 0.2040 - val_precision: 0.2253\n",
      "\n",
      "Epoch 00029: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 30/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0970 - acc: 0.9691 - fmeasure: 0.8763 - recall: 0.8343 - precision: 0.9249 - val_loss: 0.8773 - val_acc: 0.8015 - val_fmeasure: 0.2132 - val_recall: 0.2035 - val_precision: 0.2252\n",
      "\n",
      "Epoch 00030: val_fmeasure did not improve from 0.23839\n",
      "Epoch 31/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0968 - acc: 0.9680 - fmeasure: 0.8723 - recall: 0.8313 - precision: 0.9195 - val_loss: 0.8763 - val_acc: 0.8015 - val_fmeasure: 0.2132 - val_recall: 0.2035 - val_precision: 0.2252\n",
      "\n",
      "Epoch 00031: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 32/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0957 - acc: 0.9680 - fmeasure: 0.8722 - recall: 0.8322 - precision: 0.9183 - val_loss: 0.8764 - val_acc: 0.8015 - val_fmeasure: 0.2140 - val_recall: 0.2044 - val_precision: 0.2258\n",
      "\n",
      "Epoch 00032: val_fmeasure did not improve from 0.23839\n",
      "Epoch 33/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0953 - acc: 0.9697 - fmeasure: 0.8790 - recall: 0.8391 - precision: 0.9249 - val_loss: 0.8732 - val_acc: 0.8016 - val_fmeasure: 0.2146 - val_recall: 0.2050 - val_precision: 0.2263\n",
      "\n",
      "Epoch 00033: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 34/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0965 - acc: 0.9686 - fmeasure: 0.8751 - recall: 0.8364 - precision: 0.9200 - val_loss: 0.8771 - val_acc: 0.8013 - val_fmeasure: 0.2130 - val_recall: 0.2035 - val_precision: 0.2247\n",
      "\n",
      "Epoch 00034: val_fmeasure did not improve from 0.23839\n",
      "Epoch 35/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0967 - acc: 0.9681 - fmeasure: 0.8728 - recall: 0.8322 - precision: 0.9204 - val_loss: 0.8733 - val_acc: 0.8014 - val_fmeasure: 0.2135 - val_recall: 0.2040 - val_precision: 0.2253\n",
      "\n",
      "Epoch 00035: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 36/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0958 - acc: 0.9685 - fmeasure: 0.8743 - recall: 0.8308 - precision: 0.9249 - val_loss: 0.8769 - val_acc: 0.8014 - val_fmeasure: 0.2131 - val_recall: 0.2035 - val_precision: 0.2248\n",
      "\n",
      "Epoch 00036: val_fmeasure did not improve from 0.23839\n",
      "Epoch 37/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0983 - acc: 0.9677 - fmeasure: 0.8706 - recall: 0.8278 - precision: 0.9207 - val_loss: 0.8736 - val_acc: 0.8015 - val_fmeasure: 0.2136 - val_recall: 0.2040 - val_precision: 0.2254\n",
      "\n",
      "Epoch 00037: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 38/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0972 - acc: 0.9681 - fmeasure: 0.8723 - recall: 0.8322 - precision: 0.9193 - val_loss: 0.8781 - val_acc: 0.8014 - val_fmeasure: 0.2131 - val_recall: 0.2035 - val_precision: 0.2248\n",
      "\n",
      "Epoch 00038: val_fmeasure did not improve from 0.23839\n",
      "Epoch 39/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0962 - acc: 0.9681 - fmeasure: 0.8730 - recall: 0.8324 - precision: 0.9198 - val_loss: 0.8769 - val_acc: 0.8013 - val_fmeasure: 0.2130 - val_recall: 0.2035 - val_precision: 0.2247\n",
      "\n",
      "Epoch 00039: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 40/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0970 - acc: 0.9683 - fmeasure: 0.8737 - recall: 0.8335 - precision: 0.9202 - val_loss: 0.8732 - val_acc: 0.8016 - val_fmeasure: 0.2144 - val_recall: 0.2050 - val_precision: 0.2260\n",
      "\n",
      "Epoch 00040: val_fmeasure did not improve from 0.23839\n",
      "Epoch 41/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0967 - acc: 0.9686 - fmeasure: 0.8744 - recall: 0.8319 - precision: 0.9235 - val_loss: 0.8765 - val_acc: 0.8014 - val_fmeasure: 0.2135 - val_recall: 0.2040 - val_precision: 0.2252\n",
      "\n",
      "Epoch 00041: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 42/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0980 - acc: 0.9685 - fmeasure: 0.8739 - recall: 0.8339 - precision: 0.9205 - val_loss: 0.8794 - val_acc: 0.8013 - val_fmeasure: 0.2130 - val_recall: 0.2035 - val_precision: 0.2248\n",
      "\n",
      "Epoch 00042: val_fmeasure did not improve from 0.23839\n",
      "Epoch 43/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0977 - acc: 0.9681 - fmeasure: 0.8726 - recall: 0.8302 - precision: 0.9224 - val_loss: 0.8796 - val_acc: 0.8012 - val_fmeasure: 0.2129 - val_recall: 0.2035 - val_precision: 0.2244\n",
      "\n",
      "Epoch 00043: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 44/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0971 - acc: 0.9687 - fmeasure: 0.8750 - recall: 0.8331 - precision: 0.9237 - val_loss: 0.8803 - val_acc: 0.8011 - val_fmeasure: 0.2136 - val_recall: 0.2044 - val_precision: 0.2249\n",
      "\n",
      "Epoch 00044: val_fmeasure did not improve from 0.23839\n",
      "Epoch 45/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0970 - acc: 0.9685 - fmeasure: 0.8745 - recall: 0.8340 - precision: 0.9211 - val_loss: 0.8798 - val_acc: 0.8012 - val_fmeasure: 0.2132 - val_recall: 0.2040 - val_precision: 0.2246\n",
      "\n",
      "Epoch 00045: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 46/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0964 - acc: 0.9686 - fmeasure: 0.8740 - recall: 0.8327 - precision: 0.9223 - val_loss: 0.8755 - val_acc: 0.8014 - val_fmeasure: 0.2135 - val_recall: 0.2040 - val_precision: 0.2252\n",
      "\n",
      "Epoch 00046: val_fmeasure did not improve from 0.23839\n",
      "Epoch 47/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0977 - acc: 0.9675 - fmeasure: 0.8699 - recall: 0.8288 - precision: 0.9175 - val_loss: 0.8752 - val_acc: 0.8015 - val_fmeasure: 0.2137 - val_recall: 0.2040 - val_precision: 0.2255\n",
      "\n",
      "Epoch 00047: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 48/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0968 - acc: 0.9680 - fmeasure: 0.8723 - recall: 0.8310 - precision: 0.9202 - val_loss: 0.8758 - val_acc: 0.8012 - val_fmeasure: 0.2140 - val_recall: 0.2050 - val_precision: 0.2252\n",
      "\n",
      "Epoch 00048: val_fmeasure did not improve from 0.23839\n",
      "Epoch 49/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0969 - acc: 0.9673 - fmeasure: 0.8692 - recall: 0.8276 - precision: 0.9174 - val_loss: 0.8766 - val_acc: 0.8013 - val_fmeasure: 0.2130 - val_recall: 0.2035 - val_precision: 0.2246\n",
      "\n",
      "Epoch 00049: val_fmeasure did not improve from 0.23839\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 50/50\n",
      "4875/4875 [==============================] - 7s 1ms/step - loss: 0.0958 - acc: 0.9683 - fmeasure: 0.8727 - recall: 0.8302 - precision: 0.9222 - val_loss: 0.8725 - val_acc: 0.8017 - val_fmeasure: 0.2142 - val_recall: 0.2046 - val_precision: 0.2261\n",
      "\n",
      "Epoch 00050: val_fmeasure did not improve from 0.23839\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "add_compile(model, config)\n",
    "\n",
    "model_name = 'resnet12.h5'\n",
    "earlystop = EarlyStopping(\n",
    "            monitor='val_fmeasure',#'val_categorical_accuracy',\n",
    "            patience=10,\n",
    "            )\n",
    "checkpoint = ModelCheckpoint(filepath=model_name,\n",
    "                             monitor='val_categorical_accuracy', mode='max',\n",
    "                             save_best_only='True')\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(config.lr_schedule)\n",
    "\n",
    "callback_lists = [checkpointer,reduce]#[checkpointer,earlystop,lr_scheduler]\n",
    "#[checkpoint, earlystop,lr_scheduler] \n",
    "\n",
    "history = model.fit(x=X_tr, y=y_tr, batch_size=32, epochs=50,  #class_weight=cw,#'auto',\n",
    "          verbose=1, validation_data=(X_vld, y_vld), callbacks=callback_lists )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:08, 61.03it/s]\n"
     ]
    }
   ],
   "source": [
    "test_x = read_data_sample(os.getcwd(),split='Val',preprocess=True)#read_data_sample(os.getcwd(),split='Val',preprocess=True)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_y = model.predict(test_x)  #predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.8503208e-01, 5.1888824e-04, 8.5613132e-04, ..., 3.2211900e-02,\n",
       "        2.3009121e-02, 2.7737021e-04],\n",
       "       [9.8081112e-02, 1.2540817e-04, 4.1021705e-03, ..., 1.4446229e-02,\n",
       "        8.8787079e-04, 7.2325170e-03],\n",
       "       [1.7760694e-03, 3.1007361e-01, 3.2027692e-02, ..., 6.4637882e-01,\n",
       "        5.6105852e-03, 5.0534213e-01],\n",
       "       ...,\n",
       "       [1.8537045e-05, 4.6483576e-03, 3.2123178e-02, ..., 3.3958137e-02,\n",
       "        3.6358833e-05, 5.6140423e-03],\n",
       "       [6.6468149e-02, 2.0886660e-03, 7.9625750e-01, ..., 5.1576585e-02,\n",
       "        9.8766983e-03, 2.2523215e-01],\n",
       "       [7.8547132e-01, 1.2442470e-03, 3.0868053e-03, ..., 3.9200875e-01,\n",
       "        4.6236217e-03, 1.8393993e-03]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 select best_threshold based on train data reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import hamming_loss\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold:  [0.6 0.5 0.3 0.2 0.2 0.4 0.4 0.4 0.4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03011965811965812"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_y = model.predict(train_x)#X_tr\n",
    "\n",
    "threshold = np.arange(0.1,0.9,0.1)\n",
    "\n",
    "out = x_tr_y\n",
    "y_test = bin_label#y_tr\n",
    "\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(out.shape[1])\n",
    "for i in range(out.shape[1]):\n",
    "    y_prob = np.array(out[:,i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
    "        acc.append( matthews_corrcoef(y_test[:,i],y_pred))\n",
    "    acc   = np.array(acc)\n",
    "    index = np.where(acc==acc.max()) \n",
    "    accuracies.append(acc.max()) \n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold: \",best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if out[i,j]>=best_threshold[j] else 0 for j in range(y_test.shape[1])] for i in range(len(y_test))])\n",
    "\n",
    "y_pred \n",
    "\n",
    "y_test\n",
    "\n",
    "hamming_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "y_pred = [[1 if test_y[i,j]>=best_threshold[j] else 0 for j in range(test_y.shape[1])] \n",
    "          for i in range(len(test_y))]\n",
    "pred=[]\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(9) if y_pred[j][i] == 1])\n",
    "\n",
    "with open('answers2.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "            \n",
    "            result = pred[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 select a single threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "thred = 0.2\n",
    "\n",
    "pred = []\n",
    "for i in range(test_y.shape[0]):\n",
    "    \n",
    "    try:\n",
    "        pred_list = list(np.hstack(np.argwhere(test_y[i]>thred)))\n",
    "    except ValueError:\n",
    "        print(\" ValueError !!! \")\n",
    "        pred_list = ['']\n",
    "        \n",
    "    pred.append(pred_list)\n",
    "    \n",
    "with open('answers.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "            \n",
    "            result = pred[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove 0 or others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "thred = 0.2#0.25\n",
    "\n",
    "pred = []\n",
    "for i in range(test_y.shape[0]):\n",
    "    if np.argmax(test_y[i]) == 0:\n",
    "        pred.append([0])\n",
    "        continue\n",
    "    try:\n",
    "        pred_list = list(np.hstack(np.argwhere(test_y[i]>thred)))\n",
    "    except ValueError:\n",
    "        pred_list = ['']\n",
    "        \n",
    "    if 0 in pred_list:\n",
    "        if np.argmax(test_y[i]) != 0:\n",
    "            pred_list.remove(0)\n",
    "            \n",
    "    pred.append(pred_list)\n",
    "    \n",
    "with open('answers.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "            \n",
    "            result = pred[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "NFOLDS = 5\n",
    "history = []\n",
    "y_test_pred = []\n",
    "\n",
    "train = X_tr #x_tr\n",
    "test = test_x\n",
    "train_label =  y_tr\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed)\n",
    "kf = kfold.split(train, train_label)\n",
    "\n",
    "config = Config()\n",
    "add_compile(model, config)\n",
    "\n",
    "model_name = 'resnet12.h5'\n",
    "earlystop = EarlyStopping(\n",
    "            monitor='val_categorical_accuracy',\n",
    "            patience=10,\n",
    "            )\n",
    "checkpoint = ModelCheckpoint(filepath=model_name,\n",
    "                             monitor='val_categorical_accuracy', mode='max',\n",
    "                             save_best_only='True')\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(config.lr_schedule)\n",
    "\n",
    "callback_lists = [checkpoint, earlystop,lr_scheduler]\n",
    "\n",
    "history = model.fit(x=X_tr, y=y_tr, batch_size=32, epochs=50,\n",
    "          verbose=1, validation_data=(X_vld, y_vld), callbacks=[checkpointer,reduce])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "NFOLDS = 5\n",
    "history = []#np.zeros(NFOLDS)\n",
    "y_test_pred = []#np.zeros(NFOLDS)\n",
    "\n",
    "train = train_x #x_tr\n",
    "#test = x_test[:400]\n",
    "train_label =  bin_label#y_tr[1].values\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed)\n",
    "kf = kfold.split(train, train_label)\n",
    "\n",
    "train_data_use = train\n",
    "#test_data_use = test\n",
    "pre_train_data_use = pre_x_tr\n",
    "\n",
    "for i, (train_fold, validate) in enumerate(kf):\n",
    "    print('fold: ',i, ' training')\n",
    "    np.random.shuffle(train_fold)\n",
    "    np.random.shuffle(validate)\n",
    "    #print(train_fold)\n",
    "    #print(validate)\n",
    "    \n",
    "    # original data\n",
    "    X_train0, X_val0, label_train0, label_val0 = train_data_use[train_fold], train_data_use[validate], \\\n",
    "                                                train_label[train_fold], train_label[validate]\n",
    "    # preprocessed data\n",
    "    X_train1, X_val1, label_train1, label_val1 = pre_train_data_use[train_fold], pre_train_data_use[validate], \\\n",
    "                                                train_label[train_fold], train_label[validate]\n",
    "        \n",
    "    X_train = np.concatenate([X_train0 , X_train1],axis=0)\n",
    "    X_val = np.concatenate([X_val0 , X_val1],axis=0)\n",
    "    label_train = np.concatenate([label_train0 , label_train1],axis=0)\n",
    "    label_val = np.concatenate([label_val0 , label_val1],axis=0)\n",
    "    \n",
    "    #only original data for train\n",
    "    #X_train, X_val, label_train, label_val= X_train0, X_val0, label_train0, label_val0\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_val.shape)\n",
    "    \n",
    "    print(label_train.shape)\n",
    "    print(label_val.shape)\n",
    "    \n",
    "    # convert one-hot\n",
    "    one_hot_y_tr = one_hot(label_train)\n",
    "    one_hot_y_val  = one_hot(label_val)\n",
    "    \n",
    "    config = Config()\n",
    "    add_compile(model, config)\n",
    "    model_name = 'merge_densenet_fold{0}.h5'.format(i+1)\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_categorical_accuracy',\n",
    "                patience=20,\n",
    "                )\n",
    "    checkpoint = ModelCheckpoint(filepath=model_name,\n",
    "                                 monitor='val_categorical_accuracy', mode='max',\n",
    "                                 save_best_only='True')\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(config.lr_schedule)\n",
    "    callback_lists = [checkpoint, earlystop,lr_scheduler]\n",
    "    history.append( model.fit(x=X_train, y=one_hot_y_tr, batch_size=config.batch_size, epochs=100,\n",
    "              verbose=1, validation_data=(X_val, one_hot_y_val), callbacks=callback_lists))\n",
    "    \n",
    "    #y_test_pred.append( model.predict(test_data_use))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
