{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wfdb\n",
    "import os\n",
    "import wfdb.processing as wp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import shutil\n",
    "\n",
    "import time\n",
    "from scipy import sparse\n",
    "import os\n",
    "import warnings\n",
    "import scipy.io as sio\n",
    "\n",
    "from resnet_ecg.densenet import DenseNet\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_path = os.getcwd()+\"/Train/\"\n",
    "val_dataset_path = os.getcwd()+\"/Val/\"\n",
    "\n",
    "train_files = os.listdir(train_dataset_path)\n",
    "train_files.sort()\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "      <th>label6</th>\n",
       "      <th>label7</th>\n",
       "      <th>label8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN0001</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN0001</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN0001</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN0002</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN0002</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File_name  label1  label2  label3  label4  label5  label6  label7  label8\n",
       "0  TRAIN0001       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "1  TRAIN0001       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "2  TRAIN0001       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "3  TRAIN0002       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN\n",
       "4  TRAIN0002       8     NaN     NaN     NaN     NaN     NaN     NaN     NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"en_all_labels.csv\")\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = np.load('en_all_train_x.npy')#en_train_x_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500, 3000, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "sum1_label=0\n",
    "sum2_label=0\n",
    "sum3_label=0\n",
    "sum4_label=0\n",
    "sum5_label=0\n",
    "sum6_label=0\n",
    "sum7_label=0\n",
    "''' \n",
    "bin_label = np.zeros((labels.shape[0],9))\n",
    "for i in range(labels.shape[0]):\n",
    "    label_nona = labels.loc[i].dropna()\n",
    "    ''' \n",
    "    if label_nona.shape[0] == 2:\n",
    "        sum1_label+=1\n",
    "    if label_nona.shape[0] == 3:\n",
    "        sum2_label+=1\n",
    "    if label_nona.shape[0] == 4:\n",
    "        sum3_label+=1\n",
    "    if label_nona.shape[0] == 5:\n",
    "        sum4_label+=1\n",
    "    if label_nona.shape[0] == 6:\n",
    "        sum5_label+=1\n",
    "    if label_nona.shape[0] == 7:\n",
    "        sum6_label+=1\n",
    "    if label_nona.shape[0] == 8:\n",
    "        sum7_label+=1\n",
    "    '''\n",
    "    for j in range(1,label_nona.shape[0]):\n",
    "        bin_label[i,int(label_nona[j])]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler,EarlyStopping,ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.conv_subsample_lengths = [1, 2, 1, 2, 1, 2, 1, 2]\n",
    "        self.conv_filter_length = 32\n",
    "        self.conv_num_filters_start = 12\n",
    "        self.conv_init = \"he_normal\"\n",
    "        self.conv_activation = \"relu\"\n",
    "        self.conv_dropout = 0.5\n",
    "        self.conv_num_skip = 2\n",
    "        self.conv_increase_channels_at = 2\n",
    "        self.batch_size = 32#128\n",
    "        self.input_shape = [2560, 12]#[1280, 1]\n",
    "        self.num_categories = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 0.1\n",
    "        if epoch >= 20 and epoch < 40:\n",
    "            lr = 0.01\n",
    "        if epoch >= 40:\n",
    "            lr = 0.001\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DenseNet 0.0.1\n",
      "#############################################\n",
      "Dense blocks: 3\n",
      "Layers per dense block: [2, 2, 2]\n",
      "#############################################\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "block::: 0\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/uuser/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "block::: 1\n",
      "nb_channels:: Tensor(\"concatenate_6/concat:0\", shape=(?, 187, 60), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(session )\n",
    "\n",
    "n_steps = 3000\n",
    "model = DenseNet(input_shape=(n_steps,12), nb_classes=9, depth=10, growth_rate=16,\n",
    "                          dropout_rate=0.1, bottleneck=False, compression=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3000, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 3000, 16)     6144        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 3000, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 3000, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3000, 16)     8192        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3000, 16)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3000, 32)     0           conv1d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 3000, 32)     128         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 3000, 32)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 3000, 16)     16384       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 3000, 16)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3000, 48)     0           conv1d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 3000, 48)     192         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 3000, 48)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1500, 24)     36864       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1500, 24)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePoo (None, 750, 24)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 750, 24)      96          average_pooling1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 750, 24)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 750, 16)      12288       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 750, 16)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 750, 40)      0           average_pooling1d_1[0][0]        \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 750, 40)      160         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 750, 40)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 750, 16)      20480       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 750, 16)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 750, 56)      0           average_pooling1d_1[0][0]        \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 750, 56)      224         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 750, 56)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 375, 28)      50176       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 375, 28)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePoo (None, 187, 28)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 187, 28)      112         average_pooling1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 187, 28)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 187, 64)      57344       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 187, 64)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 187, 64)      256         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 187, 64)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 187, 16)      32768       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 187, 16)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 187, 44)      0           average_pooling1d_2[0][0]        \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 187, 44)      176         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 187, 44)      0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 187, 64)      90112       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 187, 64)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 187, 64)      256         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 187, 64)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 187, 16)      32768       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 187, 16)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 187, 60)      0           average_pooling1d_2[0][0]        \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 187, 60)      240         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 187, 60)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 60)           0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 9)            549         global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 365,973\n",
      "Trainable params: 365,021\n",
      "Non-trainable params: 952\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "    \n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(train_x, labels['label1'], \n",
    "                                        stratify = labels['label1'], #test_size=0.2,\n",
    "                                        shuffle=True, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8083, 15109, 1974, 538, 6050]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_tr.index.tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_y(y,num_class=9):\n",
    "    bin_label = np.zeros((y.shape[0],num_class))\n",
    "    for i in range(y.shape[0]):\n",
    "        label_nona = labels.loc[lab_tr.index.tolist()[i]].dropna()\n",
    "        for j in range(1,label_nona.shape[0]):\n",
    "            bin_label[i,int(label_nona[j])]=1\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tr = preprocess_y(lab_tr)\n",
    "y_vld = preprocess_y(lab_vld) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights_best_simple_model_2.hdf5', \n",
    "                            monitor='val_fmeasure',verbose=1, save_best_only=True, mode='max')\n",
    "reduce = ReduceLROnPlateau(monitor='val_fmeasure',factor=0.5,patience=2,verbose=1,min_delta=1e-4,mode='max')\n",
    "''' \n",
    "model.compile(optimizer = 'adam',\n",
    "           loss='binary_crossentropy',\n",
    "           metrics=['accuracy',fmeasure,recall,precision])\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "       validation_data = val_generator,\n",
    "       epochs=epochs,\n",
    "       callbacks=[checkpointer,reduce],\n",
    "       verbose=1)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "#2156+224+672+654+180+826+534+504+1953\n",
    "\n",
    "def weighted_loss(labels,logits):\n",
    "    logits_log = (-1) * K.log(logits)\n",
    "    num_class = logits.shape[-1]\n",
    "    \n",
    "    y_0 = 0.8 * labels[:,0:1]\n",
    "    y_1 = 1.2 * labels[:,1:2]\n",
    "    y_2 = 1.0 * labels[:,2:3]\n",
    "    y_3 = 1.0 * labels[:,3:4]\n",
    "    y_4 = 1.2 * labels[:,4:5]\n",
    "    y_5 = 1.0 * labels[:,5:6]\n",
    "    y_6 = 1.0 * labels[:,6:7]\n",
    "    y_7 = 1.0 * labels[:,7:8]\n",
    "    y_8 = 0.8 * labels[:,8:]\n",
    "    \n",
    "    weight_labels = K.concatenate([y_0,y_1,y_2,y_3,y_4,y_5,y_6,y_7,y_8],axis=1)\n",
    "    loss = K.sum(logits_log * weight_labels,axis=1)\n",
    "    \n",
    "    loss = K.mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_compile(model, config):\n",
    "    \n",
    "    optimizer = SGD(lr=config.lr_schedule(0), momentum=0.9)#Adam()#\n",
    "    model.compile(loss='binary_crossentropy',#weighted_loss,#'binary_crossentropy',\n",
    "                  optimizer='adam',#optimizer\n",
    "                  metrics=['accuracy',fmeasure,recall,precision])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.1\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 14625 samples, validate on 4875 samples\n",
      "Epoch 1/50\n",
      "14625/14625 [==============================] - 27s 2ms/step - loss: 0.3479 - acc: 0.8786 - fmeasure: 0.4139 - recall: 0.3063 - precision: 0.7080 - val_loss: 0.5680 - val_acc: 0.8333 - val_fmeasure: 0.2111 - val_recall: 0.1708 - val_precision: 0.2789\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.21109, saving model to weights_best_simple_model_2.hdf5\n",
      "Epoch 2/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.2595 - acc: 0.9073 - fmeasure: 0.5174 - recall: 0.3804 - precision: 0.8173 - val_loss: 0.6726 - val_acc: 0.8396 - val_fmeasure: 0.2154 - val_recall: 0.1685 - val_precision: 0.3006\n",
      "\n",
      "Epoch 00002: val_fmeasure improved from 0.21109 to 0.21539, saving model to weights_best_simple_model_2.hdf5\n",
      "Epoch 3/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.2322 - acc: 0.9156 - fmeasure: 0.5773 - recall: 0.4411 - precision: 0.8429 - val_loss: 0.6590 - val_acc: 0.8266 - val_fmeasure: 0.2010 - val_recall: 0.1668 - val_precision: 0.2546\n",
      "\n",
      "Epoch 00003: val_fmeasure did not improve from 0.21539\n",
      "Epoch 4/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.2171 - acc: 0.9209 - fmeasure: 0.6158 - recall: 0.4842 - precision: 0.8514 - val_loss: 0.6619 - val_acc: 0.8190 - val_fmeasure: 0.1779 - val_recall: 0.1497 - val_precision: 0.2208\n",
      "\n",
      "Epoch 00004: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.2020 - acc: 0.9271 - fmeasure: 0.6513 - recall: 0.5217 - precision: 0.8728 - val_loss: 0.6394 - val_acc: 0.8281 - val_fmeasure: 0.1803 - val_recall: 0.1446 - val_precision: 0.2410\n",
      "\n",
      "Epoch 00005: val_fmeasure did not improve from 0.21539\n",
      "Epoch 6/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1945 - acc: 0.9296 - fmeasure: 0.6683 - recall: 0.5417 - precision: 0.8781 - val_loss: 0.7541 - val_acc: 0.8229 - val_fmeasure: 0.1825 - val_recall: 0.1513 - val_precision: 0.2319\n",
      "\n",
      "Epoch 00006: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1864 - acc: 0.9321 - fmeasure: 0.6819 - recall: 0.5560 - precision: 0.8862 - val_loss: 0.6964 - val_acc: 0.8233 - val_fmeasure: 0.1901 - val_recall: 0.1586 - val_precision: 0.2390\n",
      "\n",
      "Epoch 00007: val_fmeasure did not improve from 0.21539\n",
      "Epoch 8/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1820 - acc: 0.9343 - fmeasure: 0.6954 - recall: 0.5731 - precision: 0.8889 - val_loss: 0.7458 - val_acc: 0.8219 - val_fmeasure: 0.1873 - val_recall: 0.1569 - val_precision: 0.2341\n",
      "\n",
      "Epoch 00008: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1766 - acc: 0.9358 - fmeasure: 0.7029 - recall: 0.5814 - precision: 0.8932 - val_loss: 0.7532 - val_acc: 0.8196 - val_fmeasure: 0.1802 - val_recall: 0.1514 - val_precision: 0.2241\n",
      "\n",
      "Epoch 00009: val_fmeasure did not improve from 0.21539\n",
      "Epoch 10/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1748 - acc: 0.9361 - fmeasure: 0.7050 - recall: 0.5840 - precision: 0.8940 - val_loss: 0.7513 - val_acc: 0.8206 - val_fmeasure: 0.1911 - val_recall: 0.1619 - val_precision: 0.2347\n",
      "\n",
      "Epoch 00010: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1715 - acc: 0.9374 - fmeasure: 0.7126 - recall: 0.5934 - precision: 0.8961 - val_loss: 0.7610 - val_acc: 0.8206 - val_fmeasure: 0.1930 - val_recall: 0.1637 - val_precision: 0.2362\n",
      "\n",
      "Epoch 00011: val_fmeasure did not improve from 0.21539\n",
      "Epoch 12/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1697 - acc: 0.9375 - fmeasure: 0.7122 - recall: 0.5922 - precision: 0.8985 - val_loss: 0.7853 - val_acc: 0.8201 - val_fmeasure: 0.1918 - val_recall: 0.1633 - val_precision: 0.2339\n",
      "\n",
      "Epoch 00012: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1675 - acc: 0.9393 - fmeasure: 0.7210 - recall: 0.6010 - precision: 0.9057 - val_loss: 0.7562 - val_acc: 0.8197 - val_fmeasure: 0.1907 - val_recall: 0.1626 - val_precision: 0.2321\n",
      "\n",
      "Epoch 00013: val_fmeasure did not improve from 0.21539\n",
      "Epoch 14/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1665 - acc: 0.9394 - fmeasure: 0.7233 - recall: 0.6047 - precision: 0.9034 - val_loss: 0.7772 - val_acc: 0.8195 - val_fmeasure: 0.1927 - val_recall: 0.1647 - val_precision: 0.2337\n",
      "\n",
      "Epoch 00014: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 15/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1662 - acc: 0.9396 - fmeasure: 0.7230 - recall: 0.6028 - precision: 0.9080 - val_loss: 0.7823 - val_acc: 0.8197 - val_fmeasure: 0.1940 - val_recall: 0.1660 - val_precision: 0.2350\n",
      "\n",
      "Epoch 00015: val_fmeasure did not improve from 0.21539\n",
      "Epoch 16/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1649 - acc: 0.9403 - fmeasure: 0.7266 - recall: 0.6067 - precision: 0.9097 - val_loss: 0.7895 - val_acc: 0.8190 - val_fmeasure: 0.1926 - val_recall: 0.1650 - val_precision: 0.2327\n",
      "\n",
      "Epoch 00016: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 17/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1647 - acc: 0.9395 - fmeasure: 0.7237 - recall: 0.6058 - precision: 0.9026 - val_loss: 0.7863 - val_acc: 0.8192 - val_fmeasure: 0.1923 - val_recall: 0.1645 - val_precision: 0.2328\n",
      "\n",
      "Epoch 00017: val_fmeasure did not improve from 0.21539\n",
      "Epoch 18/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1647 - acc: 0.9400 - fmeasure: 0.7255 - recall: 0.6067 - precision: 0.9060 - val_loss: 0.7849 - val_acc: 0.8190 - val_fmeasure: 0.1911 - val_recall: 0.1634 - val_precision: 0.2312\n",
      "\n",
      "Epoch 00018: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 19/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1638 - acc: 0.9400 - fmeasure: 0.7263 - recall: 0.6081 - precision: 0.9051 - val_loss: 0.7833 - val_acc: 0.8195 - val_fmeasure: 0.1927 - val_recall: 0.1647 - val_precision: 0.2337\n",
      "\n",
      "Epoch 00019: val_fmeasure did not improve from 0.21539\n",
      "Epoch 20/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1643 - acc: 0.9398 - fmeasure: 0.7246 - recall: 0.6058 - precision: 0.9057 - val_loss: 0.7843 - val_acc: 0.8193 - val_fmeasure: 0.1916 - val_recall: 0.1638 - val_precision: 0.2323\n",
      "\n",
      "Epoch 00020: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 21/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1643 - acc: 0.9401 - fmeasure: 0.7266 - recall: 0.6096 - precision: 0.9040 - val_loss: 0.7882 - val_acc: 0.8188 - val_fmeasure: 0.1900 - val_recall: 0.1625 - val_precision: 0.2300\n",
      "\n",
      "Epoch 00021: val_fmeasure did not improve from 0.21539\n",
      "Epoch 22/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1639 - acc: 0.9400 - fmeasure: 0.7268 - recall: 0.6092 - precision: 0.9052 - val_loss: 0.7870 - val_acc: 0.8188 - val_fmeasure: 0.1901 - val_recall: 0.1626 - val_precision: 0.2302\n",
      "\n",
      "Epoch 00022: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 23/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1641 - acc: 0.9398 - fmeasure: 0.7257 - recall: 0.6079 - precision: 0.9042 - val_loss: 0.7877 - val_acc: 0.8190 - val_fmeasure: 0.1914 - val_recall: 0.1638 - val_precision: 0.2316\n",
      "\n",
      "Epoch 00023: val_fmeasure did not improve from 0.21539\n",
      "Epoch 24/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1637 - acc: 0.9404 - fmeasure: 0.7276 - recall: 0.6089 - precision: 0.9084 - val_loss: 0.7849 - val_acc: 0.8191 - val_fmeasure: 0.1910 - val_recall: 0.1633 - val_precision: 0.2314\n",
      "\n",
      "Epoch 00024: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 25/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1638 - acc: 0.9399 - fmeasure: 0.7262 - recall: 0.6095 - precision: 0.9026 - val_loss: 0.7867 - val_acc: 0.8188 - val_fmeasure: 0.1918 - val_recall: 0.1643 - val_precision: 0.2316\n",
      "\n",
      "Epoch 00025: val_fmeasure did not improve from 0.21539\n",
      "Epoch 26/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1633 - acc: 0.9404 - fmeasure: 0.7280 - recall: 0.6101 - precision: 0.9066 - val_loss: 0.7872 - val_acc: 0.8190 - val_fmeasure: 0.1924 - val_recall: 0.1650 - val_precision: 0.2323\n",
      "\n",
      "Epoch 00026: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 27/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1641 - acc: 0.9401 - fmeasure: 0.7267 - recall: 0.6086 - precision: 0.9058 - val_loss: 0.7882 - val_acc: 0.8192 - val_fmeasure: 0.1934 - val_recall: 0.1657 - val_precision: 0.2336\n",
      "\n",
      "Epoch 00027: val_fmeasure did not improve from 0.21539\n",
      "Epoch 28/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1641 - acc: 0.9397 - fmeasure: 0.7250 - recall: 0.6079 - precision: 0.9019 - val_loss: 0.7896 - val_acc: 0.8191 - val_fmeasure: 0.1950 - val_recall: 0.1675 - val_precision: 0.2348\n",
      "\n",
      "Epoch 00028: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 29/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1639 - acc: 0.9401 - fmeasure: 0.7267 - recall: 0.6094 - precision: 0.9048 - val_loss: 0.7890 - val_acc: 0.8192 - val_fmeasure: 0.1933 - val_recall: 0.1657 - val_precision: 0.2335\n",
      "\n",
      "Epoch 00029: val_fmeasure did not improve from 0.21539\n",
      "Epoch 30/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1642 - acc: 0.9399 - fmeasure: 0.7256 - recall: 0.6069 - precision: 0.9058 - val_loss: 0.7882 - val_acc: 0.8193 - val_fmeasure: 0.1927 - val_recall: 0.1650 - val_precision: 0.2332\n",
      "\n",
      "Epoch 00030: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 31/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1639 - acc: 0.9401 - fmeasure: 0.7268 - recall: 0.6099 - precision: 0.9044 - val_loss: 0.7882 - val_acc: 0.8189 - val_fmeasure: 0.1904 - val_recall: 0.1627 - val_precision: 0.2308\n",
      "\n",
      "Epoch 00031: val_fmeasure did not improve from 0.21539\n",
      "Epoch 32/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1642 - acc: 0.9402 - fmeasure: 0.7274 - recall: 0.6103 - precision: 0.9043 - val_loss: 0.7911 - val_acc: 0.8191 - val_fmeasure: 0.1917 - val_recall: 0.1640 - val_precision: 0.2320\n",
      "\n",
      "Epoch 00032: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 33/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1642 - acc: 0.9403 - fmeasure: 0.7273 - recall: 0.6093 - precision: 0.9060 - val_loss: 0.7886 - val_acc: 0.8191 - val_fmeasure: 0.1928 - val_recall: 0.1652 - val_precision: 0.2330\n",
      "\n",
      "Epoch 00033: val_fmeasure did not improve from 0.21539\n",
      "Epoch 34/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1632 - acc: 0.9403 - fmeasure: 0.7277 - recall: 0.6096 - precision: 0.9069 - val_loss: 0.7882 - val_acc: 0.8191 - val_fmeasure: 0.1911 - val_recall: 0.1634 - val_precision: 0.2316\n",
      "\n",
      "Epoch 00034: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 35/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1642 - acc: 0.9400 - fmeasure: 0.7262 - recall: 0.6082 - precision: 0.9050 - val_loss: 0.7889 - val_acc: 0.8193 - val_fmeasure: 0.1943 - val_recall: 0.1665 - val_precision: 0.2345\n",
      "\n",
      "Epoch 00035: val_fmeasure did not improve from 0.21539\n",
      "Epoch 36/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1636 - acc: 0.9401 - fmeasure: 0.7265 - recall: 0.6092 - precision: 0.9046 - val_loss: 0.7896 - val_acc: 0.8193 - val_fmeasure: 0.1932 - val_recall: 0.1654 - val_precision: 0.2337\n",
      "\n",
      "Epoch 00036: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 37/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1630 - acc: 0.9411 - fmeasure: 0.7313 - recall: 0.6129 - precision: 0.9106 - val_loss: 0.7887 - val_acc: 0.8188 - val_fmeasure: 0.1915 - val_recall: 0.1641 - val_precision: 0.2313\n",
      "\n",
      "Epoch 00037: val_fmeasure did not improve from 0.21539\n",
      "Epoch 38/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1629 - acc: 0.9410 - fmeasure: 0.7306 - recall: 0.6121 - precision: 0.9104 - val_loss: 0.7907 - val_acc: 0.8193 - val_fmeasure: 0.1928 - val_recall: 0.1650 - val_precision: 0.2333\n",
      "\n",
      "Epoch 00038: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 39/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1636 - acc: 0.9400 - fmeasure: 0.7266 - recall: 0.6090 - precision: 0.9048 - val_loss: 0.7902 - val_acc: 0.8188 - val_fmeasure: 0.1918 - val_recall: 0.1643 - val_precision: 0.2316\n",
      "\n",
      "Epoch 00039: val_fmeasure did not improve from 0.21539\n",
      "Epoch 40/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1638 - acc: 0.9401 - fmeasure: 0.7265 - recall: 0.6084 - precision: 0.9065 - val_loss: 0.7850 - val_acc: 0.8188 - val_fmeasure: 0.1912 - val_recall: 0.1636 - val_precision: 0.2312\n",
      "\n",
      "Epoch 00040: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 41/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1644 - acc: 0.9400 - fmeasure: 0.7260 - recall: 0.6083 - precision: 0.9046 - val_loss: 0.7882 - val_acc: 0.8189 - val_fmeasure: 0.1913 - val_recall: 0.1638 - val_precision: 0.2316\n",
      "\n",
      "Epoch 00041: val_fmeasure did not improve from 0.21539\n",
      "Epoch 42/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1639 - acc: 0.9399 - fmeasure: 0.7258 - recall: 0.6077 - precision: 0.9053 - val_loss: 0.7863 - val_acc: 0.8192 - val_fmeasure: 0.1922 - val_recall: 0.1645 - val_precision: 0.2327\n",
      "\n",
      "Epoch 00042: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 43/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1632 - acc: 0.9406 - fmeasure: 0.7296 - recall: 0.6119 - precision: 0.9074 - val_loss: 0.7872 - val_acc: 0.8191 - val_fmeasure: 0.1923 - val_recall: 0.1646 - val_precision: 0.2324\n",
      "\n",
      "Epoch 00043: val_fmeasure did not improve from 0.21539\n",
      "Epoch 44/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1637 - acc: 0.9400 - fmeasure: 0.7260 - recall: 0.6079 - precision: 0.9054 - val_loss: 0.7888 - val_acc: 0.8192 - val_fmeasure: 0.1925 - val_recall: 0.1647 - val_precision: 0.2330\n",
      "\n",
      "Epoch 00044: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 45/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1639 - acc: 0.9402 - fmeasure: 0.7270 - recall: 0.6091 - precision: 0.9062 - val_loss: 0.7873 - val_acc: 0.8189 - val_fmeasure: 0.1923 - val_recall: 0.1648 - val_precision: 0.2322\n",
      "\n",
      "Epoch 00045: val_fmeasure did not improve from 0.21539\n",
      "Epoch 46/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1634 - acc: 0.9406 - fmeasure: 0.7284 - recall: 0.6092 - precision: 0.9099 - val_loss: 0.7888 - val_acc: 0.8191 - val_fmeasure: 0.1923 - val_recall: 0.1647 - val_precision: 0.2327\n",
      "\n",
      "Epoch 00046: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 47/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1643 - acc: 0.9402 - fmeasure: 0.7266 - recall: 0.6074 - precision: 0.9081 - val_loss: 0.7855 - val_acc: 0.8191 - val_fmeasure: 0.1918 - val_recall: 0.1642 - val_precision: 0.2321\n",
      "\n",
      "Epoch 00047: val_fmeasure did not improve from 0.21539\n",
      "Epoch 48/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1635 - acc: 0.9403 - fmeasure: 0.7275 - recall: 0.6084 - precision: 0.9088 - val_loss: 0.7864 - val_acc: 0.8188 - val_fmeasure: 0.1917 - val_recall: 0.1643 - val_precision: 0.2315\n",
      "\n",
      "Epoch 00048: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 49/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1628 - acc: 0.9406 - fmeasure: 0.7293 - recall: 0.6109 - precision: 0.9088 - val_loss: 0.7875 - val_acc: 0.8191 - val_fmeasure: 0.1922 - val_recall: 0.1645 - val_precision: 0.2326\n",
      "\n",
      "Epoch 00049: val_fmeasure did not improve from 0.21539\n",
      "Epoch 50/50\n",
      "14625/14625 [==============================] - 18s 1ms/step - loss: 0.1634 - acc: 0.9404 - fmeasure: 0.7282 - recall: 0.6105 - precision: 0.9057 - val_loss: 0.7888 - val_acc: 0.8188 - val_fmeasure: 0.1919 - val_recall: 0.1645 - val_precision: 0.2317\n",
      "\n",
      "Epoch 00050: val_fmeasure did not improve from 0.21539\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "add_compile(model, config)\n",
    "\n",
    "model_name = 'resnet12.h5'\n",
    "earlystop = EarlyStopping(\n",
    "            monitor='val_fmeasure',#'val_categorical_accuracy',\n",
    "            patience=10,\n",
    "            )\n",
    "checkpoint = ModelCheckpoint(filepath=model_name,\n",
    "                             monitor='val_categorical_accuracy', mode='max',\n",
    "                             save_best_only='True')\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(config.lr_schedule)\n",
    "\n",
    "callback_lists = [checkpointer,reduce]#[checkpointer,earlystop,lr_scheduler]\n",
    "#[checkpoint, earlystop,lr_scheduler] \n",
    "\n",
    "history = model.fit(x=X_tr, y=y_tr, batch_size=64, epochs=50,  #class_weight='auto',\n",
    "          verbose=1, validation_data=(X_vld, y_vld), callbacks=callback_lists )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "def read_data_seg(data_path, split = \"Val\",preprocess=False):\n",
    "    \"\"\" Read data \"\"\"\n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 3000#2560\n",
    "\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    #print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12#len(channel_files)\n",
    "    #posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "    X = np.zeros((len(channel_files), n_steps, n_channels))\n",
    "    i_ch = 0\n",
    "    \n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1','II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II','aVR','V2','V5']\n",
    "    channel_post_name = ['III','aVF','V3','V6']\n",
    "    \n",
    "    for i_ch,fil_ch in tqdm(enumerate(channel_files[:])):\n",
    "        ecg = sio.loadmat(os.path.join(path_signals,fil_ch))\n",
    "        ecg_channels = np.zeros((n_steps, n_channels))\n",
    "        for i_n,ch_name in enumerate(channel_name):\n",
    "            \n",
    "            if ch_name in channel_mid_name:\n",
    "                mid_ind = int(ecg[ch_name].T.shape[0]/2)\n",
    "                ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[mid_ind-2500:mid_ind+2500],n_steps).T \n",
    "            elif ch_name in channel_post_name:\n",
    "                ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[-5000:],n_steps).T\n",
    "            else:\n",
    "                ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[:5000],n_steps).T\n",
    "            #ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T,2560).T\n",
    "            if preprocess:\n",
    "                data = ecg_preprocessing(ecg_channels[:,i_n].reshape(1,n_steps), 'sym8', 8, 3, n_steps/10)\n",
    "                ecg_channels[:,i_n] = data[0]#ecg['data']\n",
    "            else:\n",
    "                pass\n",
    "        X[i_ch,:,:] = ecg_channels\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "import pywt\n",
    "def read_data_sample(data_path, split = \"Val\",preprocess=False):\n",
    "    \"\"\" Read data \"\"\"\n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 3000#2560\n",
    "\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    #print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12#len(channel_files)\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "    X = np.zeros((len(channel_files), n_steps, n_channels))\n",
    "    i_ch = 0\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1',  'II', 'aVR', 'V3', 'III', 'V5']\n",
    "    \n",
    "    #shape_ecg = []\n",
    "    \n",
    "    for i_ch,fil_ch in tqdm(enumerate(channel_files[:])):       \n",
    "        ecg = sio.loadmat(os.path.join(path_signals,fil_ch))        \n",
    "        ecg_channels = np.zeros((n_steps, n_channels))        \n",
    "        for i_n,ch_name in enumerate(channel_name[:]):            \n",
    "            ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T,n_steps).T\n",
    "            #shape_ecg.append(ecg[ch_name].shape[1])\n",
    "            if preprocess:\n",
    "                data = ecg_preprocessing(ecg_channels[:,i_n].reshape(1,n_steps), 'sym8', 8, 3, n_steps/10)\n",
    "                ecg_channels[:,i_n] = data[0]#ecg['data']\n",
    "            else:\n",
    "                pass\n",
    "        X[i_ch,:,:] = ecg_channels\n",
    "\n",
    "    return X#shape_ecg#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:08, 58.72it/s]\n"
     ]
    }
   ],
   "source": [
    "test_x = read_data_sample(os.getcwd(),split='Val',preprocess=True)#read_data_sample  read_data_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3000, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wavelet(ecg,wavefunc,lv,m,n):   #\n",
    "    \n",
    "    coeff = pywt.wavedec(ecg,wavefunc,mode='sym',level=lv)   #\n",
    "    #sgn = lambda x: 1 if x > 0 else -1 if x < 0 else 0\n",
    "\n",
    "    for i in range(m,n+1):  \n",
    "        cD = coeff[i]\n",
    "        for j in range(len(cD)):\n",
    "            Tr = np.sqrt(2*np.log(len(cD)))  \n",
    "            if cD[j] >= Tr:\n",
    "                coeff[i][j] = np.sign(cD[j]) - Tr \n",
    "            else:\n",
    "                coeff[i][j] = 0   \n",
    "                \n",
    "    denoised_ecg = pywt.waverec(coeff,wavefunc)\n",
    "    return denoised_ecg\n",
    "\n",
    "def wavelet_db6(sig):\n",
    "    \"\"\"\n",
    "    R J, Acharya U R, Min L C. ECG beat classification using PCA, LDA, ICA and discrete\n",
    "     wavelet transform[J].Biomedical Signal Processing and Control, 2013, 8(5): 437-448.\n",
    "\n",
    "    param sig: 1-D numpy Array\n",
    "    return: 1-D numpy Array\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = pywt.wavedec(sig, 'db6', level=9)\n",
    "\n",
    "    coeffs[-1] = np.zeros(len(coeffs[-1]))\n",
    "\n",
    "    coeffs[-2] = np.zeros(len(coeffs[-2]))\n",
    "\n",
    "    coeffs[0] = np.zeros(len(coeffs[0]))\n",
    "\n",
    "    sig_filt = pywt.waverec(coeffs, 'db6')\n",
    "\n",
    "    return sig_filt\n",
    "\n",
    "def read_test_data(data_path, split = \"Val\",preprocess=True):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    n_class = 2\n",
    "    n_steps = 3000 #2560\n",
    "\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    #print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12#len(channel_files)\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "    #X = np.zeros((len(channel_files), n_steps, n_channels))\n",
    "    \n",
    "    i_ch = 0\n",
    "    \n",
    "    data_x_w1 = []\n",
    "    data_x_w2 = []\n",
    "    data_x_w3 = []\n",
    "    \n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    \n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1','II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II','aVR','V2','V5']\n",
    "    channel_post_name = ['III','aVF','V3','V6']\n",
    "    \n",
    "    for i_ch,fil_ch in tqdm(enumerate(channel_files[:])):\n",
    "        #print(fil_ch)\n",
    "\n",
    "        ecg = sio.loadmat(os.path.join(path_signals,fil_ch))\n",
    "        \n",
    "        if True:#7 in labels_list[1:] or 4 in labels_list[1:]:\n",
    "            for i_filter in range(3):\n",
    "                \n",
    "                ecg_channels_w1 = np.zeros((n_steps, n_channels))\n",
    "                ecg_channels_w2 = np.zeros((n_steps, n_channels))\n",
    "                ecg_channels_w3 = np.zeros((n_steps, n_channels))\n",
    "                ecg_channels    = np.zeros((n_steps, n_channels))\n",
    "                \n",
    "                for i_n,ch_name in enumerate(channel_name[:]):\n",
    "\n",
    "                    # method 1\n",
    "                    '''  \n",
    "                    if ch_name in channel_mid_name:\n",
    "                        mid_ind = int(ecg[ch_name].T.shape[0]/2)\n",
    "                        ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[mid_ind-2500:mid_ind+2500],n_steps).T \n",
    "                    elif ch_name in channel_post_name:\n",
    "                        ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[-5000:],n_steps).T\n",
    "                    else:\n",
    "                        ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[:5000],n_steps).T\n",
    "                    '''\n",
    "                    #method 2\n",
    "                    ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T,n_steps).T\n",
    "                    \n",
    "                    #print(ecg_channels[:,i_n].shape)\n",
    "                    if preprocess and i_filter == 0:\n",
    "                        data = ecg_preprocessing(ecg_channels[:,i_n].reshape(1,n_steps),'sym8',8,3,n_steps/10)\n",
    "                        ecg_channels_w1[:,i_n] = data[0]#ecg['data']\n",
    "                    elif i_filter == 1:\n",
    "                        ecg_channels_w2[:,i_n] = wavelet(ecg_channels[:,i_n],'db4',4,2,4)[0]\n",
    "                    elif i_filter == 1:\n",
    "                        ecg_channels_w3[:,i_n] = wavelet_db6(ecg_channels[:,i_n].reshape(1,n_steps))[0]\n",
    "                    else:\n",
    "                        pass\n",
    "                        #ecg_channels[:,i_n] = ecg_channels[:,i_n]\n",
    "                #X[i_ch,:,:] = ecg_channels\n",
    "                data_x_w1.append(ecg_channels_w1)\n",
    "                data_x_w2.append(ecg_channels_w2)\n",
    "                data_x_w3.append(ecg_channels_w3)\n",
    "        else:\n",
    "            ecg_channels = np.zeros((n_steps, n_channels))\n",
    "            for i_n,ch_name in enumerate(channel_name[:]):\n",
    "\n",
    "                # method 1\n",
    "                '''\n",
    "                if ch_name in channel_mid_name:\n",
    "                    mid_ind = int(ecg[ch_name].T.shape[0]/2)\n",
    "                    ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[mid_ind-2500:mid_ind+2500],2560).T \n",
    "                elif ch_name in channel_post_name:\n",
    "                    ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[-5000:],2560).T\n",
    "                else:\n",
    "                    ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T[:5000],2560).T\n",
    "                '''\n",
    "                #method 2\n",
    "                ecg_channels[:,i_n] = signal.resample(ecg[ch_name].T,n_steps).T\n",
    "\n",
    "                if preprocess:\n",
    "                    data = ecg_preprocessing(ecg_channels[:,i_n].reshape(1,n_steps),'sym8',8,3,n_steps/10)\n",
    "                    ecg_channels[:,i_n] = data[0]#ecg['data']\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            #X[i_ch,:,:] = ecg_channels\n",
    "            data_x.append(ecg_channels)\n",
    "\n",
    "    # Return \n",
    "    return np.array(data_x_w1).astype('float32'),np.array(data_x_w2).astype('float32'),np.array(data_x_w3).astype('float32')#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_x_w1,test_x_w2,test_x_w3 = read_test_data(os.getcwd())\n",
    "#test_y_w1 = model.predict(test_x_w1)\n",
    "#test_y_w2 = model.predict(test_x_w2)\n",
    "#test_y_w3 = model.predict(test_x_w3)\n",
    "#test_y = test_y_w1+test_y_w2+test_y_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.63643658e-01, 4.57567513e-01, 4.29272652e-04, ...,\n",
       "        1.67076588e-02, 1.49106979e-03, 3.85248959e-02],\n",
       "       [2.24000216e-03, 1.06341064e-01, 2.53799558e-03, ...,\n",
       "        3.16327214e-02, 4.67002392e-05, 1.14878714e-02],\n",
       "       [3.58781219e-03, 9.06751096e-01, 2.02680826e-02, ...,\n",
       "        6.52618408e-02, 5.51044941e-04, 3.27953279e-01],\n",
       "       ...,\n",
       "       [5.66244125e-07, 3.39648128e-03, 4.09376621e-03, ...,\n",
       "        5.45324147e-01, 1.22922659e-03, 3.32450867e-03],\n",
       "       [2.06321955e-01, 1.28063560e-03, 5.16609073e-01, ...,\n",
       "        3.33055258e-02, 3.77246737e-03, 5.29712558e-01],\n",
       "       [2.18667775e-01, 2.04932988e-01, 1.55791640e-03, ...,\n",
       "        1.11970663e-01, 2.40185857e-03, 7.78462589e-02]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tr_y = model.predict(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05860588793922127"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = np.arange(0.1,0.9,0.1)\n",
    "\n",
    "out = x_tr_y\n",
    "y_test = y_tr\n",
    "\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(out.shape[1])\n",
    "for i in range(out.shape[1]):\n",
    "    y_prob = np.array(out[:,i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
    "        acc.append( matthews_corrcoef(y_test[:,i],y_pred))\n",
    "    acc   = np.array(acc)\n",
    "    index = np.where(acc==acc.max()) \n",
    "    accuracies.append(acc.max()) \n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "best_threshold\n",
    "\n",
    "y_pred = np.array([[1 if out[i,j]>=best_threshold[j] else 0 for j in range(y_test.shape[1])] for i in range(len(y_test))])\n",
    "\n",
    "y_pred \n",
    "\n",
    "y_test\n",
    "\n",
    "hamming_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.7, 0.6, 0.6, 0.2, 0.3, 0.4, 0.2, 0.6])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = out#np.array([[ 0.01491354,  0.00838183,  0.64092934,  0.99065214,  0.00579561]])\n",
    "y_pred = []\n",
    "classes = [0,1,2,3,4,5,6,7,8]\n",
    "for j in range(pred.shape[0]):\n",
    "    y_pred.append([1 if pred[j,i]>=best_threshold[i] else 0 for i in range(pred.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=[]\n",
    "for j in range(out.shape[0]):\n",
    "    pred.append([classes[i] for i in range(9) if y_pred[j][i] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('answers11111.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "            \n",
    "            result = pred[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-d4267a36c45f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mthred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'answers.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/JDWorkSpace/uuser/anaconda3/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "thred = 0.2\n",
    "\n",
    "pred = []\n",
    "for i in range(test_y.shape[0]):\n",
    "    pred.append( list(np.hstack(np.argwhere(test_y[i]>thred))))\n",
    "    \n",
    "with open('answers.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "            \n",
    "            result = pred[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "thred = 0.15#0.25\n",
    "\n",
    "pred = []\n",
    "for i in range(test_y.shape[0]):\n",
    "    if np.argmax(test_y[i]) == 0:\n",
    "        pred.append([0])\n",
    "        continue\n",
    "    try:\n",
    "        pred_list = list(np.hstack(np.argwhere(test_y[i]>thred)))\n",
    "    except ValueError:\n",
    "        print('valueError')\n",
    "        pred_list = ['']\n",
    "        \n",
    "    if 0 in pred_list:\n",
    "        if np.argmax(test_y[i]) != 0:\n",
    "            pred_list.remove(0)\n",
    "            \n",
    "    pred.append(pred_list)\n",
    "    \n",
    "with open('answers.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "            \n",
    "            result = pred[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
