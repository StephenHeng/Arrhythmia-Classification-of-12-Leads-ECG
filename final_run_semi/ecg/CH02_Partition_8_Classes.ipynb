{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index shape : (7703,)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from utils import extract_basic_features\n",
    "\n",
    "#import wfdb\n",
    "import os\n",
    "#import wfdb.processing as wp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "#from utils import find_noise_features, extract_basic_features\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import random as rn\n",
    "#from lightgbm import LGBMClassifier\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "import scipy.io as sio\n",
    "\n",
    "#from resnet_ecg.utils import one_hot,get_batches\n",
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler,EarlyStopping,ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "path = '/media/jdcloud/'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session )\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.conv_subsample_lengths = [1, 2, 1, 2, 1, 2, 1, 2]\n",
    "        self.conv_filter_length = 32\n",
    "        self.conv_num_filters_start = 12\n",
    "        self.conv_init = \"he_normal\"\n",
    "        self.conv_activation = \"relu\"\n",
    "        self.conv_dropout = 0.5\n",
    "        self.conv_num_skip = 2\n",
    "        self.conv_increase_channels_at = 2\n",
    "        self.batch_size = 32#128\n",
    "        self.input_shape = [2560, 12]#[1280, 1]\n",
    "        self.num_categories = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def lr_schedule(epoch):\n",
    "        lr = 0.1\n",
    "        if epoch >= 10 and epoch < 20:\n",
    "            lr = 0.01\n",
    "        if epoch >= 20:\n",
    "            lr = 0.001\n",
    "        print('Learning rate: ', lr)\n",
    "        return lr\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "\n",
    "def read_data_seg(data_path, split=\"Train\", preprocess=False, fs=500, newFs=256, winSecond=10, winNum=10, n_index=0):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    # n_index = 0\n",
    "    n_class = 9\n",
    "    winSize = winSecond * fs\n",
    "    new_winSize = winSecond * newFs\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    # label_path = os.path.join(data_path, \"reference.txt\")\n",
    "    # labels = pd.read_csv(label_path, sep='\\t',header = None)\n",
    "    # labels = pd.read_csv(\"reference.csv\")\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    # print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12  # len(channel_files)\n",
    "    # posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "\n",
    "    X = np.zeros((len(channel_files), new_winSize, n_channels)).astype('float32') \n",
    "    i_ch = 0\n",
    "\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1', 'II', 'aVR', 'V3', 'III', 'V5']\n",
    "    channel_mid_name = ['II', 'aVR', 'V2', 'V5']\n",
    "    channel_post_name = ['III', 'aVF', 'V3', 'V6']\n",
    "\n",
    "    for i_ch, fil_ch in enumerate(channel_files[:]):  # tqdm\n",
    "        \n",
    "        if i_ch % 2000 == 0:\n",
    "            print(i_ch)\n",
    "            \n",
    "        ecg = sio.loadmat(os.path.join(path_signals, fil_ch))\n",
    "        ecg_length = ecg[\"I\"].shape[1]\n",
    "\n",
    "        if ecg_length > fs * winNum * winSecond:\n",
    "            print(\" too long !!!\", ecg_length)\n",
    "            ecg_length = fs * winNum * winSecond\n",
    "        if ecg_length < 4500:\n",
    "            print(\" too short !!!\", ecg_length)\n",
    "            break\n",
    "\n",
    "        slide_steps = int((ecg_length - winSize) / winSecond)\n",
    "\n",
    "        if ecg_length <= 4500:\n",
    "            slide_steps = 0\n",
    "\n",
    "        ecg_channels = np.zeros((new_winSize, n_channels)).astype('float32') \n",
    "\n",
    "        for i_n, ch_name in enumerate(channel_name):\n",
    "\n",
    "            ecg_channels[:, i_n] = signal.resample(ecg[ch_name]\n",
    "                                                   [:, n_index * slide_steps:n_index * slide_steps + winSize].T\n",
    "                                                   , new_winSize).T\n",
    "            if preprocess:\n",
    "                data = ecg_preprocessing(ecg_channels[:, i_n].reshape(1, new_winSize), 'sym8', 8, 3, newFs)\n",
    "                ecg_channels[:, i_n] = data[0]\n",
    "            else:\n",
    "                pass\n",
    "                ecg_channels[:, i_n] = ecg_channels[:, i_n]\n",
    "\n",
    "        X[i_ch, :, :] = ecg_channels\n",
    "\n",
    "    return X\n",
    "\n",
    "def read_train_data(path):\n",
    "\n",
    "    ecg12_seg0 = read_data_seg(path, n_index=0)\n",
    "    ecg12_seg1 = read_data_seg(path, n_index=1)\n",
    "    ecg12_seg2 = read_data_seg(path, n_index=2)\n",
    "    ecg12_seg3 = read_data_seg(path, n_index=3)\n",
    "    ecg12_seg4 = read_data_seg(path, n_index=4)\n",
    "\n",
    "    ecg12_seg5 = read_data_seg(path, n_index=5)\n",
    "    ecg12_seg6 = read_data_seg(path, n_index=6)\n",
    "    ecg12_seg7 = read_data_seg(path, n_index=7)\n",
    "    ecg12_seg8 = read_data_seg(path, n_index=8)\n",
    "    ecg12_seg9 = read_data_seg(path, n_index=9)\n",
    "\n",
    "    X = [ecg12_seg0, ecg12_seg1, ecg12_seg2, ecg12_seg3,\n",
    "         ecg12_seg4, ecg12_seg5, ecg12_seg6, ecg12_seg7,\n",
    "         ecg12_seg8, ecg12_seg9,\n",
    "           ]\n",
    "\n",
    "    del ecg12_seg0, ecg12_seg1, ecg12_seg2, ecg12_seg3, ecg12_seg4\n",
    "    del ecg12_seg5, ecg12_seg6, ecg12_seg7, ecg12_seg8, ecg12_seg9\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return X\n",
    "\n",
    "def read_test_data(path):\n",
    "\n",
    "    test_x_seg0 = read_data_seg(path, split='Val', n_index=0)\n",
    "    test_x_seg1 = read_data_seg(path, split='Val', n_index=1)\n",
    "    test_x_seg2 = read_data_seg(path, split='Val', n_index=2)\n",
    "    test_x_seg3 = read_data_seg(path, split='Val', n_index=3)\n",
    "    test_x_seg4 = read_data_seg(path, split='Val', n_index=4)\n",
    "\n",
    "    test_x_seg5 = read_data_seg(path, split='Val', n_index=5)\n",
    "    test_x_seg6 = read_data_seg(path, split='Val', n_index=6)\n",
    "    test_x_seg7 = read_data_seg(path, split='Val', n_index=7)\n",
    "    test_x_seg8 = read_data_seg(path, split='Val', n_index=8)\n",
    "    test_x_seg9 = read_data_seg(path, split='Val', n_index=9)\n",
    "\n",
    "    test_x = [test_x_seg0, test_x_seg1, test_x_seg2, test_x_seg3, test_x_seg4,\n",
    "              test_x_seg5, test_x_seg6, test_x_seg7, test_x_seg8, test_x_seg9,\n",
    "             ]\n",
    "\n",
    "    del test_x_seg0, test_x_seg1, test_x_seg2, test_x_seg3, test_x_seg4\n",
    "    del test_x_seg5, test_x_seg6, test_x_seg7, test_x_seg8, test_x_seg9\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return test_x\n",
    "\n",
    "def preprocess_y(labels,y,num_class=9):\n",
    "    bin_label = np.zeros((len(y),num_class)).astype('int8') \n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1,label_nona.shape[0]):\n",
    "            bin_label[i,int(label_nona[j])]=1\n",
    "    return bin_label\n",
    "\n",
    "\n",
    "def add_compile(model, config):\n",
    "    optimizer = SGD(lr=config.lr_schedule(0), momentum=0.9)  # Adam()#\n",
    "    model.compile(loss='binary_crossentropy',  # weighted_loss,#'binary_crossentropy',\n",
    "                  optimizer='adam',  # optimizer,#'adam',\n",
    "                  metrics=['accuracy', fmeasure, precision])#recall\n",
    "    # ['accuracy',fbetaMacro,recallMacro,precisionMacro])\n",
    "    # ['accuracy',fmeasure,recall,precision])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_dataset_path = path + \"/Train/\"\n",
    "    val_dataset_path = path + \"/Val/\"\n",
    "\n",
    "    train_files = os.listdir(train_dataset_path)\n",
    "    train_files.sort()\n",
    "    val_files = os.listdir(val_dataset_path)\n",
    "    val_files.sort()\n",
    "\n",
    "    labels = pd.read_csv(path+\"reference.csv\")\n",
    "\n",
    "    #print(labels.head())\n",
    "\n",
    "    bin_label = np.zeros((6500,9))\n",
    "    for i in range(labels.shape[0]):\n",
    "        label_nona = labels.loc[i].dropna()\n",
    "        for j in range(1,label_nona.shape[0]):\n",
    "            bin_label[i,int(label_nona[j])]=1\n",
    "\n",
    "    cv_pred_all = 0\n",
    "    en_amount = 1\n",
    "\n",
    "    labels_en = pd.read_csv(path + \"kfold_labels_en.csv\")\n",
    "    #print(labels_en.shape)\n",
    "    #print(labels_en.head())\n",
    "\n",
    "    data_info = pd.read_csv(path + \"data_info.csv\")\n",
    "    #print(data_info.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index shape : (7703,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_index = np.arange(6500).astype('int16')\n",
    "\n",
    "label2_list = data_info[data_info.labels_num == 2].index.tolist()\n",
    "label3_list = data_info[data_info.labels_num == 3].index.tolist()\n",
    "label4_list = data_info[data_info.labels_num == 4].index.tolist()\n",
    "label5_list = data_info[data_info.labels_num == 5].index.tolist()\n",
    "label6_list = data_info[data_info.labels_num == 6].index.tolist()\n",
    "\n",
    "train_index = np.insert(train_index, label2_list, label2_list)  # [145:155]\n",
    "\n",
    "train_index = np.insert(train_index, label3_list, label3_list)\n",
    "train_index = np.insert(train_index, label3_list, label3_list)\n",
    "\n",
    "train_index = np.insert(train_index, label4_list, label4_list)\n",
    "train_index = np.insert(train_index, label4_list, label4_list)\n",
    "train_index = np.insert(train_index, label4_list, label4_list)\n",
    "\n",
    "train_index = np.insert(train_index, label5_list, label5_list)\n",
    "train_index = np.insert(train_index, label5_list, label5_list)\n",
    "train_index = np.insert(train_index, label5_list, label5_list)\n",
    "train_index = np.insert(train_index, label5_list, label5_list)\n",
    "\n",
    "train_index = np.insert(train_index, label6_list, label6_list)\n",
    "train_index = np.insert(train_index, label6_list, label6_list)\n",
    "train_index = np.insert(train_index, label6_list, label6_list)\n",
    "train_index = np.insert(train_index, label6_list, label6_list)\n",
    "train_index = np.insert(train_index, label6_list, label6_list)\n",
    "\n",
    "#print(train_index.dtype)\n",
    "\n",
    "train_index = train_index.astype(np.int16)\n",
    "\n",
    "train_index.sort()\n",
    "\n",
    "print(\"train_index shape :\",train_index.shape)\n",
    "#print(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 6497, 6498, 6499], dtype=int16)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_index = labels_en[labels_en[\"label1\"]!=0.0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_index = train_index[label_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 6413, 6432, 6487], dtype=int16)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index shape : (7703,)\n"
     ]
    }
   ],
   "source": [
    "train_index = train_index.astype(np.int16)\n",
    "train_index.sort()\n",
    "\n",
    "print(\"train_index shape :\",train_index.shape)\n",
    "#print(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg12_seg0 = read_data_seg(path, n_index=0) \n",
    "ecg12_seg1 = read_data_seg(path, n_index=1) \n",
    "ecg12_seg2 = read_data_seg(path, n_index=2) \n",
    "ecg12_seg3 = read_data_seg(path, n_index=3) \n",
    "ecg12_seg4 = read_data_seg(path, n_index=4) \n",
    "\n",
    "ecg12_seg5 = read_data_seg(path, n_index=5)\n",
    "ecg12_seg6 = read_data_seg(path, n_index=6)\n",
    "ecg12_seg7 = read_data_seg(path, n_index=7)\n",
    "ecg12_seg8 = read_data_seg(path, n_index=8)\n",
    "ecg12_seg9 = read_data_seg(path, n_index=9)\n",
    "#train_x = np.array(read_train_data(path),dtype=np.float32)\n",
    "#test_x = read_test_data(path)\n",
    "\n",
    "test_x_seg0 = read_data_seg(path, split='Val', n_index=0)\n",
    "test_x_seg1 = read_data_seg(path, split='Val', n_index=1)\n",
    "test_x_seg2 = read_data_seg(path, split='Val', n_index=2)\n",
    "test_x_seg3 = read_data_seg(path, split='Val', n_index=3)\n",
    "test_x_seg4 = read_data_seg(path, split='Val', n_index=4)\n",
    "\n",
    "test_x_seg5 = read_data_seg(path, split='Val', n_index=5)\n",
    "test_x_seg6 = read_data_seg(path, split='Val', n_index=6)\n",
    "test_x_seg7 = read_data_seg(path, split='Val', n_index=7)\n",
    "test_x_seg8 = read_data_seg(path, split='Val', n_index=8)\n",
    "test_x_seg9 = read_data_seg(path, split='Val', n_index=9)\n",
    "\n",
    "test_x = [test_x_seg0, test_x_seg1, test_x_seg2, test_x_seg3, test_x_seg4,\n",
    "          test_x_seg5, test_x_seg6, test_x_seg7, test_x_seg8, test_x_seg9,\n",
    "         ]\n",
    "\n",
    "del test_x_seg0, test_x_seg1, test_x_seg2, test_x_seg3, test_x_seg4\n",
    "del test_x_seg5, test_x_seg6, test_x_seg7, test_x_seg8, test_x_seg9\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/uuser/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 2560, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2560, 12)     444         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 2560, 12)     444         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 2560, 12)     444         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 2560, 12)     444         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 2560, 12)     444         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, 2560, 12)     444         input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, 2560, 12)     444         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_106 (Conv1D)             (None, 2560, 12)     444         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, 2560, 12)     444         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_136 (Conv1D)             (None, 2560, 12)     444         input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 2560, 12)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 2560, 12)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, 2560, 12)     0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, 2560, 12)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, 2560, 12)     0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)      (None, 2560, 12)     0           conv1d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_103 (LeakyReLU)     (None, 2560, 12)     0           conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_120 (LeakyReLU)     (None, 2560, 12)     0           conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_137 (LeakyReLU)     (None, 2560, 12)     0           conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_154 (LeakyReLU)     (None, 2560, 12)     0           conv1d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2560, 12)     444         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, 2560, 12)     444         leaky_re_lu_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_107 (Conv1D)             (None, 2560, 12)     444         leaky_re_lu_120[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, 2560, 12)     444         leaky_re_lu_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_137 (Conv1D)             (None, 2560, 12)     444         leaky_re_lu_154[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 2560, 12)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 2560, 12)     0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, 2560, 12)     0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, 2560, 12)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, 2560, 12)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)      (None, 2560, 12)     0           conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_104 (LeakyReLU)     (None, 2560, 12)     0           conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_121 (LeakyReLU)     (None, 2560, 12)     0           conv1d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_138 (LeakyReLU)     (None, 2560, 12)     0           conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_155 (LeakyReLU)     (None, 2560, 12)     0           conv1d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1280, 12)     3468        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, 1280, 12)     3468        leaky_re_lu_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, 1280, 12)     3468        leaky_re_lu_121[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, 1280, 12)     3468        leaky_re_lu_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_138 (Conv1D)             (None, 1280, 12)     3468        leaky_re_lu_155[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 1280, 12)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 1280, 12)     0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, 1280, 12)     0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 1280, 12)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 1280, 12)     0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)      (None, 1280, 12)     0           conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_105 (LeakyReLU)     (None, 1280, 12)     0           conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_122 (LeakyReLU)     (None, 1280, 12)     0           conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_139 (LeakyReLU)     (None, 1280, 12)     0           conv1d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_156 (LeakyReLU)     (None, 1280, 12)     0           conv1d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1280, 12)     0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1280, 12)     0           leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_122[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_139[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 1280, 12)     0           leaky_re_lu_156[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1280, 12)     444         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1280, 12)     444         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 1280, 12)     444         dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 1280, 12)     444         dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 1280, 12)     444         dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, 1280, 12)     444         dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 1280, 12)     444         dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, 1280, 12)     444         dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, 1280, 12)     444         dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_139 (Conv1D)             (None, 1280, 12)     444         dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 1280, 12)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 1280, 12)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, 1280, 12)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 1280, 12)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, 1280, 12)     0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_89 (LeakyReLU)      (None, 1280, 12)     0           conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_106 (LeakyReLU)     (None, 1280, 12)     0           conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_123 (LeakyReLU)     (None, 1280, 12)     0           conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_140 (LeakyReLU)     (None, 1280, 12)     0           conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_157 (LeakyReLU)     (None, 1280, 12)     0           conv1d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1280, 12)     444         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, 1280, 12)     444         leaky_re_lu_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, 1280, 12)     444         leaky_re_lu_123[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, 1280, 12)     444         leaky_re_lu_140[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_140 (Conv1D)             (None, 1280, 12)     444         leaky_re_lu_157[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1280, 12)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 1280, 12)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, 1280, 12)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 1280, 12)     0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)      (None, 1280, 12)     0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_90 (LeakyReLU)      (None, 1280, 12)     0           conv1d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_107 (LeakyReLU)     (None, 1280, 12)     0           conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_124 (LeakyReLU)     (None, 1280, 12)     0           conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_141 (LeakyReLU)     (None, 1280, 12)     0           conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_158 (LeakyReLU)     (None, 1280, 12)     0           conv1d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 640, 12)      3468        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 640, 12)      3468        leaky_re_lu_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, 640, 12)      3468        leaky_re_lu_124[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, 640, 12)      3468        leaky_re_lu_141[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_141 (Conv1D)             (None, 640, 12)      3468        leaky_re_lu_158[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 640, 12)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 640, 12)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 640, 12)      0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 640, 12)      0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)      (None, 640, 12)      0           conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_91 (LeakyReLU)      (None, 640, 12)      0           conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_108 (LeakyReLU)     (None, 640, 12)      0           conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_125 (LeakyReLU)     (None, 640, 12)      0           conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_142 (LeakyReLU)     (None, 640, 12)      0           conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_159 (LeakyReLU)     (None, 640, 12)      0           conv1d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 640, 12)      0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 640, 12)      0           leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 640, 12)      0           leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 640, 12)      0           leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 640, 12)      0           leaky_re_lu_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 640, 12)      0           leaky_re_lu_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 640, 12)      0           leaky_re_lu_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 640, 12)      0           leaky_re_lu_125[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 640, 12)      0           leaky_re_lu_142[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 640, 12)      0           leaky_re_lu_159[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 640, 12)      444         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 640, 12)      444         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 640, 12)      444         dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 640, 12)      444         dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 640, 12)      444         dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 640, 12)      444         dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 640, 12)      444         dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, 640, 12)      444         dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_127 (Conv1D)             (None, 640, 12)      444         dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_142 (Conv1D)             (None, 640, 12)      444         dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 640, 12)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 640, 12)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 640, 12)      0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, 640, 12)      0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)      (None, 640, 12)      0           conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_92 (LeakyReLU)      (None, 640, 12)      0           conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_109 (LeakyReLU)     (None, 640, 12)      0           conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_126 (LeakyReLU)     (None, 640, 12)      0           conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_143 (LeakyReLU)     (None, 640, 12)      0           conv1d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_160 (LeakyReLU)     (None, 640, 12)      0           conv1d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 640, 12)      444         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 640, 12)      444         leaky_re_lu_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, 640, 12)      444         leaky_re_lu_126[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_128 (Conv1D)             (None, 640, 12)      444         leaky_re_lu_143[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_143 (Conv1D)             (None, 640, 12)      444         leaky_re_lu_160[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 640, 12)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 640, 12)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 640, 12)      0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, 640, 12)      0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)      (None, 640, 12)      0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_93 (LeakyReLU)      (None, 640, 12)      0           conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_110 (LeakyReLU)     (None, 640, 12)      0           conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_127 (LeakyReLU)     (None, 640, 12)      0           conv1d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_144 (LeakyReLU)     (None, 640, 12)      0           conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_161 (LeakyReLU)     (None, 640, 12)      0           conv1d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 320, 12)      3468        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 320, 12)      3468        leaky_re_lu_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, 320, 12)      3468        leaky_re_lu_127[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_129 (Conv1D)             (None, 320, 12)      3468        leaky_re_lu_144[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_144 (Conv1D)             (None, 320, 12)      3468        leaky_re_lu_161[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 320, 12)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 320, 12)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 320, 12)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, 320, 12)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)      (None, 320, 12)      0           conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_94 (LeakyReLU)      (None, 320, 12)      0           conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_111 (LeakyReLU)     (None, 320, 12)      0           conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_128 (LeakyReLU)     (None, 320, 12)      0           conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_145 (LeakyReLU)     (None, 320, 12)      0           conv1d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_162 (LeakyReLU)     (None, 320, 12)      0           conv1d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 320, 12)      0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 320, 12)      0           leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 320, 12)      0           leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 320, 12)      0           leaky_re_lu_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 320, 12)      0           leaky_re_lu_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 320, 12)      0           leaky_re_lu_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 320, 12)      0           leaky_re_lu_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 320, 12)      0           leaky_re_lu_128[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 320, 12)      0           leaky_re_lu_145[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 320, 12)      0           leaky_re_lu_162[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 320, 12)      444         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 320, 12)      444         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 320, 12)      444         dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 320, 12)      444         dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, 320, 12)      444         dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 320, 12)      444         dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 320, 12)      444         dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, 320, 12)      444         dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_130 (Conv1D)             (None, 320, 12)      444         dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, 320, 12)      444         dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 320, 12)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 320, 12)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 320, 12)      0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 320, 12)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)      (None, 320, 12)      0           conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_95 (LeakyReLU)      (None, 320, 12)      0           conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_112 (LeakyReLU)     (None, 320, 12)      0           conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_129 (LeakyReLU)     (None, 320, 12)      0           conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_146 (LeakyReLU)     (None, 320, 12)      0           conv1d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_163 (LeakyReLU)     (None, 320, 12)      0           conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, 320, 12)      444         leaky_re_lu_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, 320, 12)      444         leaky_re_lu_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, 320, 12)      444         leaky_re_lu_129[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_131 (Conv1D)             (None, 320, 12)      444         leaky_re_lu_146[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, 320, 12)      444         leaky_re_lu_163[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 320, 12)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 320, 12)      0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 320, 12)      0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 320, 12)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)      (None, 320, 12)      0           conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_96 (LeakyReLU)      (None, 320, 12)      0           conv1d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_113 (LeakyReLU)     (None, 320, 12)      0           conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_130 (LeakyReLU)     (None, 320, 12)      0           conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_147 (LeakyReLU)     (None, 320, 12)      0           conv1d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_164 (LeakyReLU)     (None, 320, 12)      0           conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, 160, 12)      3468        leaky_re_lu_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, 160, 12)      3468        leaky_re_lu_113[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, 160, 12)      3468        leaky_re_lu_130[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_132 (Conv1D)             (None, 160, 12)      3468        leaky_re_lu_147[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_147 (Conv1D)             (None, 160, 12)      3468        leaky_re_lu_164[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 160, 12)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 160, 12)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 160, 12)      0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 160, 12)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)      (None, 160, 12)      0           conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_97 (LeakyReLU)      (None, 160, 12)      0           conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_114 (LeakyReLU)     (None, 160, 12)      0           conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_131 (LeakyReLU)     (None, 160, 12)      0           conv1d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_148 (LeakyReLU)     (None, 160, 12)      0           conv1d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_165 (LeakyReLU)     (None, 160, 12)      0           conv1d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 160, 12)      0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 160, 12)      0           leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 160, 12)      0           leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 160, 12)      0           leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 160, 12)      0           leaky_re_lu_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 160, 12)      0           leaky_re_lu_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 160, 12)      0           leaky_re_lu_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 160, 12)      0           leaky_re_lu_131[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 160, 12)      0           leaky_re_lu_148[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 160, 12)      0           leaky_re_lu_165[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 160, 12)      444         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 160, 12)      444         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 160, 12)      444         dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 160, 12)      444         dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_73 (Conv1D)              (None, 160, 12)      444         dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, 160, 12)      444         dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, 160, 12)      444         dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, 160, 12)      444         dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_133 (Conv1D)             (None, 160, 12)      444         dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_148 (Conv1D)             (None, 160, 12)      444         dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 160, 12)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 160, 12)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, 160, 12)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 160, 12)      0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_81 (LeakyReLU)      (None, 160, 12)      0           conv1d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_98 (LeakyReLU)      (None, 160, 12)      0           conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_115 (LeakyReLU)     (None, 160, 12)      0           conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_132 (LeakyReLU)     (None, 160, 12)      0           conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_149 (LeakyReLU)     (None, 160, 12)      0           conv1d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_166 (LeakyReLU)     (None, 160, 12)      0           conv1d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_74 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, 160, 12)      444         leaky_re_lu_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_104 (Conv1D)             (None, 160, 12)      444         leaky_re_lu_115[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, 160, 12)      444         leaky_re_lu_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_134 (Conv1D)             (None, 160, 12)      444         leaky_re_lu_149[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_149 (Conv1D)             (None, 160, 12)      444         leaky_re_lu_166[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 160, 12)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 160, 12)      0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, 160, 12)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 160, 12)      0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_82 (LeakyReLU)      (None, 160, 12)      0           conv1d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_99 (LeakyReLU)      (None, 160, 12)      0           conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_116 (LeakyReLU)     (None, 160, 12)      0           conv1d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_133 (LeakyReLU)     (None, 160, 12)      0           conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_150 (LeakyReLU)     (None, 160, 12)      0           conv1d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_167 (LeakyReLU)     (None, 160, 12)      0           conv1d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, 80, 12)       6924        leaky_re_lu_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, 80, 12)       6924        leaky_re_lu_116[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, 80, 12)       6924        leaky_re_lu_133[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_135 (Conv1D)             (None, 80, 12)       6924        leaky_re_lu_150[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_150 (Conv1D)             (None, 80, 12)       6924        leaky_re_lu_167[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 80, 12)       0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 80, 12)       0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, 80, 12)       0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, 80, 12)       0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_83 (LeakyReLU)      (None, 80, 12)       0           conv1d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_100 (LeakyReLU)     (None, 80, 12)       0           conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_117 (LeakyReLU)     (None, 80, 12)       0           conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_134 (LeakyReLU)     (None, 80, 12)       0           conv1d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_151 (LeakyReLU)     (None, 80, 12)       0           conv1d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_168 (LeakyReLU)     (None, 80, 12)       0           conv1d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 80, 12)       0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 80, 12)       0           leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 80, 12)       0           leaky_re_lu_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 80, 12)       0           leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 80, 12)       0           leaky_re_lu_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 80, 12)       0           leaky_re_lu_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 80, 12)       0           leaky_re_lu_117[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 80, 12)       0           leaky_re_lu_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 80, 12)       0           leaky_re_lu_151[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 80, 12)       0           leaky_re_lu_168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 80, 24)       2496        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 80, 24)       2496        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 80, 24)       2496        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 80, 24)       2496        dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 80, 24)       2496        dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 80, 24)       2496        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 80, 24)       2496        dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 80, 24)       2496        dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 80, 24)       2496        dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 80, 24)       2496        dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 80, 24)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 80, 24)       0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, 80, 24)       0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, 80, 24)       0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_84 (LeakyReLU)      (None, 80, 24)       0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_101 (LeakyReLU)     (None, 80, 24)       0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_118 (LeakyReLU)     (None, 80, 24)       0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_135 (LeakyReLU)     (None, 80, 24)       0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_152 (LeakyReLU)     (None, 80, 24)       0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_169 (LeakyReLU)     (None, 80, 24)       0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 80, 24)       0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 80, 24)       0           leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 80, 24)       0           leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 80, 24)       0           leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 80, 24)       0           leaky_re_lu_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 80, 24)       0           leaky_re_lu_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 80, 24)       0           leaky_re_lu_118[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 80, 24)       0           leaky_re_lu_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 80, 24)       0           leaky_re_lu_152[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 80, 24)       0           leaky_re_lu_169[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_1 (Atten (None, 24)           624         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_2 (Atten (None, 24)           624         dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_3 (Atten (None, 24)           624         dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_4 (Atten (None, 24)           624         dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_5 (Atten (None, 24)           624         dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_6 (Atten (None, 24)           624         dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_7 (Atten (None, 24)           624         dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_8 (Atten (None, 24)           624         dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_9 (Atten (None, 24)           624         dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_10 (Atte (None, 24)           624         dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 24)        0           attention_with_context_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 24)        0           attention_with_context_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 24)        0           attention_with_context_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 24)        0           attention_with_context_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 24)        0           attention_with_context_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 24)        0           attention_with_context_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 24)        0           attention_with_context_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 24)        0           attention_with_context_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 24)        0           attention_with_context_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 24)        0           attention_with_context_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 24)        96          reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 24)        96          reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 24)        96          reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 24)        96          reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 24)        96          reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 24)        96          reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 1, 24)        96          reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1, 24)        96          reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 24)        96          reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1, 24)        96          reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)      (None, 1, 24)        0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_102 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_119 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_136 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_153 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_170 (LeakyReLU)     (None, 1, 24)        0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1, 24)        0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1, 24)        0           leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 1, 24)        0           leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 1, 24)        0           leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 1, 24)        0           leaky_re_lu_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 1, 24)        0           leaky_re_lu_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 1, 24)        0           leaky_re_lu_119[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 1, 24)        0           leaky_re_lu_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 1, 24)        0           leaky_re_lu_153[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 1, 24)        0           leaky_re_lu_170[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 24)       0           dropout_7[0][0]                  \n",
      "                                                                 dropout_14[0][0]                 \n",
      "                                                                 dropout_21[0][0]                 \n",
      "                                                                 dropout_28[0][0]                 \n",
      "                                                                 dropout_35[0][0]                 \n",
      "                                                                 dropout_42[0][0]                 \n",
      "                                                                 dropout_49[0][0]                 \n",
      "                                                                 dropout_56[0][0]                 \n",
      "                                                                 dropout_63[0][0]                 \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 10, 24)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 240)          0           dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            1928        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 286,448\n",
      "Trainable params: 285,968\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from resnet_ecg import attentionmodel_ceight\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model,load_model\n",
    "\n",
    "'''   '''\n",
    "inputs0 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs1 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs2 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs3 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs4 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs5 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs6 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs7 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs8 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "inputs9 = Input(shape=(2560,12),dtype=\"float32\")\n",
    "\n",
    "inputs_list = [inputs0,inputs1,inputs2,inputs3,inputs4,inputs5,inputs6,inputs7,inputs8,inputs9]\n",
    "\n",
    "outputs = attentionmodel_ceight.build_network(inputs_list,0.5,num_classes=8,block_size=4,relu=False)\n",
    "\n",
    "model = Model(inputs =inputs_list,outputs=outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label = labels_en['label1'].values\n",
    "train_label = train_label[label_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label = train_label-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 7., 7., ..., 7., 7., 7.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5750,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 10, 11, 12, 12, 12, 13,\n",
       "       14, 15, 16], dtype=int16)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5684, 5732, 5737, 5765, 5781, 5803, 5811, 5847, 6006, 6058, 6167,\n",
       "       6308, 6316, 6344, 6367, 6385, 6408, 6413, 6432, 6487], dtype=int16)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "fold:  1  training\n",
      "[   0    3    5 ... 5744 5747 5749]\n",
      "[   0    3    5 ... 6367 6413 6487]\n",
      "2875\n",
      "(2875, 8)\n",
      "fold:  2  training\n",
      "[   1    2    4 ... 5745 5746 5748]\n",
      "[   1    2    4 ... 6385 6408 6432]\n",
      "2875\n",
      "(2875, 8)\n"
     ]
    }
   ],
   "source": [
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 2\n",
    "    n_classes = 2\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "    kf = kfold.split(train_index, train_label)\n",
    "\n",
    "    blend_train = np.zeros((6500, n_classes)).astype('float32') #len(train_x)\n",
    "    blend_test = np.zeros((500, n_fold, n_classes)).astype('float32') #len(test_x)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i+1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        index_tr = train_index[index_train]\n",
    "        index_vld = train_index[index_valid]\n",
    "        \n",
    "        print(index_train)\n",
    "        print(index_tr)\n",
    "        print(len(index_train))\n",
    "        \n",
    "        y_tr = keras.utils.to_categorical(labels_en['label1'].values[index_tr]-1 ,num_classes=8) #preprocess_y(labels,index_tr)\n",
    "        y_vld = keras.utils.to_categorical(labels_en['label1'].values[index_vld]-1 ,num_classes=8)   #preprocess_y(labels,index_vld)\n",
    "\n",
    "        print(y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "fold:  1  training\n",
      "Learning rate:  0.1\n",
      "Train on 3832 samples, validate on 1918 samples\n",
      "Epoch 1/20\n",
      "3832/3832 [==============================] - 63s 16ms/step - loss: 0.1678 - acc: 0.9373 - fmeasure: 0.7229 - precision: 0.7942 - val_loss: 0.1097 - val_acc: 0.9567 - val_fmeasure: 0.8180 - val_precision: 0.8530\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.81803, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 2/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1418 - acc: 0.9457 - fmeasure: 0.7662 - precision: 0.8225 - val_loss: 0.1076 - val_acc: 0.9589 - val_fmeasure: 0.8208 - val_precision: 0.8835\n",
      "\n",
      "Epoch 00002: val_fmeasure improved from 0.81803 to 0.82076, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 3/20\n",
      "3832/3832 [==============================] - 22s 6ms/step - loss: 0.1546 - acc: 0.9436 - fmeasure: 0.7507 - precision: 0.8239 - val_loss: 0.1464 - val_acc: 0.9441 - val_fmeasure: 0.7355 - val_precision: 0.8670\n",
      "\n",
      "Epoch 00003: val_fmeasure did not improve from 0.82076\n",
      "Epoch 4/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1505 - acc: 0.9439 - fmeasure: 0.7620 - precision: 0.8080 - val_loss: 0.1208 - val_acc: 0.9553 - val_fmeasure: 0.7986 - val_precision: 0.8893\n",
      "\n",
      "Epoch 00004: val_fmeasure did not improve from 0.82076\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1573 - acc: 0.9416 - fmeasure: 0.7436 - precision: 0.8061 - val_loss: 0.1032 - val_acc: 0.9600 - val_fmeasure: 0.8299 - val_precision: 0.8725\n",
      "\n",
      "Epoch 00005: val_fmeasure improved from 0.82076 to 0.82992, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 6/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1387 - acc: 0.9495 - fmeasure: 0.7746 - precision: 0.8281 - val_loss: 0.1051 - val_acc: 0.9621 - val_fmeasure: 0.8389 - val_precision: 0.8831\n",
      "\n",
      "Epoch 00006: val_fmeasure improved from 0.82992 to 0.83887, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 7/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1264 - acc: 0.9520 - fmeasure: 0.7950 - precision: 0.8449 - val_loss: 0.1093 - val_acc: 0.9593 - val_fmeasure: 0.8267 - val_precision: 0.8679\n",
      "\n",
      "Epoch 00007: val_fmeasure did not improve from 0.83887\n",
      "Epoch 8/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1366 - acc: 0.9491 - fmeasure: 0.7848 - precision: 0.8265 - val_loss: 0.1006 - val_acc: 0.9623 - val_fmeasure: 0.8390 - val_precision: 0.8910\n",
      "\n",
      "Epoch 00008: val_fmeasure improved from 0.83887 to 0.83904, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 9/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1296 - acc: 0.9538 - fmeasure: 0.7972 - precision: 0.8385 - val_loss: 0.0973 - val_acc: 0.9643 - val_fmeasure: 0.8453 - val_precision: 0.9062\n",
      "\n",
      "Epoch 00009: val_fmeasure improved from 0.83904 to 0.84531, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 10/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1265 - acc: 0.9531 - fmeasure: 0.8012 - precision: 0.8467 - val_loss: 0.1082 - val_acc: 0.9580 - val_fmeasure: 0.8213 - val_precision: 0.8639\n",
      "\n",
      "Epoch 00010: val_fmeasure did not improve from 0.84531\n",
      "Epoch 11/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1203 - acc: 0.9560 - fmeasure: 0.8149 - precision: 0.8607 - val_loss: 0.0927 - val_acc: 0.9655 - val_fmeasure: 0.8560 - val_precision: 0.8829\n",
      "\n",
      "Epoch 00011: val_fmeasure improved from 0.84531 to 0.85602, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 12/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1173 - acc: 0.9569 - fmeasure: 0.8194 - precision: 0.8562 - val_loss: 0.1021 - val_acc: 0.9620 - val_fmeasure: 0.8401 - val_precision: 0.8759\n",
      "\n",
      "Epoch 00012: val_fmeasure did not improve from 0.85602\n",
      "Epoch 13/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1364 - acc: 0.9494 - fmeasure: 0.7805 - precision: 0.8316 - val_loss: 0.1026 - val_acc: 0.9620 - val_fmeasure: 0.8384 - val_precision: 0.8858\n",
      "\n",
      "Epoch 00013: val_fmeasure did not improve from 0.85602\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1233 - acc: 0.9554 - fmeasure: 0.8110 - precision: 0.8548 - val_loss: 0.0946 - val_acc: 0.9657 - val_fmeasure: 0.8550 - val_precision: 0.8932\n",
      "\n",
      "Epoch 00014: val_fmeasure did not improve from 0.85602\n",
      "Epoch 15/20\n",
      "3832/3832 [==============================] - 20s 5ms/step - loss: 0.1143 - acc: 0.9576 - fmeasure: 0.8207 - precision: 0.8588 - val_loss: 0.0919 - val_acc: 0.9664 - val_fmeasure: 0.8582 - val_precision: 0.8969\n",
      "\n",
      "Epoch 00015: val_fmeasure improved from 0.85602 to 0.85818, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 16/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1215 - acc: 0.9544 - fmeasure: 0.8062 - precision: 0.8502 - val_loss: 0.0897 - val_acc: 0.9667 - val_fmeasure: 0.8602 - val_precision: 0.8958\n",
      "\n",
      "Epoch 00016: val_fmeasure improved from 0.85818 to 0.86019, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 17/20\n",
      "3832/3832 [==============================] - 20s 5ms/step - loss: 0.1107 - acc: 0.9588 - fmeasure: 0.8273 - precision: 0.8665 - val_loss: 0.0906 - val_acc: 0.9669 - val_fmeasure: 0.8622 - val_precision: 0.8896\n",
      "\n",
      "Epoch 00017: val_fmeasure improved from 0.86019 to 0.86222, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 18/20\n",
      "3832/3832 [==============================] - 20s 5ms/step - loss: 0.1265 - acc: 0.9539 - fmeasure: 0.8035 - precision: 0.8416 - val_loss: 0.0909 - val_acc: 0.9681 - val_fmeasure: 0.8650 - val_precision: 0.9046\n",
      "\n",
      "Epoch 00018: val_fmeasure improved from 0.86222 to 0.86497, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r0.hdf5\n",
      "Epoch 19/20\n",
      "3832/3832 [==============================] - 20s 5ms/step - loss: 0.1164 - acc: 0.9567 - fmeasure: 0.8193 - precision: 0.8537 - val_loss: 0.0893 - val_acc: 0.9671 - val_fmeasure: 0.8620 - val_precision: 0.8969\n",
      "\n",
      "Epoch 00019: val_fmeasure did not improve from 0.86497\n",
      "Epoch 20/20\n",
      "3832/3832 [==============================] - 19s 5ms/step - loss: 0.1152 - acc: 0.9578 - fmeasure: 0.8218 - precision: 0.8621 - val_loss: 0.0897 - val_acc: 0.9676 - val_fmeasure: 0.8642 - val_precision: 0.8982\n",
      "\n",
      "Epoch 00020: val_fmeasure did not improve from 0.86497\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "fold:  2  training\n",
      "Learning rate:  0.1\n",
      "Train on 3833 samples, validate on 1917 samples\n",
      "Epoch 1/20\n",
      "3833/3833 [==============================] - 69s 18ms/step - loss: 0.1423 - acc: 0.9473 - fmeasure: 0.7776 - precision: 0.8234 - val_loss: 0.0944 - val_acc: 0.9643 - val_fmeasure: 0.8478 - val_precision: 0.9010\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.84783, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 2/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1409 - acc: 0.9491 - fmeasure: 0.7815 - precision: 0.8334 - val_loss: 0.0988 - val_acc: 0.9617 - val_fmeasure: 0.8340 - val_precision: 0.8896\n",
      "\n",
      "Epoch 00002: val_fmeasure did not improve from 0.84783\n",
      "Epoch 3/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1553 - acc: 0.9439 - fmeasure: 0.7510 - precision: 0.8029 - val_loss: 0.1151 - val_acc: 0.9585 - val_fmeasure: 0.8219 - val_precision: 0.8732\n",
      "\n",
      "Epoch 00003: val_fmeasure did not improve from 0.84783\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/20\n",
      "3833/3833 [==============================] - 20s 5ms/step - loss: 0.1389 - acc: 0.9484 - fmeasure: 0.7807 - precision: 0.8243 - val_loss: 0.0790 - val_acc: 0.9700 - val_fmeasure: 0.8754 - val_precision: 0.9021\n",
      "\n",
      "Epoch 00004: val_fmeasure improved from 0.84783 to 0.87537, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 5/20\n",
      "3833/3833 [==============================] - 21s 5ms/step - loss: 0.1288 - acc: 0.9520 - fmeasure: 0.7967 - precision: 0.8428 - val_loss: 0.0864 - val_acc: 0.9662 - val_fmeasure: 0.8627 - val_precision: 0.8737\n",
      "\n",
      "Epoch 00005: val_fmeasure did not improve from 0.87537\n",
      "Epoch 6/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1252 - acc: 0.9524 - fmeasure: 0.7992 - precision: 0.8410 - val_loss: 0.0761 - val_acc: 0.9697 - val_fmeasure: 0.8745 - val_precision: 0.9020\n",
      "\n",
      "Epoch 00006: val_fmeasure did not improve from 0.87537\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1260 - acc: 0.9542 - fmeasure: 0.8060 - precision: 0.8474 - val_loss: 0.0797 - val_acc: 0.9693 - val_fmeasure: 0.8724 - val_precision: 0.8991\n",
      "\n",
      "Epoch 00007: val_fmeasure did not improve from 0.87537\n",
      "Epoch 8/20\n",
      "3833/3833 [==============================] - 21s 5ms/step - loss: 0.1059 - acc: 0.9606 - fmeasure: 0.8355 - precision: 0.8700 - val_loss: 0.0746 - val_acc: 0.9707 - val_fmeasure: 0.8781 - val_precision: 0.9090\n",
      "\n",
      "Epoch 00008: val_fmeasure improved from 0.87537 to 0.87811, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 9/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1161 - acc: 0.9594 - fmeasure: 0.8304 - precision: 0.8651 - val_loss: 0.0732 - val_acc: 0.9716 - val_fmeasure: 0.8823 - val_precision: 0.9108\n",
      "\n",
      "Epoch 00009: val_fmeasure improved from 0.87811 to 0.88231, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 10/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1039 - acc: 0.9623 - fmeasure: 0.8419 - precision: 0.8813 - val_loss: 0.0711 - val_acc: 0.9738 - val_fmeasure: 0.8916 - val_precision: 0.9173\n",
      "\n",
      "Epoch 00010: val_fmeasure improved from 0.88231 to 0.89159, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 11/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1238 - acc: 0.9553 - fmeasure: 0.8036 - precision: 0.8424 - val_loss: 0.0769 - val_acc: 0.9705 - val_fmeasure: 0.8785 - val_precision: 0.9010\n",
      "\n",
      "Epoch 00011: val_fmeasure did not improve from 0.89159\n",
      "Epoch 12/20\n",
      "3833/3833 [==============================] - 20s 5ms/step - loss: 0.1085 - acc: 0.9604 - fmeasure: 0.8269 - precision: 0.8615 - val_loss: 0.0758 - val_acc: 0.9718 - val_fmeasure: 0.8826 - val_precision: 0.9108\n",
      "\n",
      "Epoch 00012: val_fmeasure did not improve from 0.89159\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/20\n",
      "3833/3833 [==============================] - 20s 5ms/step - loss: 0.1106 - acc: 0.9585 - fmeasure: 0.8231 - precision: 0.8583 - val_loss: 0.0718 - val_acc: 0.9729 - val_fmeasure: 0.8877 - val_precision: 0.9133\n",
      "\n",
      "Epoch 00013: val_fmeasure did not improve from 0.89159\n",
      "Epoch 14/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1032 - acc: 0.9617 - fmeasure: 0.8404 - precision: 0.8681 - val_loss: 0.0732 - val_acc: 0.9714 - val_fmeasure: 0.8820 - val_precision: 0.9046\n",
      "\n",
      "Epoch 00014: val_fmeasure did not improve from 0.89159\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1121 - acc: 0.9601 - fmeasure: 0.8330 - precision: 0.8674 - val_loss: 0.0722 - val_acc: 0.9725 - val_fmeasure: 0.8872 - val_precision: 0.9051\n",
      "\n",
      "Epoch 00015: val_fmeasure did not improve from 0.89159\n",
      "Epoch 16/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1103 - acc: 0.9588 - fmeasure: 0.8266 - precision: 0.8601 - val_loss: 0.0716 - val_acc: 0.9728 - val_fmeasure: 0.8879 - val_precision: 0.9109\n",
      "\n",
      "Epoch 00016: val_fmeasure did not improve from 0.89159\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 17/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1064 - acc: 0.9612 - fmeasure: 0.8352 - precision: 0.8798 - val_loss: 0.0695 - val_acc: 0.9739 - val_fmeasure: 0.8919 - val_precision: 0.9168\n",
      "\n",
      "Epoch 00017: val_fmeasure improved from 0.89159 to 0.89187, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r1.hdf5\n",
      "Epoch 18/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1021 - acc: 0.9601 - fmeasure: 0.8315 - precision: 0.8717 - val_loss: 0.0693 - val_acc: 0.9737 - val_fmeasure: 0.8909 - val_precision: 0.9170\n",
      "\n",
      "Epoch 00018: val_fmeasure did not improve from 0.89187\n",
      "Epoch 19/20\n",
      "3833/3833 [==============================] - 20s 5ms/step - loss: 0.1033 - acc: 0.9618 - fmeasure: 0.8394 - precision: 0.8780 - val_loss: 0.0699 - val_acc: 0.9735 - val_fmeasure: 0.8905 - val_precision: 0.9142\n",
      "\n",
      "Epoch 00019: val_fmeasure did not improve from 0.89187\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 20/20\n",
      "3833/3833 [==============================] - 19s 5ms/step - loss: 0.1127 - acc: 0.9567 - fmeasure: 0.8187 - precision: 0.8588 - val_loss: 0.0703 - val_acc: 0.9733 - val_fmeasure: 0.8898 - val_precision: 0.9140\n",
      "\n",
      "Epoch 00020: val_fmeasure did not improve from 0.89187\n",
      "fold:  3  training\n",
      "Learning rate:  0.1\n",
      "Train on 3835 samples, validate on 1915 samples\n",
      "Epoch 1/20\n",
      "3835/3835 [==============================] - 75s 20ms/step - loss: 0.1356 - acc: 0.9500 - fmeasure: 0.7869 - precision: 0.8376 - val_loss: 0.0733 - val_acc: 0.9717 - val_fmeasure: 0.8815 - val_precision: 0.9206\n",
      "\n",
      "Epoch 00001: val_fmeasure improved from -inf to 0.88150, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 2/20\n",
      "3835/3835 [==============================] - 22s 6ms/step - loss: 0.1383 - acc: 0.9496 - fmeasure: 0.7861 - precision: 0.8251 - val_loss: 0.0727 - val_acc: 0.9718 - val_fmeasure: 0.8841 - val_precision: 0.9090\n",
      "\n",
      "Epoch 00002: val_fmeasure improved from 0.88150 to 0.88405, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 3/20\n",
      "3835/3835 [==============================] - 21s 5ms/step - loss: 0.1352 - acc: 0.9495 - fmeasure: 0.7842 - precision: 0.8295 - val_loss: 0.0845 - val_acc: 0.9666 - val_fmeasure: 0.8569 - val_precision: 0.9121\n",
      "\n",
      "Epoch 00003: val_fmeasure did not improve from 0.88405\n",
      "Epoch 4/20\n",
      "3835/3835 [==============================] - 19s 5ms/step - loss: 0.1356 - acc: 0.9494 - fmeasure: 0.7817 - precision: 0.8310 - val_loss: 0.0871 - val_acc: 0.9668 - val_fmeasure: 0.8597 - val_precision: 0.9036\n",
      "\n",
      "Epoch 00004: val_fmeasure did not improve from 0.88405\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "3835/3835 [==============================] - 20s 5ms/step - loss: 0.1205 - acc: 0.9555 - fmeasure: 0.8125 - precision: 0.8480 - val_loss: 0.0691 - val_acc: 0.9732 - val_fmeasure: 0.8899 - val_precision: 0.9103\n",
      "\n",
      "Epoch 00005: val_fmeasure improved from 0.88405 to 0.88986, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 6/20\n",
      "3835/3835 [==============================] - 20s 5ms/step - loss: 0.1177 - acc: 0.9572 - fmeasure: 0.8170 - precision: 0.8608 - val_loss: 0.0693 - val_acc: 0.9730 - val_fmeasure: 0.8889 - val_precision: 0.9123\n",
      "\n",
      "Epoch 00006: val_fmeasure did not improve from 0.88986\n",
      "Epoch 7/20\n",
      "3835/3835 [==============================] - 20s 5ms/step - loss: 0.1218 - acc: 0.9550 - fmeasure: 0.8004 - precision: 0.8451 - val_loss: 0.0691 - val_acc: 0.9732 - val_fmeasure: 0.8903 - val_precision: 0.9122\n",
      "\n",
      "Epoch 00007: val_fmeasure improved from 0.88986 to 0.89029, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 8/20\n",
      "3835/3835 [==============================] - 20s 5ms/step - loss: 0.1118 - acc: 0.9591 - fmeasure: 0.8262 - precision: 0.8678 - val_loss: 0.0720 - val_acc: 0.9724 - val_fmeasure: 0.8868 - val_precision: 0.9092\n",
      "\n",
      "Epoch 00008: val_fmeasure did not improve from 0.89029\n",
      "Epoch 9/20\n",
      "3835/3835 [==============================] - 20s 5ms/step - loss: 0.1136 - acc: 0.9573 - fmeasure: 0.8199 - precision: 0.8591 - val_loss: 0.0714 - val_acc: 0.9728 - val_fmeasure: 0.8879 - val_precision: 0.9145\n",
      "\n",
      "Epoch 00009: val_fmeasure did not improve from 0.89029\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/20\n",
      "3835/3835 [==============================] - 20s 5ms/step - loss: 0.1008 - acc: 0.9620 - fmeasure: 0.8410 - precision: 0.8779 - val_loss: 0.0668 - val_acc: 0.9745 - val_fmeasure: 0.8957 - val_precision: 0.9142\n",
      "\n",
      "Epoch 00010: val_fmeasure improved from 0.89029 to 0.89566, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 11/20\n",
      "3835/3835 [==============================] - 20s 5ms/step - loss: 0.1082 - acc: 0.9593 - fmeasure: 0.8282 - precision: 0.8660 - val_loss: 0.0643 - val_acc: 0.9749 - val_fmeasure: 0.8970 - val_precision: 0.9206\n",
      "\n",
      "Epoch 00011: val_fmeasure improved from 0.89566 to 0.89697, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 12/20\n",
      "3835/3835 [==============================] - 21s 5ms/step - loss: 0.1002 - acc: 0.9630 - fmeasure: 0.8460 - precision: 0.8764 - val_loss: 0.0645 - val_acc: 0.9757 - val_fmeasure: 0.8994 - val_precision: 0.9284\n",
      "\n",
      "Epoch 00012: val_fmeasure improved from 0.89697 to 0.89942, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 13/20\n",
      "3835/3835 [==============================] - 19s 5ms/step - loss: 0.0933 - acc: 0.9666 - fmeasure: 0.8601 - precision: 0.8956 - val_loss: 0.0645 - val_acc: 0.9758 - val_fmeasure: 0.9008 - val_precision: 0.9218\n",
      "\n",
      "Epoch 00013: val_fmeasure improved from 0.89942 to 0.90075, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 14/20\n",
      "3835/3835 [==============================] - 21s 5ms/step - loss: 0.1048 - acc: 0.9609 - fmeasure: 0.8346 - precision: 0.8713 - val_loss: 0.0657 - val_acc: 0.9748 - val_fmeasure: 0.8972 - val_precision: 0.9145\n",
      "\n",
      "Epoch 00014: val_fmeasure did not improve from 0.90075\n",
      "Epoch 15/20\n",
      "3835/3835 [==============================] - 19s 5ms/step - loss: 0.1066 - acc: 0.9610 - fmeasure: 0.8355 - precision: 0.8698 - val_loss: 0.0635 - val_acc: 0.9766 - val_fmeasure: 0.9034 - val_precision: 0.9309\n",
      "\n",
      "Epoch 00015: val_fmeasure improved from 0.90075 to 0.90342, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n",
      "Epoch 16/20\n",
      "3835/3835 [==============================] - 19s 5ms/step - loss: 0.0949 - acc: 0.9655 - fmeasure: 0.8558 - precision: 0.8927 - val_loss: 0.0630 - val_acc: 0.9756 - val_fmeasure: 0.8991 - val_precision: 0.9265\n",
      "\n",
      "Epoch 00016: val_fmeasure did not improve from 0.90342\n",
      "Epoch 17/20\n",
      "3835/3835 [==============================] - 19s 5ms/step - loss: 0.1064 - acc: 0.9604 - fmeasure: 0.8318 - precision: 0.8678 - val_loss: 0.0675 - val_acc: 0.9740 - val_fmeasure: 0.8912 - val_precision: 0.9291\n",
      "\n",
      "Epoch 00017: val_fmeasure did not improve from 0.90342\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 18/20\n",
      "3835/3835 [==============================] - 20s 5ms/step - loss: 0.1011 - acc: 0.9635 - fmeasure: 0.8454 - precision: 0.8792 - val_loss: 0.0627 - val_acc: 0.9765 - val_fmeasure: 0.9030 - val_precision: 0.9307\n",
      "\n",
      "Epoch 00018: val_fmeasure did not improve from 0.90342\n",
      "Epoch 19/20\n",
      "3835/3835 [==============================] - 19s 5ms/step - loss: 0.0863 - acc: 0.9673 - fmeasure: 0.8645 - precision: 0.8940 - val_loss: 0.0632 - val_acc: 0.9764 - val_fmeasure: 0.9023 - val_precision: 0.9316\n",
      "\n",
      "Epoch 00019: val_fmeasure did not improve from 0.90342\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 20/20\n",
      "3835/3835 [==============================] - 19s 5ms/step - loss: 0.1021 - acc: 0.9624 - fmeasure: 0.8416 - precision: 0.8743 - val_loss: 0.0603 - val_acc: 0.9776 - val_fmeasure: 0.9077 - val_precision: 0.9330\n",
      "\n",
      "Epoch 00020: val_fmeasure improved from 0.90342 to 0.90769, saving model to ./tst_model/tst_eight_attention_weights-best_k0_r2.hdf5\n"
     ]
    }
   ],
   "source": [
    "#print(\"train_x shape :\", train_x.shape)\n",
    "       \n",
    "model_path = './tst_model/'\n",
    "\n",
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 3\n",
    "    n_classes = 8\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "    kf = kfold.split(train_index, train_label)\n",
    "\n",
    "    blend_train = np.zeros((6500, n_classes)).astype('float32') #len(train_x)\n",
    "    blend_test = np.zeros((500, n_fold, n_classes)).astype('float32') #len(test_x)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i+1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        index_tr = train_index[index_train]\n",
    "        index_vld = train_index[index_valid]\n",
    "\n",
    "        '''  '''\n",
    "        \n",
    "        X_tr = [ecg12_seg0[index_tr], ecg12_seg1[index_tr], ecg12_seg2[index_tr], ecg12_seg3[index_tr],\n",
    "                ecg12_seg4[index_tr], ecg12_seg5[index_tr], ecg12_seg6[index_tr], ecg12_seg7[index_tr],\n",
    "                ecg12_seg8[index_tr], ecg12_seg9[index_tr],\n",
    "               ]\n",
    "\n",
    "        X_vld = [ecg12_seg0[index_vld], ecg12_seg1[index_vld], ecg12_seg2[index_vld], ecg12_seg3[index_vld],\n",
    "                 ecg12_seg4[index_vld], ecg12_seg5[index_vld], ecg12_seg6[index_vld], ecg12_seg7[index_vld],\n",
    "                 ecg12_seg8[index_vld], ecg12_seg9[index_vld],\n",
    "               ]\n",
    "        \n",
    "        #X_tr = train_x[index_tr]\n",
    "        #X_vld = train_x[index_vld]\n",
    "        #print(index_tr)\n",
    "\n",
    "        y_tr = keras.utils.to_categorical(labels['label1'].values[index_tr]-1 ,num_classes=8) #preprocess_y(labels,index_tr)\n",
    "        y_vld = keras.utils.to_categorical(labels['label1'].values[index_vld]-1 ,num_classes=8)   #preprocess_y(labels,index_vld)\n",
    "\n",
    "        #print(y_tr.shape)\n",
    "        #print(y_vld.shape)\n",
    "        #print(y_tr[:10])\n",
    "        #print(y_vld[:10])\n",
    "\n",
    "        checkpointer = ModelCheckpoint(filepath=model_path+'tst_eight_attention_weights-best_k{}_r{}.hdf5'.format(seed,i),\n",
    "                                       monitor='val_fmeasure', verbose=1, save_best_only=True,\n",
    "                                       save_weights_only=True,\n",
    "                                       mode='max')  # val_fmeasure  val_loss\n",
    "        reduce = ReduceLROnPlateau(monitor='val_fmeasure', factor=0.5, patience=2, verbose=1, min_delta=1e-4,\n",
    "                                   mode='max')\n",
    "\n",
    "        config = Config()\n",
    "        add_compile(model, config)\n",
    "        ''' \n",
    "        model_name = 'resnet21.h5'\n",
    "        earlystop = EarlyStopping(\n",
    "            monitor='val_fmeasure',  # 'val_categorical_accuracy',\n",
    "            patience=10,\n",
    "        )\n",
    "        checkpoint = ModelCheckpoint(filepath=model_name,\n",
    "                                     monitor='val_categorical_accuracy', mode='max',\n",
    "                                     save_best_only='True')\n",
    "\n",
    "        lr_scheduler = LearningRateScheduler(config.lr_schedule)\n",
    "        '''\n",
    "        callback_lists = [checkpointer, reduce]  # [checkpointer,lr_scheduler]#\n",
    "        # [checkpointer,earlystop,lr_scheduler]\n",
    "        # [checkpoint, earlystop,lr_scheduler]\n",
    "\n",
    "        history = model.fit(x=X_tr, y=y_tr, batch_size=64, epochs=20,  # class_weight=cw,#'auto',\n",
    "                            verbose=1, validation_data=(X_vld, y_vld), callbacks=callback_lists)\n",
    "\n",
    "        # Evaluate best trained model\n",
    "        #model.load_weights(model_path+'densenet_weights-best_k{}_r{}.hdf5'.format(seed, i))\n",
    "\n",
    "        test_y = model.predict(test_x)\n",
    "        val_y = model.predict(X_vld)\n",
    "        \n",
    "        del X_tr\n",
    "        del X_vld\n",
    "        \n",
    "        #K.clear_session()\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        #config = tf.ConfigProto()\n",
    "        #config.gpu_options.allow_growth=True\n",
    "        #sess = tf.Session(config=config)\n",
    "        #K.set_session(sess)\n",
    "\n",
    "        blend_train[index_vld, :] = val_y\n",
    "        blend_test[:, i, :] = test_y\n",
    "\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3, 8)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test =  np.delete(bin_label,0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7370, 7511, 7519, 7547, 7570, 7588, 7611, 7616, 7635, 7690]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 6413, 6432, 6487], dtype=int16)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5750, 8)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test[train_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_yy = 0.2*blend_test[:,1,:]+0.8*blend_test[:,2,:]#blend_test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "classes = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "test_y = test_yy\n",
    "\n",
    "y_pred = [[1 if test_y[i,j]>= 0.3 else 0 for j in range(test_y.shape[1])] #best_threshold[j]\n",
    "          for i in range(len(test_y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = [[0]+i for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=[]\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(9) if y_pred[j][i] == 1])\n",
    "\n",
    "with open('answers_attention_tst8.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                    'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "            \n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name) \n",
    "            \n",
    "            result = pred[count]\n",
    "            \n",
    "            answer.extend(result)\n",
    "            for i in range(8-len(result)):\n",
    "                answer.append('')\n",
    "                \n",
    "            #print(answer)\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
