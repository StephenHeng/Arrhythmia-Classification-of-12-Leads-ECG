{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import time\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# import wfdb\n",
    "# import wfdb.processing as wp\n",
    "# from utils import extract_basic_features\n",
    "# from utils import find_noise_features, extract_basic_features\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "''' '''\n",
    "# from resnet_ecg.utils import one_hot,get_batches\n",
    "from resnet_ecg.ecg_preprocess import ecg_preprocessing\n",
    "from resnet_ecg.densemodel import Net\n",
    "from resnet_ecg import attentionmodel\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, load_model\n",
    "import keras\n",
    "import pywt\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "''' '''\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "\n",
    "# path of training data\n",
    "path = '/media/jdcloud/'\n",
    "\n",
    "\n",
    "def wavelet(ecg, wavefunc, lv, m, n):  #\n",
    "\n",
    "    coeff = pywt.wavedec(ecg, wavefunc, mode='sym', level=lv)  #\n",
    "    # sgn = lambda x: 1 if x > 0 else -1 if x < 0 else 0\n",
    "\n",
    "    for i in range(m, n + 1):\n",
    "        cD = coeff[i]\n",
    "        for j in range(len(cD)):\n",
    "            Tr = np.sqrt(2 * np.log(len(cD)))\n",
    "            if cD[j] >= Tr:\n",
    "                coeff[i][j] = np.sign(cD[j]) - Tr\n",
    "            else:\n",
    "                coeff[i][j] = 0\n",
    "\n",
    "    denoised_ecg = pywt.waverec(coeff, wavefunc)\n",
    "    return denoised_ecg\n",
    "\n",
    "\n",
    "def wavelet_db6(sig):\n",
    "    \"\"\"\n",
    "    R J, Acharya U R, Min L C. ECG beat classification using PCA, LDA, ICA and discrete\n",
    "     wavelet transform[J].Biomedical Signal Processing and Control, 2013, 8(5): 437-448.\n",
    "    param sig: 1-D numpy Array\n",
    "    return: 1-D numpy Array\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(sig, 'db6', level=9)\n",
    "    coeffs[-1] = np.zeros(len(coeffs[-1]))\n",
    "    coeffs[-2] = np.zeros(len(coeffs[-2]))\n",
    "    coeffs[0] = np.zeros(len(coeffs[0]))\n",
    "    sig_filt = pywt.waverec(coeffs, 'db6')\n",
    "    return sig_filt\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    # Calculates the F score, the weighted harmonic mean of precision and recall.\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "def read_data_seg(data_path, split=\"Train\", preprocess=False, fs=500, newFs=256, winSecond=10, winNum=10, n_index=0,pre_type=\"sym\"):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    # n_index = 0\n",
    "    n_class = 9\n",
    "    winSize = winSecond * fs\n",
    "    new_winSize = winSecond * newFs\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read labels and one-hot encode\n",
    "    # label_path = os.path.join(data_path, \"reference.txt\")\n",
    "    # labels = pd.read_csv(label_path, sep='\\t',header = None)\n",
    "    # labels = pd.read_csv(\"reference.csv\")\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    # print(channel_files)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12  # len(channel_files)\n",
    "    # posix = len(split) + 5\n",
    "\n",
    "    # Initiate array\n",
    "    list_of_channels = []\n",
    "\n",
    "    X = np.zeros((len(channel_files), new_winSize, n_channels)).astype('float32')\n",
    "    i_ch = 0\n",
    "\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1', 'II', 'aVR', 'V3', 'III', 'V5']\n",
    "\n",
    "\n",
    "    for i_ch, fil_ch in enumerate(channel_files[:]):  # tqdm\n",
    "\n",
    "        if i_ch % 1000 == 0:\n",
    "            print(i_ch)\n",
    "\n",
    "        ecg = sio.loadmat(os.path.join(path_signals, fil_ch))\n",
    "        ecg_length = ecg[\"I\"].shape[1]\n",
    "\n",
    "        if ecg_length > fs * winNum * winSecond:\n",
    "            print(\" too long !!!\", ecg_length)\n",
    "            ecg_length = fs * winNum * winSecond\n",
    "        if ecg_length < 4500:\n",
    "            print(\" too short !!!\", ecg_length)\n",
    "            break\n",
    "\n",
    "        slide_steps = int((ecg_length - winSize) / winSecond)\n",
    "\n",
    "        if ecg_length <= 4500:\n",
    "            slide_steps = 0\n",
    "\n",
    "        ecg_channels = np.zeros((new_winSize, n_channels)).astype('float32')\n",
    "\n",
    "        for i_n, ch_name in enumerate(channel_name):\n",
    "\n",
    "            ecg_channels[:, i_n] = signal.resample(ecg[ch_name]\n",
    "                                                   [:, n_index * slide_steps:n_index * slide_steps + winSize].T\n",
    "                                                   , new_winSize).T\n",
    "            if preprocess:\n",
    "                if pre_type == \"sym\":\n",
    "                    ecg_channels[:, i_n] = ecg_preprocessing(ecg_channels[:, i_n].reshape(1, new_winSize), 'sym8', 8, 3,\n",
    "                                                             newFs, removebaseline=False, normalize=False)[0]\n",
    "                elif pre_type == \"db4\":\n",
    "                    ecg_channels[:, i_n] = wavelet(ecg_channels[:, i_n], 'db4', 4, 2, 4)\n",
    "                elif pre_type == \"db6\":\n",
    "                    ecg_channels[:, i_n] = wavelet_db6(ecg_channels[:, i_n])\n",
    "\n",
    "                # ecg_channels[:, i_n] = (ecg_channels[:, i_n]-np.mean(ecg_channels[:, i_n]))/np.std(ecg_channels[:, i_n])\n",
    "            else:\n",
    "                pass\n",
    "                print(\" no preprocess !!! \")\n",
    "\n",
    "        X[i_ch, :, :] = ecg_channels\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_y(labels, y, num_class=9):\n",
    "    bin_label = np.zeros((len(y), num_class)).astype('int8')\n",
    "    for i in range(len(y)):\n",
    "        label_nona = labels.loc[y[i]].dropna()\n",
    "        for j in range(1, label_nona.shape[0]):\n",
    "            bin_label[i, int(label_nona[j])] = 1\n",
    "    return bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_path, split=\"Train\", preprocess=True, fs=500, newFs=256, winSecond=10, winNum=10, n_index=0,pre_type=\"sym\"):\n",
    "    \"\"\" Read data \"\"\"\n",
    "\n",
    "    # Fixed params\n",
    "    # n_index = 0\n",
    "    n_class = 9\n",
    "    winSize = winSecond * fs\n",
    "    new_winSize = 23296# winSecond * newFs\n",
    "    # Paths\n",
    "    path_signals = os.path.join(data_path, split)\n",
    "\n",
    "    # Read time-series data\n",
    "    channel_files = os.listdir(path_signals)\n",
    "    channel_files.sort()\n",
    "    n_channels = 12  # len(channel_files)\n",
    "\n",
    "    X = np.zeros((len(channel_files), new_winSize, n_channels)).astype('float32')\n",
    "\n",
    "    channel_name = ['V6', 'aVF', 'I', 'V4', 'V2', 'aVL', 'V1', 'II', 'aVR', 'V3', 'III', 'V5']\n",
    "\n",
    "    for i_ch, fil_ch in enumerate(channel_files[:]):  # tqdm\n",
    "\n",
    "        if i_ch % 1000 == 0:\n",
    "            print(i_ch)\n",
    "\n",
    "        ecg = sio.loadmat(os.path.join(path_signals, fil_ch))\n",
    "        ecg_length = ecg[\"I\"].shape[1]\n",
    "        \n",
    "        if ecg_length > 45500:\n",
    "            ecg_length = 45500\n",
    "\n",
    "        ecg_channels = np.zeros((new_winSize, n_channels)).astype('float32')\n",
    "\n",
    "        for i_n, ch_name in enumerate(channel_name):\n",
    "\n",
    "            ecg_data = signal.resample(ecg[ch_name][:,:ecg_length].T, \n",
    "                                       int(ecg[ch_name][:,:ecg_length].shape[1] / 500 * newFs)).T\n",
    "            if preprocess:\n",
    "                if pre_type == \"sym\":\n",
    "                    ecg_channels[-ecg_data.shape[1]:, i_n] = ecg_preprocessing(ecg_data, 'sym8', 8, 3,\n",
    "                                                    newFs, removebaseline=False, normalize=False)[0]\n",
    "\n",
    "                elif pre_type == \"db4\":\n",
    "                    ecg_channels[-ecg_data.shape[1]:, i_n] = wavelet(ecg_data[0], 'db4', 4, 2, 4)\n",
    "                elif pre_type == \"db6\":\n",
    "                    ecg_channels[-ecg_data.shape[1]:, i_n] = wavelet_db6(ecg_data[0])\n",
    "\n",
    "                # ecg_channels[:, i_n] = (ecg_channels[:, i_n]-np.mean(ecg_channels[:, i_n]))/np.std(ecg_channels[:, i_n])\n",
    "            else:\n",
    "                pass\n",
    "                print(\" no preprocess !!! \")\n",
    "\n",
    "        X[i_ch, :, :] = ecg_channels\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/JDWorkSpace/vyuf0458/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/JDWorkSpace/uuser/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "************************\n",
      "fold:  1  training\n",
      "(6500,)\n",
      "fold:  2  training\n",
      "(6500,)\n",
      "fold:  3  training\n",
      "(6500,)\n",
      "best_threshold : [0.5 0.7 0.6 0.2 0.4 0.2 0.5 0.2 0.4]\n",
      " train data f1_score  : 0.9529411674736975\n",
      "f1 score of ab 0 is 0.9599349129901498\n",
      "f1 score of ab 1 is 0.9941117520288308\n",
      "f1 score of ab 2 is 0.9778956060483803\n",
      "f1 score of ab 3 is 0.99722620747456\n",
      "f1 score of ab 4 is 0.9767802069020152\n",
      "f1 score of ab 5 is 0.9915109802205838\n",
      "f1 score of ab 6 is 0.9845972343293512\n",
      "f1 score of ab 7 is 0.9148090014119739\n",
      "f1 score of ab 8 is 0.9560048862533019\n"
     ]
    }
   ],
   "source": [
    "pre_type = \"sym\"# \"sym\"\n",
    "\n",
    "labels = pd.read_csv(path + \"reference.csv\")\n",
    "raw_IDs = labels[\"File_name\"].values.tolist()\n",
    "\n",
    "IDs = {}\n",
    "IDs[\"sym\"] = raw_IDs\n",
    "IDs[\"db4\"] = [i + \"_db4\" for i in raw_IDs]\n",
    "IDs[\"db6\"] = [i + \"_db6\" for i in raw_IDs]\n",
    "\n",
    "input_size = (2560, 12)\n",
    "net_num = 10\n",
    "inputs_list = [Input(shape=input_size) for _ in range(net_num)]\n",
    "net = Net()\n",
    "outputs = net.nnet(inputs_list, 0.5, num_classes=9)\n",
    "model = Model(inputs=inputs_list, outputs=outputs)\n",
    "\n",
    "net_num = 10\n",
    "test_x = [read_data_seg(path, split='Val', preprocess=True, n_index=i, pre_type=pre_type) for i in range(net_num)]\n",
    "\n",
    "model_path = './official_densenet_model/'\n",
    "\n",
    "en_amount = 1\n",
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 3  # 3\n",
    "    n_classes = 9\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "    kf = kfold.split(IDs[pre_type], labels['label1'])\n",
    "\n",
    "    densenet_blend_train = np.zeros((6500, n_fold, n_classes)).astype('float32')  # len(train_x)\n",
    "    densenet_blend_test = np.zeros((500, n_fold, n_classes)).astype('float32')  # len(test_x)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i + 1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        tr_IDs = np.array(IDs[pre_type]) # [index_train]\n",
    "        # val_IDs = np.array(IDs[pre_type])[index_valid]\n",
    "        print(tr_IDs.shape)\n",
    "\n",
    "        X = np.empty((tr_IDs.shape[0], 10, 2560, 12))\n",
    "        for j, ID in enumerate(tr_IDs):\n",
    "            X[j, ] = np.load(\"training_data/\" + ID + \".npy\")\n",
    "        # X_tr = [(X[:, i] - np.mean(X[:, i])) / np.std(X[:, i]) for i in range(10)]\n",
    "        X_tr = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n",
    "        # print(X.shape)\n",
    "        del X\n",
    "\n",
    "        # Evaluate best trained model\n",
    "        model.load_weights(model_path + 'densenet_extend_weights-best_k{}_r{}.hdf5'.format(seed, i))\n",
    "\n",
    "        densenet_blend_train[:, i, :] = model.predict(X_tr)\n",
    "        densenet_blend_test[:, i, :] = model.predict(test_x)\n",
    "\n",
    "        del X_tr\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        count += 1\n",
    "\n",
    "index = np.arange(6500)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "train_y = 0.1 * densenet_blend_train[:, 0, :] + 0.1 * densenet_blend_train[:, 1, :] + 0.8 * densenet_blend_train[:, 2, :]\n",
    "\n",
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(train_y.shape[1])\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "    y_prob = np.array(train_y[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if train_y[i, j] >= best_threshold[j] else 0 for j in range(train_y.shape[1])]\n",
    "          for i in range(len(train_y))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional, LeakyReLU\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,  Input, Reshape, GRU, CuDNNGRU\n",
    "from keras.layers import Convolution1D, MaxPool1D, GlobalAveragePooling1D,concatenate,AveragePooling1D\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import Layer\n",
    "import numpy as np\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "            self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        a = K.exp(ait)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test_x shape:  (500, 23296, 12)\n",
      "************************\n",
      "fold:  1  training\n",
      "(6500,)\n",
      "fold:  2  training\n",
      "(6500,)\n",
      "fold:  3  training\n",
      "(6500,)\n"
     ]
    }
   ],
   "source": [
    "pre_type = \"db6\"# \"sym\"\n",
    "\n",
    "labels = pd.read_csv(path + \"reference.csv\")\n",
    "raw_IDs = labels[\"File_name\"].values.tolist()\n",
    "\n",
    "IDs = {}\n",
    "IDs[\"sym\"] = raw_IDs\n",
    "IDs[\"db4\"] = [i + \"_db4\" for i in raw_IDs]\n",
    "IDs[\"db6\"] = [i + \"_db6\" for i in raw_IDs]\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 9\n",
    "len_seg = 23296  # 91s\n",
    "\n",
    "main_input = Input(shape=(len_seg, 12), dtype='float32', name='main_input')\n",
    "x = Convolution1D(12, 3, padding='same')(main_input)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 24, strides=2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 24, strides=2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 24, strides=2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 24, strides=2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 48, strides=2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "cnnout = Dropout(0.2)(x)\n",
    "x = Bidirectional(CuDNNGRU(12, input_shape=(2250, 12), return_sequences=True, return_state=False))(cnnout)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = AttentionWithContext()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "main_output = Dense(num_classes, activation='sigmoid')(x)\n",
    "model = Model(inputs=main_input, outputs=main_output)\n",
    "\n",
    "test_x = read_data(path, split='Val', preprocess=True, n_index=0, pre_type=pre_type)\n",
    "print(\"test_x shape: \",test_x.shape)\n",
    "model_path = '/media/uuser/data/test/official_attention_onenet_model/'\n",
    "\n",
    "\n",
    "en_amount = 1\n",
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 3  # 3\n",
    "    n_classes = 9\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "    kf = kfold.split(IDs[pre_type], labels['label1'])\n",
    "\n",
    "    attention_one_blend_train = np.zeros((6500, n_fold, n_classes)).astype('float32')  # len(train_x)\n",
    "    attention_one_blend_test = np.zeros((500, n_fold, n_classes)).astype('float32')  # len(test_x)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i + 1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        tr_IDs = np.array(IDs[pre_type]) # [index_train]\n",
    "        # val_IDs = np.array(IDs[pre_type])[index_valid]\n",
    "        print(tr_IDs.shape)\n",
    "\n",
    "        X = np.empty((tr_IDs.shape[0], 23296, 12))\n",
    "        for j, ID in enumerate(tr_IDs):\n",
    "            X[j, ] = np.load(\"/media/uuser/data/test/training_data_pre/\" + ID + \".npy\")\n",
    "        # X_tr = [(X[:, i] - np.mean(X[:, i])) / np.std(X[:, i]) for i in range(10)]\n",
    "        X_tr = X\n",
    "        # print(X.shape)\n",
    "        del X\n",
    "\n",
    "        # Evaluate best trained model\n",
    "        model.load_weights(model_path + 'attention_extend_weights-best_k{}_r{}_0608.hdf5'.format(seed, i))\n",
    "\n",
    "        attention_one_blend_train[:, i, :] = model.predict(X_tr)\n",
    "        attention_one_blend_test[:, i, :] = model.predict(test_x)\n",
    "\n",
    "        del X_tr\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold : [0.6 0.5 0.8 0.6 0.5 0.2 0.4 0.4 0.6]\n",
      " train data f1_score  : 0.9233701703313789\n",
      "f1 score of ab 0 is 0.9538712653466751\n",
      "f1 score of ab 1 is 0.9861697600576124\n",
      "f1 score of ab 2 is 0.951993192129309\n",
      "f1 score of ab 3 is 0.9955042073968186\n",
      "f1 score of ab 4 is 0.9138753269425088\n",
      "f1 score of ab 5 is 0.9838926937780672\n",
      "f1 score of ab 6 is 0.9729410873353466\n",
      "f1 score of ab 7 is 0.9092356407791482\n",
      "f1 score of ab 8 is 0.9371545803159034\n"
     ]
    }
   ],
   "source": [
    "index = np.arange(6500)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "train_y = 0.1 * attention_one_blend_train[:, 0, :] + 0.1 * attention_one_blend_train[:, 1, :] + 0.8 * attention_one_blend_train[:, 2, :]\n",
    "\n",
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(train_y.shape[1])\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "    y_prob = np.array(train_y[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if train_y[i, j] >= best_threshold[j] else 0 for j in range(train_y.shape[1])]\n",
    "          for i in range(len(train_y))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "\n",
    "\n",
    "out = 0.1 * attention_one_blend_test[:, 0, :] + 0.1 * attention_one_blend_test[:, 1, :] + 0.8 * attention_one_blend_test[:, 2, :]\n",
    "\n",
    "y_pred_test = np.array(\n",
    "    [[1 if out[i, j] >= best_threshold[j] else 0 for j in range(out.shape[1])] for i in range(len(out))])\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "test_y = y_pred_test\n",
    "\n",
    "y_pred = [[1 if test_y[i, j] >= best_threshold[j] else 0 for j in range(test_y.shape[1])]\n",
    "          for i in range(len(test_y))]\n",
    "pred = []\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(9) if y_pred[j][i] == 1])\n",
    "\n",
    "\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('answers_attention_{}_0607.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(8 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble densenet and attention one net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold : [0.6 0.6 0.7 0.4 0.5 0.2 0.4 0.2 0.4]\n",
      " train data f1_score  : 0.9388255901947657\n",
      "f1 score of ab 0 is 0.9639216580042775\n",
      "f1 score of ab 1 is 0.9930662177842734\n",
      "f1 score of ab 2 is 0.9749852955163361\n",
      "f1 score of ab 3 is 0.9968778684583539\n",
      "f1 score of ab 4 is 0.9138753269425088\n",
      "f1 score of ab 5 is 0.9915453445285112\n",
      "f1 score of ab 6 is 0.9851604610758894\n",
      "f1 score of ab 7 is 0.9148090014119739\n",
      "f1 score of ab 8 is 0.9531725095085233\n",
      "out shape:  (500, 9)\n"
     ]
    }
   ],
   "source": [
    "index = np.arange(6500)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "thr = np.array([0.7, 0.7, 0.7, 0.7, 0., 0.7, 0.8, 1., 0.7])\n",
    "\n",
    "train_y1 =   thr * (0.1 * densenet_blend_train[:, 0, :] + \n",
    "            0.1 * densenet_blend_train[:, 1, :] + \n",
    "            0.8 * densenet_blend_train[:, 2, :])\n",
    "\n",
    "train_y2 =  (1-thr) * (0.1 * attention_one_blend_train[:, 0, :] + \n",
    "            0.1 * attention_one_blend_train[:, 1, :] + \n",
    "            0.8 * attention_one_blend_train[:, 2, :])\n",
    "\n",
    "train_y = train_y1+train_y2\n",
    "\n",
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(train_y.shape[1])\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "    y_prob = np.array(train_y[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if train_y[i, j] >= best_threshold[j] else 0 for j in range(train_y.shape[1])]\n",
    "          for i in range(len(train_y))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "    \n",
    "out1 = thr *(0.1 * densenet_blend_test[:, 0, :] + \n",
    "        0.1 * densenet_blend_test[:, 1, :] + \n",
    "        0.8 * densenet_blend_test[:, 2, :])\n",
    "\n",
    "out2 = (1-thr) *(0.1 * attention_one_blend_test[:, 0, :] + \n",
    "        0.1 * attention_one_blend_test[:, 1, :] + \n",
    "        0.8 * attention_one_blend_test[:, 2, :])\n",
    "\n",
    "out = out1+out2\n",
    "\n",
    "print(\"out shape: \",out.shape)\n",
    "y_pred_test = np.array(\n",
    "    [[1 if out[i, j] >= best_threshold[j] else 0 for j in range(out.shape[1])] for i in range(len(out))])\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "test_y = y_pred_test\n",
    "\n",
    "y_pred = [[1 if test_y[i, j] >= best_threshold[j] else 0 for j in range(test_y.shape[1])]\n",
    "          for i in range(len(test_y))]\n",
    "pred = []\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(9) if y_pred[j][i] == 1])\n",
    "    \n",
    "    \n",
    "    \n",
    "for i,val in enumerate(pred):\n",
    "    if 0 in val and len(val) > 1:\n",
    "        flag = 0\n",
    "        for j in val:\n",
    "            if (test_y[i][0] - best_threshold[0]) > (test_y[i][j] - best_threshold[j]):\n",
    "                pass\n",
    "            else:\n",
    "                flag = 1\n",
    "        if flag == 1:\n",
    "            pred[i] = val[1:]\n",
    "        else:\n",
    "            pred[i] = val[0]\n",
    "    if len(val) == 0:\n",
    "        pred[i] = [np.argmin(best_threshold-out[i])]\n",
    "            \n",
    "            \n",
    "\n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('answers_attention_{}_ensemble_0610.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(8 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin([2,3,1,1,2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train data f1_score  : 0.9529411674736975\n",
    "f1 score of ab 0 is 0.9599349129901498\n",
    "f1 score of ab 1 is 0.9941117520288308\n",
    "f1 score of ab 2 is 0.9778956060483803\n",
    "f1 score of ab 3 is 0.99722620747456\n",
    "f1 score of ab 4 is 0.9767802069020152\n",
    "f1 score of ab 5 is 0.9915109802205838\n",
    "f1 score of ab 6 is 0.9845972343293512\n",
    "f1 score of ab 7 is 0.9148090014119739\n",
    "f1 score of ab 8 is 0.9560048862533019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_threshold : [0.6 0.5 0.8 0.6 0.5 0.2 0.4 0.4 0.6]\n",
    "train data f1_score  : 0.9233701703313789\n",
    "f1 score of ab 0 is 0.9538712653466751\n",
    "f1 score of ab 1 is 0.9861697600576124\n",
    "f1 score of ab 2 is 0.951993192129309\n",
    "f1 score of ab 3 is 0.9955042073968186\n",
    "f1 score of ab 4 is 0.9138753269425088\n",
    "f1 score of ab 5 is 0.9838926937780672\n",
    "f1 score of ab 6 is 0.9729410873353466\n",
    "f1 score of ab 7 is 0.9092356407791482\n",
    "f1 score of ab 8 is 0.9371545803159034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()  \n",
    "with open(\"attention_one_net_model.json\", \"w\") as json_file:  \n",
    "    json_file.write(model_json)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "************************\n",
      "fold:  1  training\n",
      "(6500,)\n",
      "fold:  2  training\n",
      "(6500,)\n",
      "fold:  3  training\n",
      "(6500,)\n",
      "best_threshold : [0.6 0.8 0.5 0.2 0.5 0.2 0.2 0.2 0.5]\n",
      " train data f1_score  : 0.9620003898169097\n",
      "f1 score of ab 0 is 0.9732364328811891\n",
      "f1 score of ab 1 is 0.998388285763381\n",
      "f1 score of ab 2 is 0.9792111549900056\n",
      "f1 score of ab 3 is 0.9965363414515095\n",
      "f1 score of ab 4 is 0.9843293819267146\n",
      "f1 score of ab 5 is 0.9854501844600596\n",
      "f1 score of ab 6 is 0.9746731964042199\n",
      "f1 score of ab 7 is 0.9429924064453483\n",
      "f1 score of ab 8 is 0.9659495598771206\n"
     ]
    }
   ],
   "source": [
    "pre_type = \"sym\"# \"sym\"\n",
    "\n",
    "labels = pd.read_csv(path + \"reference.csv\")\n",
    "raw_IDs = labels[\"File_name\"].values.tolist()\n",
    "\n",
    "IDs = {}\n",
    "IDs[\"sym\"] = raw_IDs\n",
    "IDs[\"db4\"] = [i + \"_db4\" for i in raw_IDs]\n",
    "IDs[\"db6\"] = [i + \"_db6\" for i in raw_IDs]\n",
    "\n",
    "input_size = (2560, 12)\n",
    "net_num = 10\n",
    "inputs_list = [Input(shape=input_size) for _ in range(net_num)]\n",
    "outputs = attentionmodel.build_network(inputs_list, 0.5, num_classes=9, block_size=4, relu=False)\n",
    "model = Model(inputs=inputs_list, outputs=outputs)\n",
    "\n",
    "net_num = 10\n",
    "test_x = [read_data_seg(path, split='Val', preprocess=True, n_index=i, pre_type=pre_type) for i in range(net_num)]\n",
    "\n",
    "model_path = './official_attention_model/'\n",
    "\n",
    "en_amount = 1\n",
    "for seed in range(en_amount):\n",
    "    print(\"************************\")\n",
    "    n_fold = 3  # 3\n",
    "    n_classes = 9\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "    kf = kfold.split(IDs[pre_type], labels['label1'])\n",
    "\n",
    "    attention_blend_train = np.zeros((6500, n_fold, n_classes)).astype('float32')  # len(train_x)\n",
    "    attention_blend_test = np.zeros((500, n_fold, n_classes)).astype('float32')  # len(test_x)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i, (index_train, index_valid) in enumerate(kf):\n",
    "        print('fold: ', i + 1, ' training')\n",
    "        t = time.time()\n",
    "\n",
    "        tr_IDs = np.array(IDs[pre_type]) # [index_train]\n",
    "        # val_IDs = np.array(IDs[pre_type])[index_valid]\n",
    "        print(tr_IDs.shape)\n",
    "\n",
    "        X = np.empty((tr_IDs.shape[0], 10, 2560, 12))\n",
    "        for j, ID in enumerate(tr_IDs):\n",
    "            X[j, ] = np.load(\"training_data/\" + ID + \".npy\")\n",
    "        # X_tr = [(X[:, i] - np.mean(X[:, i])) / np.std(X[:, i]) for i in range(10)]\n",
    "        X_tr = [X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], X[:, 6], X[:, 7], X[:, 8], X[:, 9]]\n",
    "        # print(X.shape)\n",
    "        del X\n",
    "\n",
    "        # Evaluate best trained model\n",
    "        model.load_weights(model_path + 'attention_extend_weights-best_k{}_r{}_0608.hdf5'.format(seed, i))\n",
    "\n",
    "        attention_blend_train[:, i, :] = model.predict(X_tr)\n",
    "        attention_blend_test[:, i, :] = model.predict(test_x)\n",
    "\n",
    "        del X_tr\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        count += 1\n",
    "\n",
    "index = np.arange(6500)\n",
    "y_train = preprocess_y(labels, index)\n",
    "\n",
    "train_y = 0.1 * attention_blend_train[:, 0, :] + 0.1 * attention_blend_train[:, 1, :] + 0.8 * attention_blend_train[:, 2, :]\n",
    "\n",
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(train_y.shape[1])\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "    y_prob = np.array(train_y[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if train_y[i, j] >= best_threshold[j] else 0 for j in range(train_y.shape[1])]\n",
    "          for i in range(len(train_y))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneVsRestClassifier(LogisticRegression(C=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 3, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_blend_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train = np.hstack((attention_blend_train[:, 0, :] , attention_blend_train[:, 1, :], attention_blend_train[:, 2, :]))\n",
    "#X_train = np.hstack((attention_one_blend_train[:, 0, :] , \n",
    "#                     attention_one_blend_train[:, 1, :], \n",
    "#                     attention_one_blend_train[:, 2, :]))\n",
    "x1 =  (0.1 * densenet_blend_train[:, 0, :] + \n",
    "            0.1 * densenet_blend_train[:, 1, :] + \n",
    "            0.8 * densenet_blend_train[:, 2, :]) \n",
    "x2 =  (0.1 * attention_one_blend_train[:, 0, :] + \n",
    "            0.1 * attention_one_blend_train[:, 1, :] + \n",
    "            0.8 * attention_one_blend_train[:, 2, :])\n",
    "\n",
    "X_train = np.hstack((x1,x2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 18)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = OneVsRestClassifier(xgb.XGBClassifier(learning_rate=0.005,n_eatimators=500))\n",
    "clf = OneVsRestClassifier(LogisticRegression(C=100))\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test = np.hstack((attention_blend_test[:, 0, :] , attention_blend_test[:, 1, :], attention_blend_test[:, 2, :]))\n",
    "#X_test = np.hstack((attention_one_blend_test[:, 0, :] , \n",
    "#                    attention_one_blend_test[:, 1, :], \n",
    "#                    attention_one_blend_test[:, 2, :]))\n",
    "xt1 = (0.1 * densenet_blend_test[:, 0, :] + \n",
    "        0.1 * densenet_blend_test[:, 1, :] + \n",
    "        0.8 * densenet_blend_test[:, 2, :])\n",
    "\n",
    "xt2 = (0.1 * attention_one_blend_test[:, 0, :] + \n",
    "        0.1 * attention_one_blend_test[:, 1, :] + \n",
    "        0.8 * attention_one_blend_test[:, 2, :])\n",
    "\n",
    "X_test = np.hstack((xt1,xt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_p_train = clf.predict_proba(X_train)\n",
    "y_p_test = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold : [0.3 0.7 0.5 0.4 0.3 0.3 0.2 0.2 0.4]\n",
      " train data f1_score  : 0.9589222099588254\n",
      "f1 score of ab 0 is 0.9704663844317845\n",
      "f1 score of ab 1 is 0.9962461468347303\n",
      "f1 score of ab 2 is 0.9792812723926492\n",
      "f1 score of ab 3 is 0.9975716754676085\n",
      "f1 score of ab 4 is 0.9831316484012695\n",
      "f1 score of ab 5 is 0.9910562034074882\n",
      "f1 score of ab 6 is 0.987649835989822\n",
      "f1 score of ab 7 is 0.9207155546754144\n",
      "f1 score of ab 8 is 0.958464732990536\n"
     ]
    }
   ],
   "source": [
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(y_p_train.shape[1])\n",
    "\n",
    "for i in range(y_p_train.shape[1]):\n",
    "    y_prob = np.array(y_p_train[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if y_p_train[i, j] >= best_threshold[j] else 0 for j in range(y_p_train.shape[1])]\n",
    "          for i in range(len(y_p_train))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "test_y = y_p_test\n",
    "\n",
    "y_pred = [[1 if test_y[i, j] >= best_threshold[j] else 0 for j in range(test_y.shape[1])]\n",
    "          for i in range(len(test_y))]\n",
    "pred = []\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(9) if y_pred[j][i] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sym'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('answers_densenet_{}_ensemble_0609_3.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(8 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold : [0.6 0.8 0.5 0.2 0.5 0.1 0.2 0.2 0.5]\n",
      " train data f1_score  : 0.9628445837768207\n",
      "f1 score of ab 0 is 0.9735929889203465\n",
      "f1 score of ab 1 is 0.998388285763381\n",
      "f1 score of ab 2 is 0.9787580310982424\n",
      "f1 score of ab 3 is 0.9968810963570086\n",
      "f1 score of ab 4 is 0.9843293819267146\n",
      "f1 score of ab 5 is 0.9864175683529343\n",
      "f1 score of ab 6 is 0.9759978037672116\n",
      "f1 score of ab 7 is 0.9445165255394701\n",
      "f1 score of ab 8 is 0.9661107445538584\n"
     ]
    }
   ],
   "source": [
    "train_y = 0. * attention_blend_train[:, 0, :] + 0.2 * attention_blend_train[:, 1, :] + 0.8 * attention_blend_train[:, 2, :]\n",
    "\n",
    "threshold = np.arange(0.1, 0.9, 0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(train_y.shape[1])\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "    y_prob = np.array(train_y[:, i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
    "        acc.append(f1_score(y_train[:, i], y_pred, average='macro'))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc == acc.max())\n",
    "    accuracies.append(acc.max())\n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "\n",
    "print(\"best_threshold :\", best_threshold)\n",
    "\n",
    "y_pred = np.array([[1 if train_y[i, j] >= best_threshold[j] else 0 for j in range(train_y.shape[1])]\n",
    "          for i in range(len(train_y))])\n",
    "print(\" train data f1_score  :\", f1_score(y_train, y_pred, average='macro'))\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"f1 score of ab {} is {}\".format(i, f1_score(y_train[:, i], y_pred[:, i], average='macro')))\n",
    "\n",
    "out = 0. * attention_blend_test[:, 0, :] + 0.2 * attention_blend_test[:, 1, :] + 0.8 * attention_blend_test[:, 2, :]\n",
    "\n",
    "y_pred_test = np.array(\n",
    "    [[1 if out[i, j] >= best_threshold[j] else 0 for j in range(out.shape[1])] for i in range(len(out))])\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "test_y = y_pred_test\n",
    "\n",
    "y_pred = [[1 if test_y[i, j] >= best_threshold[j] else 0 for j in range(test_y.shape[1])]\n",
    "          for i in range(len(test_y))]\n",
    "pred = []\n",
    "for j in range(test_y.shape[0]):\n",
    "    pred.append([classes[i] for i in range(9) if y_pred[j][i] == 1])\n",
    "    \n",
    "val_dataset_path = path + \"/Val/\"\n",
    "val_files = os.listdir(val_dataset_path)\n",
    "val_files.sort()\n",
    "\n",
    "with open('answers_densenet_{}_ensemble_0609_2.csv'.format(pre_type), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File_name', 'label1', 'label2',\n",
    "                     'label3', 'label4', 'label5', 'label6', 'label7', 'label8'])\n",
    "    count = 0\n",
    "    for file_name in val_files:\n",
    "        if file_name.endswith('.mat'):\n",
    "\n",
    "            record_name = file_name.strip('.mat')\n",
    "            answer = []\n",
    "            answer.append(record_name)\n",
    "\n",
    "            result = pred[count]\n",
    "\n",
    "            answer.extend(result)\n",
    "            for i in range(8 - len(result)):\n",
    "                answer.append('')\n",
    "            count += 1\n",
    "            writer.writerow(answer)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
